{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amphi 6 - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. First Example: Name Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks have many applications in:\n",
    "\n",
    "- Natural Language Processing (Speech Recognition, Text Generation, Sentiment Classification, Translation, Name Entity Recognition)\n",
    "- Bioinformatics (DNA sequence analysis)\n",
    "- Video activity recognition\n",
    "- Time series\n",
    "\n",
    "## 1.1 The Problem\n",
    "\n",
    "In Name Entity Recognition problem, we want to define name entities in a sentence/paragraph. Example of inputs and outputs are below:\n",
    "\n",
    "Input: `ha noi is the capital of vietnam` (it can be in speech or text form)\n",
    "\n",
    "Output: (1, 1, 0, 0, 0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Notions\n",
    "\n",
    "Notation: \n",
    "- Data/Example $i$ is denoted by $x^{(i)}, y^{(i)}$\n",
    "- The $t^{th}$ word of $x^{(i)}$ is denoted by $x^{(i)<t>}$. So if $x^{(1)} = $`ha noi is the capital of Vietnam`, it can be rewritten as\n",
    "    $$x^{(1)} = (x^{(1)<1>}, x^{(1)<2>}, \\ldots, x^{(1)<7>})$$ where $x^{(1)<1>} = $`ha`, $\\ldots, x^{(1)<7>} = $ `vietnam`.\n",
    "and\n",
    "    $$y^{(1)} = (y^{(1)<1>}, y^{(1)<2>}, \\ldots, y^{(1)<7>})$$ where $x^{(1)<1>} = 1, \\ldots, x^{(1)<7>} =1$.\n",
    "- We denote the length of $x^{(1)}$ by $T_x^{(i)}$. In this example $T_x^{(1)} = 7$. Similarly, $T_y^{(i)}$ is the length of $T_x^{(i)}$. So $T_y^{(1)} = T_x^{(1)} = 7$ in the example.\n",
    "\n",
    "**Vocabulary**\n",
    "\n",
    "We introduce a **vocabulary**, which is a list of all possible words (in some context). For example, \n",
    "<center>\n",
    "$V = $`['a', 'abs', ..., 'capital', ..., 'is', ..., 'vietnam', ..., 'zebra']`\n",
    "</center>\n",
    "\n",
    "Suppose that orders of those words in the vocabulary are\n",
    "<center>\n",
    "    [1, 2, ..., 3001, ..., 7645, ..., 16999, ..., 20000]\n",
    "</center>\n",
    "\n",
    "then using one-hot-coding, one can rewrite\n",
    "$$\n",
    "x^{(i)<j>} = \\mathbf I_{pos} = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "\\ldots\\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\ldots\\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where $pos$, the position of 1 is the order of the associated in the vocabulary. This is a $D-$dimension vector where $D$ is the vocabulary's size.\n",
    "\n",
    "With our example, we have:\n",
    "$$\n",
    "x^{(1)<3>} = \\mathbf I_{7645}, x^{(1)<7>} = \\mathbf I_{16999}\n",
    "$$\n",
    "\n",
    "We also define the target output dimension by $K$. In our example, $K=1$ as the output can be denoted by a probability (a real number between 0-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Problems with FNN/CNN\n",
    "\n",
    "- Inputs, outputs can be of different lengths in different examples\n",
    "- They does not share features learned across different positions of text.\n",
    "\n",
    "## 1.4 RNN Model for Name Identity Recognition\n",
    "\n",
    "<img src=\"F3.png\" width=\"900\">\n",
    "\n",
    "We introduce latent variables\n",
    "$$\n",
    "a^{(i)<0>}, \\ldots, a^{(i)<T_x^{(i)}>}\n",
    "$$\n",
    "\n",
    "which are vectors of dimension $A$.\n",
    "\n",
    "For short, we ignore the subscript $(i)$ (for order of example), and write\n",
    "$$\n",
    "a^{<0>}, \\ldots, a^{<T_x>}\n",
    "$$\n",
    "\n",
    "$a^{<t>}$ will have a role to learn something from $x^{<t>}$ and its previous value $a^{<t-1>}$. It is like a history of the word stream. This history will play a role together with the new input $x^{<t+1>}$ to predict the next value $y^{<t+1>}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model supposes the following relation between $a^{<t>}, x^{<t>}, y^{<t>}$:\n",
    "\n",
    "$$\n",
    "a^{<t>} = g_a (W_{a}a^{<t-1>} + W_{x}x^{<t>} + b_a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat y^{<t>} = g_y(W_{y}a^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "where $\\mathbf W_{a}, \\mathbf W_{x}, \\mathbf W_{y}$ are $A\\times A$, $A\\times D$ and $K\\times A$-matrices, $b_a, b_y$ are $A-$dimensional and $K-$dimensional vectors, $g_a, g_y$ are activations. $g_a$ are usually $\\tanh$ function while $g_t$ depends on the output. Neither $W_{a}, W_{x}, W_{y}, b_a, b_y, g_a$ nor $g_y$ depend on $t$.\n",
    "\n",
    "We write\n",
    "$$\n",
    "\\mathbf W_a = \\begin{pmatrix} \n",
    "W_a^{1,1}& \\ldots& W_a^{1,A},& W_x^{1,1}& \\ldots& \\ldots& W_x^{1,D}\\\\ \n",
    ". & \\ldots & .,& . \\ldots& \\ldots& . \\\\\n",
    "W_a^{A,1}& \\ldots& W_a^{A,A},& W_x^{A,1}& \\ldots& \\ldots& W_x^{A,D} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "or\n",
    "$$\n",
    "\\mathbf W_a = (W_a | W_x)\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\mathbf W_y = W_y\n",
    "$$\n",
    "\n",
    "then the relations can be rewritten\n",
    "$$\n",
    "a^{<t>} = g_a (\\mathbf W_{a}[a^{<t-1>}, x^{<t>}] + b_a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat y^{<t>} = g_y(\\mathbf W_{y}a^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "Therefor, $\\mathbf W_a$ is an $A\\times (A+D)$-matrix, $\\mathbf W_y$ is a $K \\times A$-matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 The Loss Function\n",
    "\n",
    "The loss function is defined as usual: for example MSE for regression, binary or categorical crossentropy for classification. In our example, we can define component loss:\n",
    "\n",
    "$$\n",
    "L^{<t>}(y^{<t>}, \\hat y^{<t>}) = -y^{<t>}\\log \\hat y^{<t>} - (1-y)^{<t>}\\log (1-\\hat y^{<t>}) \n",
    "$$\n",
    "\n",
    "and the overall loss\n",
    "$$\n",
    "L(y, \\hat y) = \\sum_{t=1}^{T_x} L^{<t>} (y^{<t>}, \\hat y^{<t>})\n",
    "$$\n",
    "\n",
    "We want to find $\\mathbf W_a, \\mathbf W_y, b_a, b_y$ so that this quantity is small. If we have a training set, we want to optimize the sum of these overall losses on that set. This can be done by different optimizers and based on **backpropagation through time**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Simple RNN\n",
    "\n",
    "Simple RNN is a simpler version of RNN where $A = K$, $\\mathbf W_y = \\mathbf {Id} $, $b_y = 0$, i.e.,\n",
    "\n",
    "$$\n",
    "a^{<t>} = g_a(\\mathbf W_{a}[a^{<t-1>}, x^{<t>}] + b_a)\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\hat y^{<t>} = g_y(a^{<t>})\n",
    "$$\n",
    "\n",
    "Usually, $g_a$ is $\\tanh$ and $g_y$ depending on the problem ($id$, $sigmoid$, $softmax$).\n",
    "\n",
    "<img src=\"F14.png\" width=600>\n",
    "\n",
    "RNN can be viewed as Simple RNN followed by a dense layer with activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Implementation\n",
    "\n",
    "The vocabulary = English alphabet = [a, b, c, $\\ldots$, z]\n",
    "\n",
    "Input: Some text sequence like `azmbnckedsafkasdjfhasdl`\n",
    "\n",
    "Name identity rule (for example): Pattern of the form 2 vowels between 2 consonants, like `baec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "VOCABULARY = string.ascii_lowercase\n",
    "VOWELS = \"aeiou\"\n",
    "CONSONANTS = \"bcdfghjklmnpqrstvxyz\"\n",
    "LENGTH_LOWER = 50\n",
    "LENGTH_UPPER = 50 + 1\n",
    "VOCABULARY_SIZE = len(VOCABULARY)\n",
    "VOWEL_SIZE = len(VOWELS)\n",
    "CONSONANT_SIZE = len(CONSONANTS)\n",
    "NUMERIZER = {'c': 0, 'v': 0, 'C': 1, 'V': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate random text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRandomText(vowelProba = 0.3):\n",
    "    randomLength = np.random.randint(LENGTH_LOWER, LENGTH_UPPER)\n",
    "    randomText = \"\"\n",
    "    randomPattern = \"\"\n",
    "    for i in range(randomLength):\n",
    "        alpha = np.random.binomial(1, vowelProba)\n",
    "        if alpha == 1:\n",
    "            randomLetterIndex = np.random.randint(0, VOWEL_SIZE)\n",
    "            randomLetter = VOWELS[randomLetterIndex]\n",
    "            randomPattern += \"v\"\n",
    "        else:\n",
    "            randomLetterIndex = np.random.randint(0, CONSONANT_SIZE)\n",
    "            randomLetter = CONSONANTS[randomLetterIndex]\n",
    "            randomPattern += \"c\"\n",
    "        randomText += randomLetter\n",
    "    return randomText, randomPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qetcryinsmbeeuatlfxgpvmbiimgzatknbcfrjokpgrabunaui',\n",
       " 'cvccccvccccvvvvcccccccccvvcccvccccccccvccccvcvcvvv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, pattern = generateRandomText()\n",
    "text, pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get output (1 for name entity, 0 for others)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNameEntityOutput(pattern):\n",
    "    newPattern = pattern.replace('cvvc', 'CVVC')\n",
    "    newPattern2 = newPattern.replace('CvvC', 'CVVC').replace('Cvvc', 'CVVC')\n",
    "    while newPattern2 != newPattern:\n",
    "        newPattern = newPattern2\n",
    "        newPattern2 = newPattern.replace('CvvC', 'CVVC').replace('Cvvc', 'CVVC')\n",
    "    Y = []\n",
    "    for i in range(len(newPattern2)):\n",
    "        Y.append(NUMERIZER[newPattern2[i]])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "output = getNameEntityOutput(pattern)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCorpus_1(size = 1000):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(size):\n",
    "        text, pattern = generateRandomText()\n",
    "        inputs.append(text)\n",
    "        outputs.append(getNameEntityOutput(pattern))\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dkbiahvrcikzsptehlxrrndbpczaynorezntspfsdbaenaxsfj', 'rjymvatuuvtsyqqqkqvihpbkiuzjdneaqpvzfiuvdmgeoehsnv', 'enjgoaxbjhyyvpiaikcmensxvqvyqbtnhaqotrkdiviaezofev', 'bijnoukiikuzjhmujyxrtupjotrnkuobysbduypqbedkryuecj', 'jeeufdialaasgajbjkfdziibsuaygbncheihgbfqvhpckcogyo', 'fcpaaxjezncjarztazakypczxkyorobcmzlupnytoirctxdlvv', 'opedhocxgmkbyeedohnxkaflmogqyipzbizpmfmciglyfpmurp', 'duajzehikuogcffuzpuougfyivyarbclyxjhdgrzasknritprf', 'ghrmafamftlcfmlueebcitfknulifntnutotxtuqcpomljmypm', 'xieatieyshixrtcdzzoqunseuycjzbqrironqigxxkdgggiuyc']\n",
      "[[0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "X_as_text, Y = generateCorpus_1(1000)\n",
    "print(X_as_text[:10])\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode the letters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "b [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "c [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "d [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "e [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "f [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "g [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "h [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "i [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "j [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "k [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "l [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "m [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "n [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "o [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "p [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "q [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "r [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "s [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "u [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "v [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "w [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "x [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "y [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "z [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "ENCODER = {}\n",
    "\n",
    "for idx, letter in enumerate(VOCABULARY):\n",
    "    ENCODER[letter] = [0] * VOCABULARY_SIZE\n",
    "    ENCODER[letter][idx] = 1\n",
    "    \n",
    "for k, V in sorted(ENCODER.items(), key = lambda X: X[0]):\n",
    "    print(k, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode the texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeText(text):\n",
    "    result = []\n",
    "    for letter in text:\n",
    "        result.append(ENCODER[letter])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dkbiahvrcikzsptehlxrrndbpczaynorezntspfsdbaenaxsfj\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(50, 26)\n"
     ]
    }
   ],
   "source": [
    "encode_example = encodeText(X_as_text[0])\n",
    "print(X_as_text[0])\n",
    "print(encode_example)\n",
    "print(encode_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCorpus(size = 1000):\n",
    "    raw_inputs = []\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(size):\n",
    "        text, pattern = generateRandomText()\n",
    "        raw_inputs.append(text)\n",
    "        inputs.append(encodeText(text))\n",
    "        outputs.append(getNameEntityOutput(pattern))\n",
    "    return raw_inputs, np.array(inputs), np.array(outputs).reshape(len(outputs), LENGTH_LOWER, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lpcjxmacdrvzuojeodbjrkdqagacrcimjlxdqijtiyhcnhgbez', 'ysjhhkldffkorivqaxixyuvvujnaujejzfngndxsefyvvmadoe', 'yulemiolaqyiiphqsytfreudjiuuknvvoekohgssupzijsvefo', 'kebmibgaqrcxvzlofiorjuxoedzkfsidyeeexekjvzvmxbtaaf', 'bmkallqvotxeqbomyuxxoviuyejfgkozoeoogtkeokomvaxksc', 'sozamnoatupfohdtqbquomzepgfrsgbcumcfajeemtaeggzjdd', 'uoxadzeubbkaehgbvettunzoqrbposqbeseosibgmesavuzxrv', 'ovtbkinyqfsoschaofmyhejnagtooxyltxkliditledqbtdgni', 'vuyciomahobmmiohcdopugonkphsiojiauqszfexxjuvhrucqh', 'lksljutqccblqfxkdoauiuevbojcfvduedoozhoenvixosrgpy']\n",
      "['exxsoeavaqgtgvlmxmxnfqiygsitplotipqqgotsgzrugbzubs', 'xibvbuqvhtdmttmnpxqxrfnuilvupivqxxooquiotypmvihmgf', 'xenknalodjbegycubktjhinapdpradcnoeprlilohcfshbgezv', 'yuetgabogeojyxhkrbsljeclthgbevlphrhvnsukjoqjfknmda', 'sviverciioqdapsbuotrxonoitoglnjbgjfvhtsozyybdiarbn', 'acdornixkmqmkuzacuutkacuqqjqsdsiyecdabnbjihhdsfkub', 'foockjxdsizbrgszmyigtkhxnuadhvxntbpclhqgvaionaorol', 'qyqarajadybnucmolyiavvnerrtsuqftvnensctvhujklezkip', 'zxvkipephfclpiacmrvedunezuuemgkhheecikkclnryrpfusx', 'pajzvtiuilbgorjvifrbikdpvkctotafztpflljaudxqzekolo']\n",
      "Training set shape:\n",
      "(10000, 50, 26)\n",
      "(10000, 50, 1)\n",
      "Test set shape:\n",
      "(2000, 50, 26)\n",
      "(2000, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "X_as_text, X, Y = generateCorpus(10000)\n",
    "X_test_as_text, X_test, Y_test = generateCorpus(2000)\n",
    "print(X_as_text[:10])\n",
    "print(X_test_as_text[:10])\n",
    "print(\"Training set shape:\")\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(\"Test set shape:\")\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndoannguyen\\AppData\\Local\\Continuum\\miniconda3\\envs\\py35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, None, 256)         72448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 1)           257       \n",
      "=================================================================\n",
      "Total params: 72,705\n",
      "Trainable params: 72,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, SimpleRNN\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "input_shape = (None, VOCABULARY_SIZE)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units = 256, input_shape = input_shape, activation='tanh', return_sequences = True))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: install pydot and graphviz first\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Programs/release/bin'\n",
    "\n",
    "plot_model(model, to_file='F13_2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"F13_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper\n",
    "\n",
    "#precision = as_keras_metric(tf.metrics.precision)\n",
    "#recall = as_keras_metric(tf.metrics.recall)\n",
    "auc = as_keras_metric(tf.metrics.auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "mySGD = SGD(lr = 0.1, momentum = 0.9)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = mySGD, metrics = [\"accuracy\", auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      " - 6s - loss: 0.4217 - acc: 0.8231 - auc: 0.5932 - val_loss: 0.3865 - val_acc: 0.8221 - val_auc: 0.6926\n",
      "Epoch 2/50\n",
      " - 7s - loss: 0.3900 - acc: 0.8213 - auc: 0.7165 - val_loss: 0.3854 - val_acc: 0.8181 - val_auc: 0.7302\n",
      "Epoch 3/50\n",
      " - 7s - loss: 0.3881 - acc: 0.8217 - auc: 0.7371 - val_loss: 0.3842 - val_acc: 0.8256 - val_auc: 0.7420\n",
      "Epoch 4/50\n",
      " - 8s - loss: 0.3866 - acc: 0.8227 - auc: 0.7454 - val_loss: 0.3871 - val_acc: 0.8273 - val_auc: 0.7479\n",
      "Epoch 5/50\n",
      " - 7s - loss: 0.3850 - acc: 0.8249 - auc: 0.7498 - val_loss: 0.3828 - val_acc: 0.8248 - val_auc: 0.7516\n",
      "Epoch 6/50\n",
      " - 7s - loss: 0.3836 - acc: 0.8272 - auc: 0.7530 - val_loss: 0.3805 - val_acc: 0.8301 - val_auc: 0.7543\n",
      "Epoch 7/50\n",
      " - 7s - loss: 0.3805 - acc: 0.8279 - auc: 0.7554 - val_loss: 0.3759 - val_acc: 0.8335 - val_auc: 0.7566\n",
      "Epoch 8/50\n",
      " - 7s - loss: 0.3747 - acc: 0.8315 - auc: 0.7578 - val_loss: 0.3697 - val_acc: 0.8303 - val_auc: 0.7591\n",
      "Epoch 9/50\n",
      " - 7s - loss: 0.3648 - acc: 0.8452 - auc: 0.7604 - val_loss: 0.3561 - val_acc: 0.8600 - val_auc: 0.7620\n",
      "Epoch 10/50\n",
      " - 8s - loss: 0.3511 - acc: 0.8686 - auc: 0.7639 - val_loss: 0.3450 - val_acc: 0.8632 - val_auc: 0.7658\n",
      "Epoch 11/50\n",
      " - 7s - loss: 0.3382 - acc: 0.8769 - auc: 0.7677 - val_loss: 0.3315 - val_acc: 0.8792 - val_auc: 0.7697\n",
      "Epoch 12/50\n",
      " - 7s - loss: 0.3264 - acc: 0.8828 - auc: 0.7717 - val_loss: 0.3223 - val_acc: 0.8872 - val_auc: 0.7736\n",
      "Epoch 13/50\n",
      " - 7s - loss: 0.3194 - acc: 0.8901 - auc: 0.7754 - val_loss: 0.3237 - val_acc: 0.8908 - val_auc: 0.7771\n",
      "Epoch 14/50\n",
      " - 7s - loss: 0.3126 - acc: 0.8968 - auc: 0.7788 - val_loss: 0.3136 - val_acc: 0.8985 - val_auc: 0.7805\n",
      "Epoch 15/50\n",
      " - 7s - loss: 0.3100 - acc: 0.8984 - auc: 0.7821 - val_loss: 0.3083 - val_acc: 0.9004 - val_auc: 0.7835\n",
      "Epoch 16/50\n",
      " - 7s - loss: 0.3036 - acc: 0.8998 - auc: 0.7851 - val_loss: 0.3028 - val_acc: 0.9007 - val_auc: 0.7866\n",
      "Epoch 17/50\n",
      " - 7s - loss: 0.3002 - acc: 0.8997 - auc: 0.7880 - val_loss: 0.3004 - val_acc: 0.9000 - val_auc: 0.7894\n",
      "Epoch 18/50\n",
      " - 7s - loss: 0.2981 - acc: 0.8997 - auc: 0.7907 - val_loss: 0.3013 - val_acc: 0.9001 - val_auc: 0.7919\n",
      "Epoch 19/50\n",
      " - 7s - loss: 0.2966 - acc: 0.8998 - auc: 0.7930 - val_loss: 0.2986 - val_acc: 0.8991 - val_auc: 0.7942\n",
      "Epoch 20/50\n",
      " - 8s - loss: 0.2943 - acc: 0.8998 - auc: 0.7954 - val_loss: 0.2990 - val_acc: 0.8993 - val_auc: 0.7964\n",
      "Epoch 21/50\n",
      " - 7s - loss: 0.2926 - acc: 0.8999 - auc: 0.7975 - val_loss: 0.2982 - val_acc: 0.9002 - val_auc: 0.7985\n",
      "Epoch 22/50\n",
      " - 7s - loss: 0.2918 - acc: 0.9000 - auc: 0.7996 - val_loss: 0.3018 - val_acc: 0.9004 - val_auc: 0.8005\n",
      "Epoch 23/50\n",
      " - 7s - loss: 0.2918 - acc: 0.8998 - auc: 0.8014 - val_loss: 0.2971 - val_acc: 0.8998 - val_auc: 0.8023\n",
      "Epoch 24/50\n",
      " - 7s - loss: 0.2892 - acc: 0.9000 - auc: 0.8032 - val_loss: 0.2957 - val_acc: 0.9006 - val_auc: 0.8041\n",
      "Epoch 25/50\n",
      " - 7s - loss: 0.2893 - acc: 0.9000 - auc: 0.8049 - val_loss: 0.3002 - val_acc: 0.8985 - val_auc: 0.8057\n",
      "Epoch 26/50\n",
      " - 7s - loss: 0.2880 - acc: 0.9000 - auc: 0.8065 - val_loss: 0.2964 - val_acc: 0.8992 - val_auc: 0.8072\n",
      "Epoch 27/50\n",
      " - 7s - loss: 0.2869 - acc: 0.9000 - auc: 0.8080 - val_loss: 0.2973 - val_acc: 0.8988 - val_auc: 0.8087\n",
      "Epoch 28/50\n",
      " - 7s - loss: 0.2869 - acc: 0.9001 - auc: 0.8094 - val_loss: 0.2962 - val_acc: 0.8996 - val_auc: 0.8101\n",
      "Epoch 29/50\n",
      " - 7s - loss: 0.2861 - acc: 0.9001 - auc: 0.8108 - val_loss: 0.2958 - val_acc: 0.9005 - val_auc: 0.8115\n",
      "Epoch 30/50\n",
      " - 7s - loss: 0.2848 - acc: 0.9002 - auc: 0.8122 - val_loss: 0.2981 - val_acc: 0.8986 - val_auc: 0.8128\n",
      "Epoch 31/50\n",
      " - 7s - loss: 0.2845 - acc: 0.9004 - auc: 0.8134 - val_loss: 0.3030 - val_acc: 0.8946 - val_auc: 0.8140\n",
      "Epoch 32/50\n",
      " - 7s - loss: 0.2827 - acc: 0.9004 - auc: 0.8146 - val_loss: 0.2998 - val_acc: 0.8995 - val_auc: 0.8152\n",
      "Epoch 33/50\n",
      " - 7s - loss: 0.2827 - acc: 0.9004 - auc: 0.8157 - val_loss: 0.2987 - val_acc: 0.8988 - val_auc: 0.8163\n",
      "Epoch 34/50\n",
      " - 7s - loss: 0.2815 - acc: 0.9007 - auc: 0.8169 - val_loss: 0.2998 - val_acc: 0.8983 - val_auc: 0.8174\n",
      "Epoch 35/50\n",
      " - 7s - loss: 0.2792 - acc: 0.9009 - auc: 0.8180 - val_loss: 0.3001 - val_acc: 0.8988 - val_auc: 0.8186\n",
      "Epoch 36/50\n",
      " - 7s - loss: 0.2811 - acc: 0.9007 - auc: 0.8191 - val_loss: 0.3001 - val_acc: 0.8981 - val_auc: 0.8195\n",
      "Epoch 37/50\n",
      " - 8s - loss: 0.2792 - acc: 0.9008 - auc: 0.8201 - val_loss: 0.3033 - val_acc: 0.8980 - val_auc: 0.8205\n",
      "Epoch 38/50\n",
      " - 8s - loss: 0.2769 - acc: 0.9013 - auc: 0.8211 - val_loss: 0.3015 - val_acc: 0.8980 - val_auc: 0.8216\n",
      "Epoch 39/50\n",
      " - 8s - loss: 0.2769 - acc: 0.9013 - auc: 0.8221 - val_loss: 0.3042 - val_acc: 0.8963 - val_auc: 0.8225\n",
      "Epoch 40/50\n",
      " - 8s - loss: 0.2764 - acc: 0.9012 - auc: 0.8230 - val_loss: 0.3092 - val_acc: 0.8944 - val_auc: 0.8234\n",
      "Epoch 41/50\n",
      " - 8s - loss: 0.2739 - acc: 0.9018 - auc: 0.8239 - val_loss: 0.3085 - val_acc: 0.8950 - val_auc: 0.8244\n",
      "Epoch 42/50\n",
      " - 8s - loss: 0.2744 - acc: 0.9019 - auc: 0.8248 - val_loss: 0.3062 - val_acc: 0.8960 - val_auc: 0.8252\n",
      "Epoch 43/50\n",
      " - 8s - loss: 0.2725 - acc: 0.9020 - auc: 0.8257 - val_loss: 0.3136 - val_acc: 0.8933 - val_auc: 0.8261\n",
      "Epoch 44/50\n",
      " - 8s - loss: 0.2713 - acc: 0.9026 - auc: 0.8265 - val_loss: 0.3128 - val_acc: 0.8924 - val_auc: 0.8270\n",
      "Epoch 45/50\n",
      " - 8s - loss: 0.2695 - acc: 0.9030 - auc: 0.8274 - val_loss: 0.3102 - val_acc: 0.8949 - val_auc: 0.8278\n",
      "Epoch 46/50\n",
      " - 8s - loss: 0.2687 - acc: 0.9030 - auc: 0.8283 - val_loss: 0.3135 - val_acc: 0.8943 - val_auc: 0.8287\n",
      "Epoch 47/50\n",
      " - 8s - loss: 0.2693 - acc: 0.9029 - auc: 0.8291 - val_loss: 0.3138 - val_acc: 0.8929 - val_auc: 0.8295\n",
      "Epoch 48/50\n",
      " - 8s - loss: 0.2675 - acc: 0.9031 - auc: 0.8299 - val_loss: 0.3198 - val_acc: 0.8913 - val_auc: 0.8302\n",
      "Epoch 49/50\n",
      " - 8s - loss: 0.2669 - acc: 0.9033 - auc: 0.8306 - val_loss: 0.3157 - val_acc: 0.8931 - val_auc: 0.8310\n",
      "Epoch 50/50\n",
      " - 8s - loss: 0.2643 - acc: 0.9043 - auc: 0.8314 - val_loss: 0.3146 - val_acc: 0.8920 - val_auc: 0.8318\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, epochs = 50, batch_size = 128, verbose = 2, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 668us/step\n",
      "[0.31134568071365354, 0.8941700048446656, 0.8317164134979248]\n"
     ]
    }
   ],
   "source": [
    "score_test = model.evaluate(X_test, Y_test)\n",
    "print(score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exxsoeavaqgtgvlmxmxnfqiygsitplotipqqgotsgzrugbzubs', 'xibvbuqvhtdmttmnpxqxrfnuilvupivqxxooquiotypmvihmgf', 'xenknalodjbegycubktjhinapdpradcnoeprlilohcfshbgezv', 'yuetgabogeojyxhkrbsljeclthgbevlphrhvnsukjoqjfknmda', 'sviverciioqdapsbuotrxonoitoglnjbgjfvhtsozyybdiarbn', 'acdornixkmqmkuzacuutkacuqqjqsdsiyecdabnbjihhdsfkub', 'foockjxdsizbrgszmyigtkhxnuadhvxntbpclhqgvaionaorol', 'qyqarajadybnucmolyiavvnerrtsuqftvnensctvhujklezkip', 'zxvkipephfclpiacmrvedunezuuemgkhheecikkclnryrpfusx', 'pajzvtiuilbgorjvifrbikdpvkctotafztpflljaudxqzekolo']\n",
      "Prediction as probability for first test data:\n",
      "[0.00662487 0.04238252 0.07605126 0.07756739 0.2867158  0.81349176\n",
      " 0.01670473 0.03427134 0.05120999 0.13458757 0.10910339 0.20737395\n",
      " 0.11546207 0.04964935 0.08184267 0.04454813 0.0252463  0.08957209\n",
      " 0.05117536 0.01705522 0.05111629 0.11633451 0.3681083  0.03131708\n",
      " 0.05376703 0.04352481 0.2444832  0.05293031 0.08188481 0.06153117\n",
      " 0.3853891  0.0195616  0.02128983 0.00500048 0.27767462 0.04827879\n",
      " 0.10354298 0.487492   0.03753908 0.00833212 0.01123059 0.10124528\n",
      " 0.04086285 0.13648252 0.05736223 0.07169726 0.01181587 0.05894317\n",
      " 0.00339317 0.00141321]\n",
      "True and predicted values\n"
     ]
    }
   ],
   "source": [
    "Y_pred_proba = model.predict(X_test)\n",
    "Y_pred = model.predict_classes(X_test)\n",
    "print(X_test_as_text[:10])\n",
    "print(\"Prediction as probability for first test data:\")\n",
    "print(Y_pred_proba[0].reshape(Y_test.shape[1]))\n",
    "print(\"True and predicted values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "1:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "2:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "3:\n",
      "[1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "4:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0]\n",
      "5:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "6:\n",
      "[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      "[0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "7:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "8:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "9:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for i, (trueVal, predVal) in enumerate(zip(Y_test[:10], Y_pred[:10])):\n",
    "    print(str(i) + \":\")\n",
    "    print(trueVal.reshape(50))\n",
    "    print(predVal.reshape(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapt probability threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes_with_threshold(model, X, threshold = 0.5):\n",
    "    Y_proba = model.predict(X)\n",
    "    return np.ceil(Y_proba - threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "1:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "2:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "3:\n",
      "[1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "4:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0]\n",
      "5:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "6:\n",
      "[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      "[0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0]\n",
      "7:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "8:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "9:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Y_pred_new = predict_classes_with_threshold(model, X_test, 0.33)\n",
    "for i, (trueVal, predVal) in enumerate(zip(Y_test[:10], Y_pred_new[:10])):\n",
    "    print(str(i) + \":\")\n",
    "    print(trueVal.reshape(50))\n",
    "    print(predVal.reshape(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe**\n",
    "\n",
    "- Somehow, the model has learnt the rules \"after 0,1,1, it should be 1; after 0,1,1,1, it should be 1.\"\n",
    "- If the model can learn the rules \"before 1,1,0, it should be 1; before 1,1,1,0, it should be 1\", it would be a better success. This motivates us to consider a model that can learn \"from future\". We will talk later about bidirectional RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Other Examples\n",
    "\n",
    "## 2.1 Text Generation\n",
    "\n",
    "Input: of length $T_x = 0$ or 1.\n",
    "\n",
    "Output: of some length $T_y > 0$\n",
    "\n",
    "This is an example of **one-to-many** models.\n",
    "\n",
    "<img src=\"F6.png\" width=600>\n",
    "\n",
    "## 2.2 Sentiment Classification\n",
    "\n",
    "Input: of length $T_x > 0$\n",
    "\n",
    "Output: a real number for example, so $T_y = 1$; or categorical variable, so $T_y = K$, a constant.\n",
    "\n",
    "This is an example of **many-to-one** models.\n",
    "\n",
    "<img src=\"F4.png\" width=600>\n",
    "\n",
    "## 2.3 Machine Translation\n",
    "\n",
    "In general, $T_x \\neq T_y$.\n",
    "\n",
    "This is an example of **many-to-many** models, like in name entity recognition model.\n",
    "\n",
    "The model can be described as follows:\n",
    "\n",
    "<img src=\"F7.png\" width=600>\n",
    "\n",
    "This is known as **encoder-decoder** model. The first \"half\" is called **encoder** while second half is **decoder**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Another Example: Language Modelling\n",
    "\n",
    "## 3.1 Notion\n",
    "\n",
    "Language modelling aims to build a probability distribution of meeting some combination of words. For example, given some text \"The Earth rotates around the\", what the next word should be. After a model is trained, it can predict something like:\n",
    "<center>\n",
    "$\n",
    "P($ `Sun` | `The Earth rotates around the` $) = 0.64$ \n",
    "</center>\n",
    "\n",
    "while\n",
    "<center>\n",
    "$\n",
    "P($ `son` | `The Earth rotates around the` $) = 1e-4$ \n",
    "</center>    \n",
    "\n",
    "In application, for **speech to text** problem, if we hear some sound like \"sun\" or \"son\" after \"The Earth rotates around ...\", the model helps us to choose the better word associated with this context. In our example, \"sun\" is the better choice.\n",
    "\n",
    "In general, language modelling answers the question: given a sequence $y^{<1>}, \\ldots, y^{<T_y>}$, what is the probability of meeting $y^{<1>}, \\ldots, y^{<T_y>}$ consecutively?\n",
    "\n",
    "## 3.2 Model\n",
    "\n",
    "- Training set: Large corpus of text in a language\n",
    "- We build a model to predict the probability of first word. Let $a_1$ is of dimension A, $y_1$ of dimension $D$ (size of vocabulary). We have\n",
    "$$\n",
    "a_1 = g_a(b_a)\n",
    "$$\n",
    "$$\n",
    "y_1 = g_y(\\mathbf W_y a_1 + b_y)\n",
    "$$\n",
    "\n",
    "This corresponds to the following model with $a_0 = 0, x_1 = 0$.\n",
    "\n",
    "<img src=\"F9.png\" width=200>\n",
    "\n",
    "- Choose $g_y$ is the softmax function. This model can be used to determine the probability of words in the vocabulary.\n",
    "- To predict the next words, we use $x^{<t>} = y^{<t-1>}$:\n",
    "\n",
    "<img src=\"F11.png\" width=600>\n",
    "\n",
    "- Using softmax as $g_y$ at each $t$, the prediction becomes a probability distribution of\n",
    "$$\n",
    "\\mathbf P(\\cdot | y^{<1>}, \\ldots, y^{<t-1>})\n",
    "$$\n",
    "\n",
    "- The loss function for each component\n",
    "\n",
    "$$\n",
    "L^{<t>}(y^{<t>}, \\hat y^{<t>}) = -y_j^{<t>}\\log \\hat y_j^{<t>} \n",
    "$$\n",
    "\n",
    "where $j=1, \\ldots, D$ denotes the coordinate indices of the $D-$dimensional vectors $y^{<t>}$.\n",
    "\n",
    "- The overall loss function\n",
    "$$\n",
    "L(y, \\hat y) = \\sum_{t=1}^{T_y} L^{<t>} (y^{<t>}, \\hat y^{<t>})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Special Cases\n",
    "\n",
    "- Punctuation: In lots of situations, punctuations are treated as words. They are included in the vocabulary.\n",
    "\n",
    "- Unknown words: Some unknown words may be found in the text although they may be not included in the vocabulary. We can treat them as a special word \"\\<UNKNOWN\\>\" or sometimes use another solution to treat them as a category.\n",
    "    \n",
    "**Example:** \"Barnaby Marmaduke sets world record.\" -> \"<UNK> <UNK> sets world record <PUNC>\" or \"<NAME_ENTITY> <NAME_ENTITY> sets world record <PUNC>\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Sampling a Sequence from the Trained Model\n",
    "\n",
    "- Use the probabilities output by the RNN to randomly sample a chosen word for that time-step as $\\hat{y}^{<t>} $\n",
    "- Then pass this selected word to the next time-step.\n",
    "\n",
    "<img src=\"F12.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Implementation\n",
    "\n",
    "We will play with Vietnamese.\n",
    "\n",
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " 'tên họ của đao ba khách rất quái dị gọi là cung thần xuân nghe nói y tinh thông hơn mười loại binh khí khác nhau tình hình thực tế ra sao trừ phi gặp được người đã quá chiêu động thủ với y bằng không e là không thể khảo cứu được từ tử lăng thầm nhủ trong số mặt nạ mà lỗ diệu tử làm ra đã có một tấm ',\n",
       " 300)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "GOOD_LEN = 300\n",
    "DATA_SIZE = 10000\n",
    "VIETNAMESE_LETTERS = '[^a-zđăâêôơưàằầèềìòồờùừỳáắấéếíóốớúứýảẳẩẻểỉỏổởủửỷãẵẫẽễĩõỗỡũữỹạặậẹệịọộợụựỵ ]+'\n",
    "\n",
    "df = pd.read_csv(\"VN.csv\", sep=\"\\t\\t\\t\\t\", header=None, engine='python', encoding = 'utf-8').values\n",
    "counter = 0\n",
    "lines = []\n",
    "while len(lines) < DATA_SIZE:\n",
    "    textLine = df[counter][0].lower()\n",
    "    textLine = re.sub(VIETNAMESE_LETTERS, '', textLine)\n",
    "    if len(textLine) >= GOOD_LEN:\n",
    "        lines.append(textLine[:GOOD_LEN])\n",
    "    counter += 1\n",
    "        \n",
    "len(lines), lines[35], len(lines[35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data: Encode $y$ as integer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "vocabulary_list = []\n",
    "counter = 0\n",
    "lines_encode_int = []\n",
    "\n",
    "for line in lines:\n",
    "    line_encode_int = []\n",
    "    for char in line:\n",
    "        if char not in vocabulary:\n",
    "            vocabulary[char] = counter\n",
    "            vocabulary_list.append(char)\n",
    "            counter += 1\n",
    "        line_encode_int.append(vocabulary[char])\n",
    "    lines_encode_int.append(line_encode_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ý': 49, 'ẹ': 86, 'ẽ': 81, 'z': 93, 'f': 89, 'ú': 68, 'ỳ': 80, 'ỹ': 87, 'ỗ': 47, 'ồ': 59, 'ắ': 44, 'ỉ': 67, 'đ': 17, 'ợ': 57, 'ử': 64, 'ọ': 30, 'ằ': 78, 'x': 55, 'c': 12, 'm': 29, 'r': 5, 'ề': 15, 'i': 11, 'ổ': 56, 'â': 48, 't': 4, 'ủ': 39, 'a': 16, 'ì': 6, 'k': 26, 'ỵ': 91, 'n': 7, 'ữ': 46, 'ỡ': 63, 'ơ': 61, 'ă': 34, 'ẩ': 74, 'v': 50, 'e': 69, 'ế': 58, 'ộ': 51, 'j': 92, 'ệ': 54, 'w': 88, 'ò': 71, 'é': 84, 'à': 21, 'ã': 65, 'l': 20, 'ù': 60, 'y': 14, 'ả': 40, 'ấ': 38, 'ự': 13, 'ẻ': 82, 'ĩ': 72, 'd': 23, 'õ': 77, 'ẫ': 66, 'ẵ': 90, 'g': 31, 'ố': 52, 'ư': 32, 'u': 1, 'è': 73, 'ở': 42, 'o': 19, 'ỷ': 85, 's': 41, 'ớ': 25, 'á': 2, 'ể': 36, 'ũ': 83, 'p': 10, 'ậ': 9, 'ạ': 18, 'í': 75, 'ễ': 24, ' ': 3, 'b': 43, 'ỏ': 79, 'ó': 27, 'ê': 35, 'ứ': 37, 'ị': 62, 'ừ': 22, 'ờ': 33, 'ụ': 45, 'h': 8, 'ẳ': 70, 'ặ': 76, 'ầ': 28, 'ô': 53, 'q': 0}\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)\n",
    "vocalen = len(vocabulary)\n",
    "print(vocalen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 35, 7, 3, 8, 30, 3, 12, 39, 16, 3, 17, 16, 19, 3, 43, 16, 3, 26, 8, 2, 12, 8, 3, 5, 38, 4, 3, 0, 1, 2, 11, 3, 23, 62, 3, 31, 30, 11, 3, 20, 21, 3, 12, 1, 7, 31, 3, 4, 8, 28, 7, 3, 55, 1, 48, 7, 3, 7, 31, 8, 69, 3, 7, 27, 11, 3, 14, 3, 4, 11, 7, 8, 3, 4, 8, 53, 7, 31, 3, 8, 61, 7, 3, 29, 32, 33, 11, 3, 20, 19, 18, 11, 3, 43, 11, 7, 8, 3, 26, 8, 75, 3, 26, 8, 2, 12, 3, 7, 8, 16, 1, 3, 4, 6, 7, 8, 3, 8, 6, 7, 8, 3, 4, 8, 13, 12, 3, 4, 58, 3, 5, 16, 3, 41, 16, 19, 3, 4, 5, 22, 3, 10, 8, 11, 3, 31, 76, 10, 3, 17, 32, 57, 12, 3, 7, 31, 32, 33, 11, 3, 17, 65, 3, 0, 1, 2, 3, 12, 8, 11, 35, 1, 3, 17, 51, 7, 31, 3, 4, 8, 39, 3, 50, 25, 11, 3, 14, 3, 43, 78, 7, 31, 3, 26, 8, 53, 7, 31, 3, 69, 3, 20, 21, 3, 26, 8, 53, 7, 31, 3, 4, 8, 36, 3, 26, 8, 40, 19, 3, 12, 37, 1, 3, 17, 32, 57, 12, 3, 4, 22, 3, 4, 64, 3, 20, 34, 7, 31, 3, 4, 8, 28, 29, 3, 7, 8, 39, 3, 4, 5, 19, 7, 31, 3, 41, 52, 3, 29, 76, 4, 3, 7, 18, 3, 29, 21, 3, 20, 47, 3, 23, 11, 54, 1, 3, 4, 64, 3, 20, 21, 29, 3, 5, 16, 3, 17, 65, 3, 12, 27, 3, 29, 51, 4, 3, 4, 38, 29, 3]\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(lines_encode_int[35])\n",
    "print(len(lines_encode_int[35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "maxlen = max([len(line) for line in lines_encode_int])\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data: Encode $y$ as vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300, 94)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_encode_vector = np.zeros((DATA_SIZE, GOOD_LEN, vocalen))\n",
    "for i, line in enumerate(lines_encode_int):\n",
    "    for j, char in enumerate(line):\n",
    "        lines_encode_vector[i][j][char] = 1\n",
    "        \n",
    "lines_encode_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_encode_vector[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_encode_vector[35][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decodeChar(char):\n",
    "    \"\"\"\n",
    "        Char is a vector\n",
    "    \"\"\"\n",
    "    return vocabulary_list[np.argmax(char)]\n",
    "\n",
    "decodeChar([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tên họ của đao ba khách rất quái dị gọi là cung thần xuân nghe nói y tinh thông hơn mười loại binh khí khác nhau tình hình thực tế ra sao trừ phi gặp được người đã quá chiêu động thủ với y bằng không e là không thể khảo cứu được từ tử lăng thầm nhủ trong số mặt nạ mà lỗ diệu tử làm ra đã có một tấm '"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decodeLine(line):\n",
    "    return \"\".join(decodeChar(char) for char in line)\n",
    "\n",
    "decodeLine(lines_encode_vector[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quá trình'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decodeInt(line):\n",
    "    return \"\".join([vocabulary_list[charInt] for charInt in line])\n",
    "\n",
    "decodeInt([0, 1, 2, 3, 4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (None, 300, 256)          89856     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300, 94)           24158     \n",
      "=================================================================\n",
      "Total params: 114,014\n",
      "Trainable params: 114,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, SimpleRNN\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "input_shape_2 = (GOOD_LEN, vocalen)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(SimpleRNN(units = 256, input_shape = input_shape_2, activation='tanh', return_sequences = True))\n",
    "model2.add(Dense(units = vocalen, activation = 'softmax'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "mySGD = SGD(lr = 0.2, momentum = 0.99)\n",
    "model2.compile(loss = \"binary_crossentropy\", optimizer = mySGD, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2 = lines_encode_vector[:]\n",
    "X2 = np.zeros(Y2.shape)\n",
    "X2[:,1:,:] = Y2[:,:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/2\n",
      "9000/9000 [==============================] - 63s 7ms/step - loss: 0.0494 - acc: 0.9893 - val_loss: 0.0455 - val_acc: 0.9893\n",
      "Epoch 2/2\n",
      "9000/9000 [==============================] - 69s 8ms/step - loss: 0.0439 - acc: 0.9894 - val_loss: 0.0418 - val_acc: 0.9900\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X2, Y2, epochs = 2, batch_size = 128, verbose = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00861543 0.01550654 0.01278425 0.06880084 0.0234017 ]\n",
      "  [0.00249593 0.01863731 0.01107734 0.32687682 0.04443014]\n",
      "  [0.00120255 0.02026371 0.00577814 0.3738823  0.04816699]\n",
      "  ...\n",
      "  [0.00078103 0.00605488 0.00247188 0.7388783  0.01416358]\n",
      "  [0.00349976 0.0124269  0.00795586 0.07511744 0.13161837]\n",
      "  [0.00390961 0.02486826 0.00817277 0.06053606 0.03643496]]]\n"
     ]
    }
   ],
   "source": [
    "Y2_pred = model2.predict(X2[:1])\n",
    "print(Y2_pred[:,:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00861543 0.01550654 0.01278425 0.06880084 0.0234017 ]\n",
      "  [0.00249593 0.01863731 0.01107734 0.32687682 0.04443014]\n",
      "  [0.00120255 0.02026371 0.00577814 0.3738823  0.04816699]\n",
      "  ...\n",
      "  [0.00078103 0.00605488 0.00247188 0.7388783  0.01416358]\n",
      "  [0.00349976 0.0124269  0.00795586 0.07511744 0.13161837]\n",
      "  [0.00390961 0.02486826 0.00817277 0.06053606 0.03643496]]]\n"
     ]
    }
   ],
   "source": [
    "Y2_pred = model2.predict(X2[:1])\n",
    "print(Y2_pred[:,:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94, 256), (256, 256), (256,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wax, Waa, ba = model2.get_layer(index=0).get_weights()\n",
    "Wax.shape, Waa.shape, ba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 94), (94,))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wya, by = model2.get_layer(index=1).get_weights()\n",
    "Wya.shape, by.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is done by:\n",
    "\n",
    "$$\n",
    "a^{<t>} = g_a(\\mathbf W_{a}[a^{<t-1>}, x^{<t>}] + b_a)\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\hat y^{<t>} = g_y(a^{<t>})\n",
    "$$\n",
    "\n",
    "where $g_a$ is $\\tanh$ and $g_y$ is softmax. We will verify with `Y_pred` calculated by `model.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n",
      "(94,)\n"
     ]
    }
   ],
   "source": [
    "new_a = np.tanh(np.dot(Wax.T, X2[0,0]) + np.dot(Waa.T, np.zeros(256)) + ba)\n",
    "print(new_a.shape)\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "new_y = softmax(np.dot(Wya.T, new_a) + by)\n",
    "print(new_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`new_y` ($y^{(0)<1>}$) should be identique to `model.predict(X2)[0,0,:]` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0., -0., -0.,\n",
       "        -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        -0., -0., -0., -0.,  0., -0., -0.,  0.,  0.,  0.,  0.,  0., -0.,\n",
       "        -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0.,\n",
       "        -0., -0.,  0., -0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0.,\n",
       "        -0.,  0.,  0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(Y2_pred[:,0,:] - new_y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0., -0., -0.,  0., -0., -0., -0., -0.,  0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0.,\n",
       "        -0., -0., -0., -0., -0.,  0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0.,  0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0., -0., -0., -0.,  0.,  0., -0., -0., -0., -0.,\n",
       "        -0., -0., -0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0., -0.,\n",
       "        -0., -0., -0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_a = np.tanh(np.dot(Wax.T, X2[0,1]) + np.dot(Waa.T, new_a) + ba)\n",
    "new_y = softmax(np.dot(Wya.T, new_a) + by)\n",
    "np.round(Y2_pred[:,1,:] - new_y, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ôm ât thh ni iấờ taệả ghỉ  tả tcúâ  ại nưđq  hn t ht ó ôấqến ưện  gệư gsư g no  ég ng r ềăn trgcbmn  min  aừ  t x nto ciycgảẵ nảuhỹã g nhờc  lc ư ặưe êiếư u àrầ ạà  g  lnà ỡ l ệhimàýsuạm hn  ãn ccuảntmi ôàhư oịấ ign aà  hpnh lyh  â kớnị nô hưưnhp   hòđ đơ càlt uòỗê nếồảnhsạ cci hlwẫ toi ớhừưgwàâỡ t '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def passState(x, a_prev, Waa, Wax, Wya, ba, by):\n",
    "    a = np.tanh(np.dot(Wax.T, x) + np.dot(Waa.T, a_prev) + ba)\n",
    "    y = softmax(np.dot(Wya.T, a) + by)\n",
    "    return y, a\n",
    "\n",
    "\n",
    "a_prev = np.zeros(256)\n",
    "x = np.zeros(vocalen)\n",
    "generated_text = []\n",
    "\n",
    "np.random.seed(270)\n",
    "for t in range(GOOD_LEN):\n",
    "    y, a = passState(x, a_prev, Waa, Wax, Wya, ba, by)\n",
    "    a_prev = a[:]\n",
    "    idx = np.random.choice(range(vocalen), p = y.ravel())\n",
    "    x = np.zeros(vocalen)\n",
    "    x[idx] = 1\n",
    "    generated_text.append(vocabulary_list[idx])\n",
    "\n",
    "\"\".join(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/8\n",
      "9000/9000 [==============================] - 72s 8ms/step - loss: 0.0400 - acc: 0.9903 - val_loss: 0.0383 - val_acc: 0.9905\n",
      "Epoch 2/8\n",
      "9000/9000 [==============================] - 75s 8ms/step - loss: 0.0370 - acc: 0.9907 - val_loss: 0.0359 - val_acc: 0.9909\n",
      "Epoch 3/8\n",
      "9000/9000 [==============================] - 76s 8ms/step - loss: 0.0351 - acc: 0.9909 - val_loss: 0.0346 - val_acc: 0.9910\n",
      "Epoch 4/8\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0340 - acc: 0.9911 - val_loss: 0.0336 - val_acc: 0.9911\n",
      "Epoch 5/8\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0331 - acc: 0.9912 - val_loss: 0.0329 - val_acc: 0.9912\n",
      "Epoch 6/8\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0325 - acc: 0.9913 - val_loss: 0.0322 - val_acc: 0.9913\n",
      "Epoch 7/8\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0318 - acc: 0.9914 - val_loss: 0.0317 - val_acc: 0.9914\n",
      "Epoch 8/8\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0313 - acc: 0.9915 - val_loss: 0.0312 - val_acc: 0.9915\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X2, Y2, epochs = 8, batch_size = 128, verbose = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sg bnn nàng nớư nhưếc khính kring tràn cải trận đánh duợc và kệt ngệờ kăờ g nài ơê thi tay thiảng hì long cà trám nhưipng mơé nguầẽ tà hào nhưiic vữệ gaệ tu lanh ch lu đựi vry bong sổu khừ thí từn sà dà bể mìng tiôn lư thàng lànc guểi vi biich ci táng tiãu lưi qỏủ nhồệ hạà họnh nóéh thư loiềtc êẩ tr'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wax, Waa, ba = model2.get_layer(index=0).get_weights()\n",
    "Wya, by = model2.get_layer(index=1).get_weights()\n",
    "\n",
    "a_prev = np.zeros(256)\n",
    "x = np.zeros(vocalen)\n",
    "generated_text = []\n",
    "np.random.seed(270)\n",
    "\n",
    "for t in range(GOOD_LEN):\n",
    "    y, a = passState(x, a_prev, Waa, Wax, Wya, ba, by)\n",
    "    a_prev = a[:]\n",
    "    idx = np.random.choice(range(vocalen), p = y.ravel())\n",
    "    x = np.zeros(vocalen)\n",
    "    x[idx] = 1\n",
    "    generated_text.append(vocabulary_list[idx])\n",
    "\n",
    "\"\".join(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train more (2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 62s 7ms/step - loss: 0.0308 - acc: 0.9916 - val_loss: 0.0307 - val_acc: 0.9916\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 68s 8ms/step - loss: 0.0304 - acc: 0.9917 - val_loss: 0.0303 - val_acc: 0.9917\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 72s 8ms/step - loss: 0.0300 - acc: 0.9918 - val_loss: 0.0299 - val_acc: 0.9918\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0296 - acc: 0.9919 - val_loss: 0.0296 - val_acc: 0.9919\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0293 - acc: 0.9919 - val_loss: 0.0293 - val_acc: 0.9919\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0290 - acc: 0.9920 - val_loss: 0.0291 - val_acc: 0.9920\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0287 - acc: 0.9920 - val_loss: 0.0288 - val_acc: 0.9920\n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0285 - acc: 0.9921 - val_loss: 0.0286 - val_acc: 0.9921\n",
      "Epoch 9/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0282 - acc: 0.9921 - val_loss: 0.0283 - val_acc: 0.9921\n",
      "Epoch 10/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0280 - acc: 0.9922 - val_loss: 0.0281 - val_acc: 0.9921\n",
      "Epoch 11/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0278 - acc: 0.9922 - val_loss: 0.0279 - val_acc: 0.9922\n",
      "Epoch 12/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0276 - acc: 0.9923 - val_loss: 0.0277 - val_acc: 0.9922\n",
      "Epoch 13/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0274 - acc: 0.9923 - val_loss: 0.0276 - val_acc: 0.9923\n",
      "Epoch 14/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0273 - acc: 0.9923 - val_loss: 0.0274 - val_acc: 0.9923\n",
      "Epoch 15/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0271 - acc: 0.9924 - val_loss: 0.0273 - val_acc: 0.9923\n",
      "Epoch 16/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0270 - acc: 0.9924 - val_loss: 0.0272 - val_acc: 0.9923\n",
      "Epoch 17/20\n",
      "9000/9000 [==============================] - 75s 8ms/step - loss: 0.0269 - acc: 0.9924 - val_loss: 0.0270 - val_acc: 0.9924\n",
      "Epoch 18/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0268 - acc: 0.9924 - val_loss: 0.0269 - val_acc: 0.9924\n",
      "Epoch 19/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0266 - acc: 0.9925 - val_loss: 0.0268 - val_acc: 0.9924\n",
      "Epoch 20/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0265 - acc: 0.9925 - val_loss: 0.0267 - val_acc: 0.9924\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X2, Y2, epochs = 20, batch_size = 128, verbose = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mộ bia này chể mề húo doợn tầu cẽ trọn yổi tranh thi lựm qma kiou lúc kào ghi chén cể nhưbng tay khi lành đý tráy hiện phâa ịuhxu íỏ m thông chương việt ấu tiên lưi m tiên vno bàng bơ đệu nhạk nói ở hện bảnh đònh dừ đý tho làng hìn khôi nghây đắn tram thẳ lên tuòng híộh cân can cạí hiủn dại gỷ síu t'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wax, Waa, ba = model2.get_layer(index=0).get_weights()\n",
    "Wya, by = model2.get_layer(index=1).get_weights()\n",
    "\n",
    "a_prev = np.zeros(256)\n",
    "x = np.zeros(vocalen)\n",
    "generated_text = []\n",
    "np.random.seed(270)\n",
    "\n",
    "for t in range(GOOD_LEN):\n",
    "    y, a = passState(x, a_prev, Waa, Wax, Wya, ba, by)\n",
    "    a_prev = a[:]\n",
    "    idx = np.random.choice(range(vocalen), p = y.ravel())\n",
    "    x = np.zeros(vocalen)\n",
    "    x[idx] = 1\n",
    "    generated_text.append(vocabulary_list[idx])\n",
    "\n",
    "\"\".join(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train more (3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 61s 7ms/step - loss: 0.0265 - acc: 0.9925 - val_loss: 0.0266 - val_acc: 0.9924\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 67s 7ms/step - loss: 0.0264 - acc: 0.9925 - val_loss: 0.0266 - val_acc: 0.9924\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 71s 8ms/step - loss: 0.0262 - acc: 0.9925 - val_loss: 0.0264 - val_acc: 0.9925\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0260 - acc: 0.9926 - val_loss: 0.0261 - val_acc: 0.9925\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0258 - acc: 0.9926 - val_loss: 0.0260 - val_acc: 0.9926\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0256 - acc: 0.9926 - val_loss: 0.0259 - val_acc: 0.9925\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0255 - acc: 0.9927 - val_loss: 0.0256 - val_acc: 0.9926\n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0253 - acc: 0.9927 - val_loss: 0.0255 - val_acc: 0.9927\n",
      "Epoch 9/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0251 - acc: 0.9928 - val_loss: 0.0254 - val_acc: 0.9927\n",
      "Epoch 10/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0250 - acc: 0.9928 - val_loss: 0.0252 - val_acc: 0.9927\n",
      "Epoch 11/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0249 - acc: 0.9928 - val_loss: 0.0251 - val_acc: 0.9928\n",
      "Epoch 12/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0248 - acc: 0.9929 - val_loss: 0.0250 - val_acc: 0.9928\n",
      "Epoch 13/20\n",
      "9000/9000 [==============================] - 75s 8ms/step - loss: 0.0247 - acc: 0.9929 - val_loss: 0.0249 - val_acc: 0.9928\n",
      "Epoch 14/20\n",
      "9000/9000 [==============================] - 75s 8ms/step - loss: 0.0245 - acc: 0.9929 - val_loss: 0.0248 - val_acc: 0.9928\n",
      "Epoch 15/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0244 - acc: 0.9929 - val_loss: 0.0247 - val_acc: 0.9928\n",
      "Epoch 16/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0243 - acc: 0.9929 - val_loss: 0.0246 - val_acc: 0.9929\n",
      "Epoch 17/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0242 - acc: 0.9930 - val_loss: 0.0245 - val_acc: 0.9929\n",
      "Epoch 18/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0241 - acc: 0.9930 - val_loss: 0.0244 - val_acc: 0.9929\n",
      "Epoch 19/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0240 - acc: 0.9930 - val_loss: 0.0243 - val_acc: 0.9929\n",
      "Epoch 20/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0239 - acc: 0.9930 - val_loss: 0.0242 - val_acc: 0.9930\n"
     ]
    }
   ],
   "source": [
    "mySGD = SGD(lr = 0.5, momentum = 0.99)\n",
    "model2.compile(loss = \"binary_crossentropy\", optimizer = mySGD, metrics = [\"accuracy\"])\n",
    "history = model2.fit(X2, Y2, epochs = 20, batch_size = 128, verbose = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ừau ra hạng thiếu long nếu toán vẫn cạ hãi tra nà nhận bợá hiệm trương lệ không xa cả trược hay bbi tiểu thư nhìn nhất càng íu kì ồợ dianh huynh tặc rồi ấu thiệu lạng thiếu nó giọi sồu dận tìm ta tứ phó bản thúc cả tiểu thư phương ta cũng chúp cầu thèi đã động quốc nữm nhân ca thểm thên mưởng đãy nh'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wax, Waa, ba = model2.get_layer(index=0).get_weights()\n",
    "Wya, by = model2.get_layer(index=1).get_weights()\n",
    "\n",
    "a_prev = np.zeros(256)\n",
    "x = np.zeros(vocalen)\n",
    "generated_text = []\n",
    "np.random.seed(270)\n",
    "\n",
    "for t in range(GOOD_LEN):\n",
    "    y, a = passState(x, a_prev, Waa, Wax, Wya, ba, by)\n",
    "    a_prev = a[:]\n",
    "    idx = np.random.choice(range(vocalen), p = y.ravel())\n",
    "    x = np.zeros(vocalen)\n",
    "    x[idx] = 1\n",
    "    generated_text.append(vocabulary_list[idx])\n",
    "\n",
    "\"\".join(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train more (4)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 65s 7ms/step - loss: 0.0239 - acc: 0.9930 - val_loss: 0.0243 - val_acc: 0.9929\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0239 - acc: 0.9930 - val_loss: 0.0242 - val_acc: 0.9929\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0238 - acc: 0.9930 - val_loss: 0.0241 - val_acc: 0.9930\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 76s 8ms/step - loss: 0.0237 - acc: 0.9931 - val_loss: 0.0239 - val_acc: 0.9930\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0235 - acc: 0.9931 - val_loss: 0.0237 - val_acc: 0.9931\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0233 - acc: 0.9932 - val_loss: 0.0235 - val_acc: 0.9931\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0232 - acc: 0.9932 - val_loss: 0.0235 - val_acc: 0.9931\n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 75s 8ms/step - loss: 0.0231 - acc: 0.9933 - val_loss: 0.0234 - val_acc: 0.9932\n",
      "Epoch 9/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0229 - acc: 0.9933 - val_loss: 0.0232 - val_acc: 0.9932\n",
      "Epoch 10/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0228 - acc: 0.9933 - val_loss: 0.0231 - val_acc: 0.9933\n",
      "Epoch 11/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0226 - acc: 0.9934 - val_loss: 0.0229 - val_acc: 0.9933\n",
      "Epoch 12/20\n",
      "9000/9000 [==============================] - 75s 8ms/step - loss: 0.0225 - acc: 0.9934 - val_loss: 0.0228 - val_acc: 0.9933\n",
      "Epoch 13/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0224 - acc: 0.9934 - val_loss: 0.0227 - val_acc: 0.9933\n",
      "Epoch 14/20\n",
      "9000/9000 [==============================] - 73s 8ms/step - loss: 0.0223 - acc: 0.9934 - val_loss: 0.0227 - val_acc: 0.9934\n",
      "Epoch 15/20\n",
      "9000/9000 [==============================] - 76s 8ms/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.0226 - val_acc: 0.9934\n",
      "Epoch 16/20\n",
      "9000/9000 [==============================] - 77s 9ms/step - loss: 0.0222 - acc: 0.9935 - val_loss: 0.0226 - val_acc: 0.9934\n",
      "Epoch 17/20\n",
      "9000/9000 [==============================] - 75s 8ms/step - loss: 0.0221 - acc: 0.9935 - val_loss: 0.0225 - val_acc: 0.9934\n",
      "Epoch 18/20\n",
      "9000/9000 [==============================] - 74s 8ms/step - loss: 0.0221 - acc: 0.9935 - val_loss: 0.0224 - val_acc: 0.9934\n",
      "Epoch 19/20\n",
      "9000/9000 [==============================] - 78s 9ms/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.0224 - val_acc: 0.9934\n",
      "Epoch 20/20\n",
      "9000/9000 [==============================] - 78s 9ms/step - loss: 0.0219 - acc: 0.9935 - val_loss: 0.0223 - val_acc: 0.9935\n"
     ]
    }
   ],
   "source": [
    "mySGD = SGD(lr = 1, momentum = 0.999)\n",
    "model2.compile(loss = \"binary_crossentropy\", optimizer = mySGD, metrics = [\"accuracy\"])\n",
    "history = model2.fit(X2, Y2, epochs = 20, batch_size = 128, verbose = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ào sự thần hai gã đã từng trên còn thanh kì thiên hạ lại quả là thảm thân miễn tỏi của thủ nhiên dừng hôn hư trực ra phóng lét muối tay cảm thấy tính khỏi quả chân ta thiệu tự bành gã lời nhưng lừng ngọc được ý rồi tay thanh âm ra tại sao ngươi chu nơi đã đối trúch nắm chúng ta cứỉ hai tay gão mặt t'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wax, Waa, ba = model2.get_layer(index=0).get_weights()\n",
    "Wya, by = model2.get_layer(index=1).get_weights()\n",
    "\n",
    "a_prev = np.zeros(256)\n",
    "x = np.zeros(vocalen)\n",
    "generated_text = []\n",
    "np.random.seed(270)\n",
    "\n",
    "for t in range(GOOD_LEN):\n",
    "    y, a = passState(x, a_prev, Waa, Wax, Wya, ba, by)\n",
    "    a_prev = a[:]\n",
    "    idx = np.random.choice(range(vocalen), p = y.ravel())\n",
    "    x = np.zeros(vocalen)\n",
    "    x[idx] = 1\n",
    "    generated_text.append(vocabulary_list[idx])\n",
    "\n",
    "\"\".join(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some observations**\n",
    "<table>\n",
    "    <tr><th>After ... steps</th><th>Fact</th></tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Words with >7 letters exist, 2 spaces consecutive exist. Text respects letter frequency but does not seem to build correct words</td>\n",
    "    <tr>\n",
    "        <td>10</td>\n",
    "        <td>Words are mostly between 2 and 6 letters (like Vietnamese), no (rarely) 2 spaces consecutive, but few correct words</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>30</td>\n",
    "        <td>More correct words appear.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>50</td>\n",
    "        <td>More and more correct words appear, including complex combinations (\"tiểu thư\")</td>\n",
    "    <tr>\n",
    "        <td>70</td>\n",
    "        <td>Almost all words are correct. Very few incorrect words (only 1 in our example, \"trúch\"). More complex combinations appear (\"hai gã\", \"thiên hạ\", \"cảm thấy\", \"tại sao ngươi\", \"chúng ta\", \"hai tay\")</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The more we train, the longer dependency between time steps we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Variation of RNN Models\n",
    "\n",
    "## 4.1 Limitation of Basic RNN Models\n",
    "\n",
    "Long-term dependency is the dependency of an output of some timestep on inputs of far previous timesteps.\n",
    "\n",
    "<img src=\"F15.png\" width=600>\n",
    "\n",
    "If the network is very deep, the gradient from output $y$ will have a hard time propagating back to affect the weights of earlier layers because of **vanishing** and **exploding** gradients.\n",
    "\n",
    "So, it is difficult for basic RNN to learn long-term dependency. \n",
    "\n",
    "## 4.2 RNN Unit in a Basic RNN Models\n",
    "\n",
    "<img src=\"F16.png\" width=600>\n",
    "\n",
    "In a basic RNN model, we usually have a variable $a$ that changes its state after each time step. Each input $x$ influences the state of $a$ by:\n",
    "\n",
    "$$\n",
    "a_{t} = \\tanh {W_{a}a^{<t_1>} + W_{x}x^{<t>} + b_a}\n",
    "$$\n",
    "\n",
    "(here we suppose the activation used for $a$ is $\\tanh$)\n",
    "\n",
    "This was illustrated in the above figure: there is 2 arrows, one from the previous $a$, the other from the new $x$, that meet in a box, activated by the $\\tanh$ function. The output after activation is then used\n",
    "\n",
    "- for predicting $y$ at timestep $t$: $\\hat y^{<t>} = g_y(W_y a^{<t>} + b_y)$.\n",
    "\n",
    "- as the new $a$, to be transferred to timestep $t+1$.\n",
    "\n",
    "## 4.3 Gated Recurrent Unit (GRU) \n",
    "### 4.3.1 Simple version\n",
    "\n",
    "Gated Recurrent Unit is a model that is similar to basic RNN, but instead of using $a^{<t>} = \\tanh(W_a a^{<t-1>}+ W_x x^{<t>} + b_a)$ to transfer to the following timestep, it uses a combination of $a^{<t-1>}$ and this new value $\\tanh(W_a a^{<t-1>}+ W_x x^{<t>} + b_a)$.\n",
    "\n",
    "We use the notation $c^{<t>}$ to avoid confusion with basic RNN.\n",
    "\n",
    "For each timestep, GRU will have a previous value $c^{<t-1>}$ (standing for \"cell state\" or memory state). The transformation is as follows:\n",
    "\n",
    "- Calculate a candidate for the new cell state\n",
    "$$\n",
    "\\tilde c^{<t>} = \\tanh(W_c c^{<t-1>}, W_{xc} x^{<t>} + b_c)\n",
    "$$\n",
    "\n",
    "- Calculate a coefficient in [0,1] that represents the weight of $c^{<t>}$ in the next cell state\n",
    "$$\n",
    "\\Gamma_u = \\sigma(W_u c^{<t-1>}, W_{xu} x^{<t>} + b_u)\n",
    "$$\n",
    "\n",
    "$\\Gamma_u = 0$ means the candidate does not have effect on the new cell state.\n",
    "(Example, if we meet some unimportant word that does not affect sentence's grammar, current state is nothing but previous state.)\n",
    "\n",
    "$\\Gamma_u = 1$ means the past cell states does not have effect on the present cell state. (Example, if we meet a \".\" punctuation, we pass to new sentence, grammar on the last sentence does not have effect anymore on the current state.)\n",
    "\n",
    "- The new cell state is a combination of the previous cell state and the new candidate\n",
    "$$\n",
    "c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>}\n",
    "$$\n",
    "\n",
    "where * stands for point-wise multiplication.\n",
    "\n",
    "- Prediction of output is calculated as\n",
    "$$\n",
    "\\hat y^{<t>} = g_y (W_y c^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "\n",
    "**Summary**\n",
    "$$\n",
    "\\tilde c^{<t>} = \\tanh(W_c c^{<t-1>}, W_{xc} x^{<t>} + b_c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma_u = \\sigma(W_u c^{<t-1>}, W_{xu} x^{<t>} + b_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat y^{<t>} = g_y (W_y c^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Full Version\n",
    "\n",
    "In full version, we do not use $c^{<t-1>}$ directly to calculate $\\hat c^{<t>}$, but a proportion of it.\n",
    "\n",
    "$$\n",
    "\\Gamma_r = \\sigma (W_r c^{<t-1>} + W_{xr} x^{<t>} + br)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat c^{<t>} = \\tanh(W_c (\\Gamma_r * c^{<t-1>}), W_{xc} x^{<t>} + b_c)\n",
    "$$\n",
    "\n",
    "**Summary**\n",
    "$$\n",
    "\\Gamma_r = \\sigma (W_r c^{<t-1>} + W_{xr} x^{<t>} + br)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma_u = \\sigma(W_u c^{<t-1>}, W_{xu} x^{<t>} + b_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat c^{<t>} = \\tanh(W_c (\\Gamma_r * c^{<t-1>}), W_{xc} x^{<t>} + b_c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c^{<t>} = \\Gamma_u * \\tilde c^{<t>} + (1 - \\Gamma_u) * c^{<t-1>}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat y^{<t>} = g_y (W_y c^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "<img src=\"F19.png\" width=600>\n",
    "<center>\n",
    "    In this figure, $h$ -> $a$, $r$ -> $\\Gamma_r$, $z$ -> $\\Gamma_u$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Long Short-Term Memory (LSTM)\n",
    "\n",
    "In LSTM, at each timestep, we consider 2 variables:\n",
    "\n",
    "- Cell state $c^{<t>}$\n",
    "\n",
    "- Output state $a^{<t>}$\n",
    "\n",
    "In GRU, we can consider $a^{<t>} = c^{<t>}$.\n",
    "\n",
    "The updates in step $t$ are:\n",
    "\n",
    "$$\n",
    "\\Gamma_f = \\sigma(W_f a^{<t-1>} + W_{xf} x^{<t>} + b_f)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma_u = \\sigma(W_u a^{<t-1>} + W_{xu} x^{<t>} + b_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma_o = \\sigma(W_o a^{<t-1>} + W_{xo} x^{<t>} + b_o)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde {c}^{<t>} = \\tanh (W_c a^{<t-1>} + W_{xc} x^{<t>} + b_c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c^{<t>} = \\Gamma_f * c^{<t-1>} + \\Gamma_u * \\tilde c^{<t>}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{<t>} = \\Gamma_o * \\tanh c^{<t>}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat y^{<t>} = g_y (W_y a^{<t>} + b_y)\n",
    "$$\n",
    " \n",
    "Instead of 1, 2 gates in GRU (simple, full version), LSTM uses 3 gates:\n",
    "\n",
    "- The forget gate, represented by the coefficient $\\Gamma_f$. It controls the amount to forget from previous state. If $\\Gamma_f = 0$, no memory from the past is kept.\n",
    "- The update gate, represented by the coefficient $\\Gamma_u$. It gives the proportion of the candidate to contribute in $c^{<t>}$.\n",
    "- The output gate, represented by the coefficient $\\Gamma_o$. It gives the proportion of $c^{<t>}$ to output in $a^{<t>}$.\n",
    "\n",
    "**Remark**\n",
    "\n",
    "Sometimes in practice, we use **hard_sigmoid** function instead of sigmoid.\n",
    "\n",
    "<img src=\"F20.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Example - Implementation of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 300, 256)          359424    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300, 94)           24158     \n",
      "=================================================================\n",
      "Total params: 383,582\n",
      "Trainable params: 383,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, SimpleRNN, LSTM\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "input_shape_3 = (GOOD_LEN, vocalen)\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(units = 256, input_shape = input_shape_3, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, unit_forget_bias=True, return_sequences=True))\n",
    "model3.add(Dense(units = vocalen, activation = 'softmax'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 266s 30ms/step - loss: 0.0390 - acc: 0.9901 - val_loss: 0.0290 - val_acc: 0.9919\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 272s 30ms/step - loss: 0.0264 - acc: 0.9924 - val_loss: 0.0249 - val_acc: 0.9927\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 280s 31ms/step - loss: 0.0238 - acc: 0.9930 - val_loss: 0.0234 - val_acc: 0.9931\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 280s 31ms/step - loss: 0.0227 - acc: 0.9933 - val_loss: 0.0228 - val_acc: 0.9933\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 302s 34ms/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.0220 - val_acc: 0.9935\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 299s 33ms/step - loss: 0.0214 - acc: 0.9937 - val_loss: 0.0216 - val_acc: 0.9936\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 284s 32ms/step - loss: 0.0209 - acc: 0.9938 - val_loss: 0.0212 - val_acc: 0.9937\n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 279s 31ms/step - loss: 0.0205 - acc: 0.9939 - val_loss: 0.0210 - val_acc: 0.9938\n",
      "Epoch 9/20\n",
      "9000/9000 [==============================] - 289s 32ms/step - loss: 0.0202 - acc: 0.9940 - val_loss: 0.0206 - val_acc: 0.9939\n",
      "Epoch 10/20\n",
      "9000/9000 [==============================] - 277s 31ms/step - loss: 0.0199 - acc: 0.9941 - val_loss: 0.0203 - val_acc: 0.9940\n",
      "Epoch 11/20\n",
      "9000/9000 [==============================] - 278s 31ms/step - loss: 0.0197 - acc: 0.9941 - val_loss: 0.0202 - val_acc: 0.9940\n",
      "Epoch 12/20\n",
      "9000/9000 [==============================] - 289s 32ms/step - loss: 0.0194 - acc: 0.9942 - val_loss: 0.0200 - val_acc: 0.9940\n",
      "Epoch 13/20\n",
      "9000/9000 [==============================] - 279s 31ms/step - loss: 0.0192 - acc: 0.9942 - val_loss: 0.0199 - val_acc: 0.9941\n",
      "Epoch 14/20\n",
      "9000/9000 [==============================] - 279s 31ms/step - loss: 0.0191 - acc: 0.9943 - val_loss: 0.0198 - val_acc: 0.9941\n",
      "Epoch 15/20\n",
      "9000/9000 [==============================] - 297s 33ms/step - loss: 0.0189 - acc: 0.9943 - val_loss: 0.0197 - val_acc: 0.9941\n",
      "Epoch 16/20\n",
      "9000/9000 [==============================] - 294s 33ms/step - loss: 0.0188 - acc: 0.9943 - val_loss: 0.0196 - val_acc: 0.9941\n",
      "Epoch 17/20\n",
      "9000/9000 [==============================] - 275s 31ms/step - loss: 0.0187 - acc: 0.9944 - val_loss: 0.0195 - val_acc: 0.9942\n",
      "Epoch 18/20\n",
      "9000/9000 [==============================] - 272s 30ms/step - loss: 0.0185 - acc: 0.9944 - val_loss: 0.0194 - val_acc: 0.9942\n",
      "Epoch 19/20\n",
      "9000/9000 [==============================] - 293s 33ms/step - loss: 0.0185 - acc: 0.9944 - val_loss: 0.0194 - val_acc: 0.9942\n",
      "Epoch 20/20\n",
      "9000/9000 [==============================] - 295s 33ms/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0194 - val_acc: 0.9942\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "myAdam = Adam(lr = 0.02)\n",
    "model3.compile(loss = \"binary_crossentropy\", optimizer = myAdam, metrics = [\"accuracy\"])\n",
    "history3 = model3.fit(X2, Y2, epochs = 20, batch_size = 128, verbose = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 256\n",
    "\n",
    "W = model3.layers[0].get_weights()[0]\n",
    "U = model3.layers[0].get_weights()[1]\n",
    "b = model3.layers[0].get_weights()[2]\n",
    "\n",
    "W_u = W[:, :units]\n",
    "W_f = W[:, units: units * 2]\n",
    "W_c = W[:, units * 2: units * 3]\n",
    "W_o = W[:, units * 3:]\n",
    "\n",
    "U_u = U[:, :units]\n",
    "U_f = U[:, units: units * 2]\n",
    "U_c = U[:, units * 2: units * 3]\n",
    "U_o = U[:, units * 3:]\n",
    "\n",
    "b_u = b[:units]\n",
    "b_f = b[units: units * 2]\n",
    "b_c = b[units * 2: units * 3]\n",
    "b_o = b[units * 3:]\n",
    "\n",
    "W_y, b_y = model3.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def passLSTMState(x, a_prev, c_prev, W_u, W_f, W_c, W_o, U_u, U_f, U_c, U_o, b_u, b_f, b_c, b_o, W_y, b_y):\n",
    "    Gamma_f = sigmoid(np.dot(U_f.T, a_prev) + np.dot(W_f.T, x) + b_f)\n",
    "    Gamma_u = sigmoid(np.dot(U_u.T, a_prev) + np.dot(W_u.T, x) + b_u)\n",
    "    Gamma_o = sigmoid(np.dot(U_o.T, a_prev) + np.dot(W_o.T, x) + b_o)\n",
    "    c_tilde = np.tanh(np.dot(U_c.T, a_prev) + np.dot(W_c.T, x) + b_c)\n",
    "    c = Gamma_f * c_prev + Gamma_u * c_tilde\n",
    "    a = Gamma_o * np.tanh(c)\n",
    "    y = softmax(np.dot(W_y.T, a) + b_y)\n",
    "    return y, a, c, [Gamma_f, Gamma_u, Gamma_o, c_tilde]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lưu khí chàng của chúng ta hai phủ nhân khó mà xảy ra phải đối diện rất đết chiến làm cung xin cho người đồng thiết là vĩ thế nào lại vui lưu bà khiến cho ta không cảm giác gốn này thế nhưng do tinh thang đi luôn giọng nói vô độ trn đế phủ quỷ tiên sinh rằng nguồn trang nuốt đầu suy tiếp vào sách ti'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_prev = np.zeros(256)\n",
    "c_prev = np.zeros(256)\n",
    "x = np.zeros(vocalen)\n",
    "generated_text = []\n",
    "np.random.seed(123)\n",
    "\n",
    "for t in range(GOOD_LEN):\n",
    "    y, a, c, otherParams = passLSTMState(x, a_prev, c_prev, W_u, W_f, W_c, W_o, U_u, U_f, U_c, U_o, b_u, b_f, b_c, b_o, W_y, b_y)\n",
    "    a_prev = a[:]\n",
    "    c_prev = c[:]\n",
    "    idx = np.random.choice(range(vocalen), p = y.ravel())\n",
    "    x = np.zeros(vocalen)\n",
    "    x[idx] = 1\n",
    "    generated_text.append(vocabulary_list[idx])\n",
    "\n",
    "\"\".join(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save and reload model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('LSTM-after-20.steps.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "savedModel3 = load_model('LSTM-after-20.steps.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 300, 256)          359424    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300, 94)           24158     \n",
      "=================================================================\n",
      "Total params: 383,582\n",
      "Trainable params: 383,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "savedModel3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bidirectional RNN\n",
    "\n",
    "Motivation of bidirectional RNN is that an output at timestep $t$ depends not only on input of previous steps but only later steps as well, as we has seen in name entity recognition.\n",
    "\n",
    "<img src=\"F21.png\">\n",
    "\n",
    "It uses 2 type variables: forward cell states (denoted for example by $a^{<t>}$) and backward cell states (denoted by $a'^{<t>}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Example with Simple Bidirectional RNN\n",
    "\n",
    "At each timestep\n",
    "\n",
    "**For forward cell states**\n",
    "$$\n",
    "a^{<t>} = g_a (W_a a^{<t-1>} + W_{ax} x^{<t>} + b_a) \n",
    "$$\n",
    "\n",
    "**For backward cell states**\n",
    "$$\n",
    "a'^{<t>} = g_{a'} (W_{a'} a'^{<t+1>}) + W_{a'x} x^{<t>}+ b_{a'})\n",
    "$$\n",
    "\n",
    "**Prediction**\n",
    "$$\n",
    "\\hat y^{<t>} = g_y (W_{ya} a^{<t>} + W_{ya'} a'^{<t>} + b_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Example with Bidirectional LSTM\n",
    "\n",
    "<img src=\"F22.png\" width=400></img>\n",
    "\n",
    "At each timestep\n",
    "\n",
    "**Forward cell states**\n",
    "$$\n",
    "\\Gamma_f = \\sigma(W_f a^{<t-1>} + W_{xf} x^{<t>} + b_f)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma_u = \\sigma(W_u a^{<t-1>} + W_{xu} x^{<t>} + b_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma_o = \\sigma(W_o a^{<t-1>} + W_{xo} x^{<t>} + b_o)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde {c}^{<t>} = \\tanh (W_c a^{<t-1>} + W_{xc} x^{<t>} + b_c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c^{<t>} = \\Gamma_f * c^{<t-1>} + \\Gamma_u * \\tilde c^{<t>}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{<t>} = \\Gamma_o * \\tanh c^{<t>}\n",
    "$$\n",
    "\n",
    "**Backward cell states**\n",
    "\n",
    "$$\n",
    "\\Gamma'_f = \\sigma(W'_f a'^{<t-1>} + W'_{xf} x^{<t>} + b'_f)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma'_u = \\sigma(W'_u a'^{<t-1>} + W'_{xu} x^{<t>} + b'_u)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Gamma'_o = \\sigma(W'_o a'^{<t-1>} + W'_{xo} x^{<t>} + b'_o)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde {c'}^{<t>} = \\tanh (W'_{c'} a'^{<t-1>} + W'_{xc'} x'^{<t>} + b'_c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "c'^{<t>} = \\Gamma'_f * c'^{<t-1>} + \\Gamma'_u * \\tilde c'^{<t>}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a'^{<t>} = \\Gamma'_o * \\tanh c'^{<t>}\n",
    "$$\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "$$\n",
    "\\hat y^{<t>} = g_y (W_y a^{<t>} + W'_y a'^{t} + b_y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] http://viet.jnlp.org/download-du-lieu-tu-vung-corpus, *Vietnamese Corpus* \n",
    "\n",
    "[2] http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "[3] I. Goodfellow, Y. Bengio, A.Courville, *Deep Learning*\n",
    "\n",
    "[4] A. Ng, K. Katanforoosh, B. Mouru, *Sequence Models* (Course in coursera.org)\n",
    "\n",
    "[5] R. Atienza, *Advanced Deep Learning with Keras*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
