{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD1 - MNIST và Bitcoin với Feedforward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô tả\n",
    "\n",
    "Trong phần này, ta dùng Feedforward Network (FFN) để phân loại các chữ số trong tập dữ liệu nổi tiếng <a href=\"https://toolbox.google.com/datasetsearch/search?query=MNIST%20Database%20(mnist.pkl.gz)&docid=QZBrQgKepkevjOimAAAAAA%3D%3D\">MNIST</a>. Ta sử dụng thư viện `keras` để làm việc với các mô hình FFN.\n",
    "\n",
    "Để download MNIST với keras, ta sử dụng đoạn code sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tại lần chạy đầu tiên của đoạn code, dữ liệu sẽ được download. Ở các lần chạy sau, dữ liệu sẽ được gọi trực tiếp từ ổ đĩa (không cần download lại)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_raw, y_train_raw), (X_test_raw, y_test_raw) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train_raw`, `y_train_raw` là dữ liệu dùng cho training với 60000 hình ảnh, mỗi ảnh có kích thước 28x28, tương ứng với nhãn là một chữ số từ 0 đến 9. Tương tự có 10000 hình ảnh dùng cho test (`X_test_raw`, `y_test_raw`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw.shape, y_train_raw.shape, X_test_raw.shape, y_test_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mỗi hình ảnh có dạng đen trắng, trong đó mỗi pixel có giá trị (cường độ) thuộc đoạn [0, 255], 0 tượng trưng cho đen và 255 tượng trưng cho trắng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong cả TD này, ta sẽ làm việc với dạng chuẩn hoá của các hình ảnh, trong đó thay cường độ các pixel được chia cho 255 và thuộc đoạn [0, 1]. Đồng thời ta sẽ chuyển mỗi hình ảnh về một vector 784 chiều thay cho một array (28 x 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_raw.reshape((60000, 784))/255\n",
    "X_test = X_test_raw.reshape((10000, 784))/255\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngoài ra, để áp dụng multiclass, classification, ta sẽ biến mỗi nhãn $0, 1, \\ldots, 9$ thành các vector 10 thành phần $(t_0, \\ldots, t_9)$ trong đó có 1 thành phần duy nhất bằng 1, các thành phần khác bằng 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train_raw, 10)\n",
    "Y_test = np_utils.to_categorical(y_test_raw, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiểm tra lại: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_raw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:5] # Các vector này tương ứng với 5 nhãn phía trên"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 1 Multinomial Logistic Regression (Softmax Regression)\n",
    "\n",
    "Nhắc lại rằng Softmax Regression tương ứng với một FFN với 1 hidden layer, với hàm kích hoạt (activation function) là hàm softmax\n",
    "\n",
    "và hàm loss được sử dụng là categorical entropy.\n",
    "$$\n",
    "L(t, y) = -\\sum_{k=1}^K t_k \\log(y_k)\n",
    "$$\n",
    "\n",
    "($K$ là số lớp trong bài toán phân loại, ở đây là 10).\n",
    "\n",
    "***Sử dụng `keras`, hãy xây dựng một mô hình (model) theo FFN nêu trên. Lưu mô hình này bằng một tên biến riêng (ví dụ `model1` để so sánh về sau).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "np.random.seed(0) \n",
    "\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 \n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 \n",
    "INPUT_SIZE = 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(NB_CLASSES, input_shape=(INPUT_SIZE,)))\n",
    "model1.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi gọi `model1.summary()`, số lượng tham số cần bằng 7850, tương ứng với 784 unit ở input layer + 1 unit tự do, nhân với số unit ở output layer. Bạn có thể dùng đoạn code dưới đây để test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_181 (Dense)            (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_183 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hãy train với Softmax Regression trên `X_train`, `Y_train` bằng cách chọn ngẫu nhiên 80% để train và 20% cho validation, sau đó dự đoán trên `X_test`, `Y_test`. Sử dụng `np.random.seed(0)` để cố định trạng thái ngẫu nhiên và SGD (các tham số ở dạng default) làm thuật toán tối ưu. Chọn minibatch size là 128. Tính accuracy trên tập train và tập validation sau 20 bước lặp. Lưu lịch sử train bằng một tên biến riêng (ví dụ `history1 = model1.fit(...)`) để sử dụng về sau.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 56us/step - loss: 1.3437 - acc: 0.6849 - val_loss: 0.8861 - val_acc: 0.8268\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.7886 - acc: 0.8273 - val_loss: 0.6561 - val_acc: 0.8581\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.6422 - acc: 0.8492 - val_loss: 0.5615 - val_acc: 0.8692\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.5706 - acc: 0.8600 - val_loss: 0.5093 - val_acc: 0.8777\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.5267 - acc: 0.8673 - val_loss: 0.4753 - val_acc: 0.8813\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4965 - acc: 0.8719 - val_loss: 0.4510 - val_acc: 0.8878\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4742 - acc: 0.8767 - val_loss: 0.4328 - val_acc: 0.8895\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.4567 - acc: 0.8805 - val_loss: 0.4189 - val_acc: 0.8917\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4428 - acc: 0.8829 - val_loss: 0.4071 - val_acc: 0.8943\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4311 - acc: 0.8855 - val_loss: 0.3975 - val_acc: 0.8959\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.4213 - acc: 0.8874 - val_loss: 0.3895 - val_acc: 0.8977\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4129 - acc: 0.8892 - val_loss: 0.3823 - val_acc: 0.8992\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.4056 - acc: 0.8909 - val_loss: 0.3763 - val_acc: 0.8995\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3991 - acc: 0.8916 - val_loss: 0.3708 - val_acc: 0.9012\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 20us/step - loss: 0.3933 - acc: 0.8937 - val_loss: 0.3660 - val_acc: 0.9021\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.3881 - acc: 0.8947 - val_loss: 0.3617 - val_acc: 0.9022\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.3834 - acc: 0.8953 - val_loss: 0.3579 - val_acc: 0.9032\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.3791 - acc: 0.8962 - val_loss: 0.3544 - val_acc: 0.9044\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.3752 - acc: 0.8972 - val_loss: 0.3511 - val_acc: 0.9045\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.3715 - acc: 0.8978 - val_loss: 0.3482 - val_acc: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history1 = model1.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 82us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34765567469596864, 0.904]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "model1.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vẽ đồ thị biểu diễn accuracy trên tập train và tập validation theo quá trình train (20 bước)***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x349d41d0>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(20), history1.history.get('acc'), label = 'Training set (80%)')\n",
    "plt.plot(range(20), history1.history.get('val_acc'), label = 'Validation set (20%)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 2. FFN 2 tầng - Chọn Hidden Unit\n",
    "\n",
    "Ta sử dụng model sau: \n",
    "\n",
    "- Tầng input: 784 unit (+1 unit tự do) như cũ\n",
    "- Tầng hidden trung gian: 8, 32, 128 hoặc 512 unit (4 lựa chọn)\n",
    "- Hàm kích hoạt ở tầng hidden trung gian: relu hoặc sigmoid (2 lựa chọn)\n",
    "- Tầng output: 10 unit\n",
    "- Hàm kích hoạt ở tầng output: softmax\n",
    "\n",
    "- Hàm loss: categorical entropy\n",
    "\n",
    "***Sử dụng `keras`, hãy xây dựng 8 mô hình (model) theo mô tả nêu trên. Lưu các mô hình này bằng một tên biến riêng (ví dụ `model2A`, ..., `model2H` để so sánh về sau).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "models2 = []\n",
    "histories2 = [];\n",
    "\n",
    "NB_HIDDEN_UNITS = [8, 32, 128, 512]\n",
    "ACTIVATION_FUNCTIONS = ['relu', 'sigmoid']\n",
    "\n",
    "for nb_hidden_unit in NB_HIDDEN_UNITS:\n",
    "    for activation_function in ACTIVATION_FUNCTIONS:\n",
    "        models2.append(Sequential())\n",
    "        models2[-1].add(Dense(nb_hidden_unit, input_shape=(INPUT_SIZE,)))\n",
    "        models2[-1].add(Activation(activation_function))\n",
    "        models2[-1].add(Dense(NB_CLASSES))\n",
    "        models2[-1].add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_182 (Dense)            (None, 8)                 6280      \n",
      "_________________________________________________________________\n",
      "activation_184 (Activation)  (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_183 (Dense)            (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "activation_185 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 6,370\n",
      "Trainable params: 6,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models2[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_196 (Dense)            (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_198 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_199 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models2[7].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hãy train với các mô hình trên `X_train`, `Y_train` bằng cách chọn ngẫu nhiên 80% để train và 20% cho validation, sau đó dự đoán trên `X_test`, `Y_test`. Sử dụng `np.random.seed(0)` để cố định trạng thái ngẫu nhiên và SGD (các tham số ở dạng default) làm thuật toán tối ưu. Chọn batch size là 128. Tính accuracy trên tập train và tập validation sau 20 bước lặp. Lưu lịch sử train bằng một tên biến riêng (ví dụ `history2A = model2A.fit(...)`) để sử dụng về sau.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 102us/step\n",
      "Nb Units: 8, activation: relu. Test result: [0.35850902096033094, 0.8972]\n",
      "10000/10000 [==============================] - 1s 117us/step\n",
      "Nb Units: 8, activation: sigmoid. Test result: [0.8136835326194763, 0.8298]\n",
      "10000/10000 [==============================] - 1s 121us/step\n",
      "Nb Units: 32, activation: relu. Test result: [0.26319868678450586, 0.9269]\n",
      "10000/10000 [==============================] - 1s 118us/step\n",
      "Nb Units: 32, activation: sigmoid. Test result: [0.47016625819206237, 0.8905]\n",
      "10000/10000 [==============================] - 1s 52us/step\n",
      "Nb Units: 128, activation: relu. Test result: [0.23383381830751895, 0.9352]\n",
      "10000/10000 [==============================] - 1s 123us/step\n",
      "Nb Units: 128, activation: sigmoid. Test result: [0.40238749358654025, 0.8945]\n",
      "10000/10000 [==============================] - ETA:  - 1s 78us/step\n",
      "Nb Units: 512, activation: relu. Test result: [0.22236659649610518, 0.9397]\n",
      "10000/10000 [==============================] - 1s 141us/step\n",
      "Nb Units: 512, activation: sigmoid. Test result: [0.3820650410056114, 0.8971]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "i = 0\n",
    "BATCH_SIZE = 128\n",
    "OPTIMIZER = SGD()\n",
    "\n",
    "for nb_hidden_unit in NB_HIDDEN_UNITS:\n",
    "    for activation_function in ACTIVATION_FUNCTIONS:\n",
    "        np.random.seed(0)\n",
    "        models2[i].compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "        histories2.append(models2[i].fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=0, validation_split=VALIDATION_SPLIT))\n",
    "        print(\"Nb Units: {}, activation: {}. Test result: {}\".format(nb_hidden_unit, activation_function, models2[i].evaluate(X_test, Y_test)))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vẽ đồ thị biểu diễn accuracy trên tập train theo quá trình train (20 bước) với từng mô hình trên. Chọn ra cặp tham số tốt nhất cho số unit ở tầng hidden và hàm activation được sử dụng. Biểu diễn số liệu ở dạng DataFrame để so sánh.***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x491cbb00>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "i = 0\n",
    "\n",
    "for nb_hidden_unit in NB_HIDDEN_UNITS:\n",
    "    for activation_function in ACTIVATION_FUNCTIONS:\n",
    "        plt.plot(range(NB_EPOCH), histories2[i].history.get('acc'), label='{} units, {}'.format(nb_hidden_unit, activation_function))\n",
    "        i += 1\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.284375</td>\n",
       "      <td>0.614229</td>\n",
       "      <td>0.771563</td>\n",
       "      <td>0.813792</td>\n",
       "      <td>0.834729</td>\n",
       "      <td>0.848208</td>\n",
       "      <td>0.857750</td>\n",
       "      <td>0.864958</td>\n",
       "      <td>0.869687</td>\n",
       "      <td>0.873208</td>\n",
       "      <td>0.877417</td>\n",
       "      <td>0.880167</td>\n",
       "      <td>0.883563</td>\n",
       "      <td>0.885979</td>\n",
       "      <td>0.887958</td>\n",
       "      <td>0.889750</td>\n",
       "      <td>0.891917</td>\n",
       "      <td>0.893292</td>\n",
       "      <td>0.893979</td>\n",
       "      <td>0.895958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139458</td>\n",
       "      <td>0.337021</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>0.549646</td>\n",
       "      <td>0.602354</td>\n",
       "      <td>0.640479</td>\n",
       "      <td>0.669312</td>\n",
       "      <td>0.694896</td>\n",
       "      <td>0.716833</td>\n",
       "      <td>0.736333</td>\n",
       "      <td>0.752229</td>\n",
       "      <td>0.765354</td>\n",
       "      <td>0.778188</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.796375</td>\n",
       "      <td>0.803479</td>\n",
       "      <td>0.809521</td>\n",
       "      <td>0.815146</td>\n",
       "      <td>0.820354</td>\n",
       "      <td>0.824729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.639875</td>\n",
       "      <td>0.841292</td>\n",
       "      <td>0.869250</td>\n",
       "      <td>0.882062</td>\n",
       "      <td>0.888521</td>\n",
       "      <td>0.893813</td>\n",
       "      <td>0.898125</td>\n",
       "      <td>0.901354</td>\n",
       "      <td>0.903292</td>\n",
       "      <td>0.906479</td>\n",
       "      <td>0.908104</td>\n",
       "      <td>0.910208</td>\n",
       "      <td>0.912875</td>\n",
       "      <td>0.914375</td>\n",
       "      <td>0.916042</td>\n",
       "      <td>0.917792</td>\n",
       "      <td>0.918750</td>\n",
       "      <td>0.920167</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.922354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.303583</td>\n",
       "      <td>0.628833</td>\n",
       "      <td>0.706125</td>\n",
       "      <td>0.752771</td>\n",
       "      <td>0.780729</td>\n",
       "      <td>0.800042</td>\n",
       "      <td>0.815958</td>\n",
       "      <td>0.827271</td>\n",
       "      <td>0.836333</td>\n",
       "      <td>0.844125</td>\n",
       "      <td>0.849833</td>\n",
       "      <td>0.854542</td>\n",
       "      <td>0.859958</td>\n",
       "      <td>0.863292</td>\n",
       "      <td>0.867042</td>\n",
       "      <td>0.869708</td>\n",
       "      <td>0.872521</td>\n",
       "      <td>0.875146</td>\n",
       "      <td>0.877479</td>\n",
       "      <td>0.879354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.689833</td>\n",
       "      <td>0.856146</td>\n",
       "      <td>0.877771</td>\n",
       "      <td>0.888896</td>\n",
       "      <td>0.896396</td>\n",
       "      <td>0.901083</td>\n",
       "      <td>0.905625</td>\n",
       "      <td>0.908771</td>\n",
       "      <td>0.911625</td>\n",
       "      <td>0.914188</td>\n",
       "      <td>0.917250</td>\n",
       "      <td>0.918687</td>\n",
       "      <td>0.921167</td>\n",
       "      <td>0.922958</td>\n",
       "      <td>0.924708</td>\n",
       "      <td>0.926646</td>\n",
       "      <td>0.928042</td>\n",
       "      <td>0.929854</td>\n",
       "      <td>0.931688</td>\n",
       "      <td>0.932771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.469979</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.773167</td>\n",
       "      <td>0.806438</td>\n",
       "      <td>0.823604</td>\n",
       "      <td>0.836021</td>\n",
       "      <td>0.844229</td>\n",
       "      <td>0.851104</td>\n",
       "      <td>0.857292</td>\n",
       "      <td>0.862083</td>\n",
       "      <td>0.865812</td>\n",
       "      <td>0.869292</td>\n",
       "      <td>0.872437</td>\n",
       "      <td>0.875458</td>\n",
       "      <td>0.877708</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.881729</td>\n",
       "      <td>0.883354</td>\n",
       "      <td>0.884833</td>\n",
       "      <td>0.886292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.727583</td>\n",
       "      <td>0.863021</td>\n",
       "      <td>0.882062</td>\n",
       "      <td>0.892563</td>\n",
       "      <td>0.899250</td>\n",
       "      <td>0.904271</td>\n",
       "      <td>0.908667</td>\n",
       "      <td>0.912375</td>\n",
       "      <td>0.914771</td>\n",
       "      <td>0.917937</td>\n",
       "      <td>0.920375</td>\n",
       "      <td>0.922375</td>\n",
       "      <td>0.924625</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>0.930583</td>\n",
       "      <td>0.932187</td>\n",
       "      <td>0.933458</td>\n",
       "      <td>0.935375</td>\n",
       "      <td>0.936271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.505458</td>\n",
       "      <td>0.724688</td>\n",
       "      <td>0.780792</td>\n",
       "      <td>0.810854</td>\n",
       "      <td>0.828271</td>\n",
       "      <td>0.840208</td>\n",
       "      <td>0.850292</td>\n",
       "      <td>0.857396</td>\n",
       "      <td>0.862229</td>\n",
       "      <td>0.867313</td>\n",
       "      <td>0.870167</td>\n",
       "      <td>0.873042</td>\n",
       "      <td>0.876062</td>\n",
       "      <td>0.878354</td>\n",
       "      <td>0.880646</td>\n",
       "      <td>0.882604</td>\n",
       "      <td>0.884250</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>0.887250</td>\n",
       "      <td>0.888646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.284375  0.614229  0.771563  0.813792  0.834729  0.848208  0.857750   \n",
       "1  0.139458  0.337021  0.474000  0.549646  0.602354  0.640479  0.669312   \n",
       "2  0.639875  0.841292  0.869250  0.882062  0.888521  0.893813  0.898125   \n",
       "3  0.303583  0.628833  0.706125  0.752771  0.780729  0.800042  0.815958   \n",
       "4  0.689833  0.856146  0.877771  0.888896  0.896396  0.901083  0.905625   \n",
       "5  0.469979  0.715000  0.773167  0.806438  0.823604  0.836021  0.844229   \n",
       "6  0.727583  0.863021  0.882062  0.892563  0.899250  0.904271  0.908667   \n",
       "7  0.505458  0.724688  0.780792  0.810854  0.828271  0.840208  0.850292   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.864958  0.869687  0.873208  0.877417  0.880167  0.883563  0.885979   \n",
       "1  0.694896  0.716833  0.736333  0.752229  0.765354  0.778188  0.786667   \n",
       "2  0.901354  0.903292  0.906479  0.908104  0.910208  0.912875  0.914375   \n",
       "3  0.827271  0.836333  0.844125  0.849833  0.854542  0.859958  0.863292   \n",
       "4  0.908771  0.911625  0.914188  0.917250  0.918687  0.921167  0.922958   \n",
       "5  0.851104  0.857292  0.862083  0.865812  0.869292  0.872437  0.875458   \n",
       "6  0.912375  0.914771  0.917937  0.920375  0.922375  0.924625  0.926667   \n",
       "7  0.857396  0.862229  0.867313  0.870167  0.873042  0.876062  0.878354   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.887958  0.889750  0.891917  0.893292  0.893979  0.895958  \n",
       "1  0.796375  0.803479  0.809521  0.815146  0.820354  0.824729  \n",
       "2  0.916042  0.917792  0.918750  0.920167  0.921687  0.922354  \n",
       "3  0.867042  0.869708  0.872521  0.875146  0.877479  0.879354  \n",
       "4  0.924708  0.926646  0.928042  0.929854  0.931688  0.932771  \n",
       "5  0.877708  0.880000  0.881729  0.883354  0.884833  0.886292  \n",
       "6  0.928500  0.930583  0.932187  0.933458  0.935375  0.936271  \n",
       "7  0.880646  0.882604  0.884250  0.885500  0.887250  0.888646  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame([h.history.get('acc') for h in histories2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 3. FFN 2 tầng. Chọn thuật toán tối ưu\n",
    "\n",
    "Sau bài 2, ta chọn activation là **`relu`** và số unit là **128**. Bây giờ ta chọn thuật toán tối ưu trong số list sau:\n",
    "\n",
    "- `SGD (lr = 0.1)`\n",
    "\n",
    "***Hãy train với các mô hình trên `X_train`, `Y_train` bằng cách chọn ngẫu nhiên 80% để train và 20% cho validation, sau đó dự đoán trên `X_test`, `Y_test`. Sử dụng `np.random.seed(0)` để cố định trạng thái ngẫu nhiên , chọn batch size là 128. Tính accuracy trên tập train và tập validation sau 20 bước lặp.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 1.2746 - acc: 0.6952 - val_loss: 0.7083 - val_acc: 0.8508\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.6107 - acc: 0.8569 - val_loss: 0.4894 - val_acc: 0.8796\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.4792 - acc: 0.8772 - val_loss: 0.4145 - val_acc: 0.8933\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 55us/step - loss: 0.4220 - acc: 0.8875 - val_loss: 0.3762 - val_acc: 0.8978\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.3883 - acc: 0.8948 - val_loss: 0.3519 - val_acc: 0.9040\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.3655 - acc: 0.9002 - val_loss: 0.3346 - val_acc: 0.9075\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.3483 - acc: 0.9040 - val_loss: 0.3211 - val_acc: 0.9102\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.3345 - acc: 0.9071 - val_loss: 0.3102 - val_acc: 0.9137\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.3230 - acc: 0.9096 - val_loss: 0.3012 - val_acc: 0.9161\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.3131 - acc: 0.9127 - val_loss: 0.2936 - val_acc: 0.9186\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.3043 - acc: 0.9150 - val_loss: 0.2858 - val_acc: 0.9202\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.2964 - acc: 0.9170 - val_loss: 0.2799 - val_acc: 0.9220\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.2893 - acc: 0.9185 - val_loss: 0.2738 - val_acc: 0.9241\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.2825 - acc: 0.9210 - val_loss: 0.2688 - val_acc: 0.9248\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.2764 - acc: 0.9225 - val_loss: 0.2629 - val_acc: 0.9277\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.2704 - acc: 0.9240 - val_loss: 0.2580 - val_acc: 0.9292\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.2648 - acc: 0.9255 - val_loss: 0.2535 - val_acc: 0.9303\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.2594 - acc: 0.9268 - val_loss: 0.2498 - val_acc: 0.9319\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2545 - acc: 0.9282 - val_loss: 0.2447 - val_acc: 0.9331\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2495 - acc: 0.9295 - val_loss: 0.2410 - val_acc: 0.9337\n",
      "10000/10000 [==============================] - 1s 99us/step\n",
      "Algo 0: [0.24111952318251134, 0.9327]\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 8s 175us/step - loss: 0.5021 - acc: 0.8641 - val_loss: 0.3011 - val_acc: 0.9159\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.2839 - acc: 0.9195 - val_loss: 0.2399 - val_acc: 0.9330\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.2338 - acc: 0.9333 - val_loss: 0.2090 - val_acc: 0.9432\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.1999 - acc: 0.9438 - val_loss: 0.1888 - val_acc: 0.9483\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.1741 - acc: 0.9503 - val_loss: 0.1710 - val_acc: 0.9528\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.1548 - acc: 0.9560 - val_loss: 0.1541 - val_acc: 0.9572\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.1394 - acc: 0.9604 - val_loss: 0.1485 - val_acc: 0.9588\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.1260 - acc: 0.9644 - val_loss: 0.1374 - val_acc: 0.9617\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.1160 - acc: 0.9671 - val_loss: 0.1300 - val_acc: 0.9647\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.1067 - acc: 0.9701 - val_loss: 0.1241 - val_acc: 0.9660\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0992 - acc: 0.9720 - val_loss: 0.1184 - val_acc: 0.9667\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0922 - acc: 0.9743 - val_loss: 0.1177 - val_acc: 0.9670\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0865 - acc: 0.9760 - val_loss: 0.1117 - val_acc: 0.9683\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.0810 - acc: 0.9778 - val_loss: 0.1088 - val_acc: 0.9688\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 0.0765 - acc: 0.9793 - val_loss: 0.1062 - val_acc: 0.9688\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.0718 - acc: 0.9801 - val_loss: 0.1040 - val_acc: 0.9700\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.0681 - acc: 0.9816 - val_loss: 0.1023 - val_acc: 0.9698\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0645 - acc: 0.9828 - val_loss: 0.1004 - val_acc: 0.9700\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.0614 - acc: 0.9834 - val_loss: 0.0999 - val_acc: 0.9708\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.0582 - acc: 0.9847 - val_loss: 0.0975 - val_acc: 0.9711\n",
      "10000/10000 [==============================] - 1s 116us/step\n",
      "Algo 1: [0.08725143189160153, 0.9724]\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2927 - acc: 0.9114 - val_loss: 0.1468 - val_acc: 0.9577\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.1177 - acc: 0.9644 - val_loss: 0.1089 - val_acc: 0.9677\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0814 - acc: 0.9754 - val_loss: 0.0998 - val_acc: 0.9707\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.0624 - acc: 0.9811 - val_loss: 0.1016 - val_acc: 0.9693: 0.06\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.0486 - acc: 0.9852 - val_loss: 0.0914 - val_acc: 0.9723\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.0377 - acc: 0.9886 - val_loss: 0.0906 - val_acc: 0.9731\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.0303 - acc: 0.9909 - val_loss: 0.0929 - val_acc: 0.9727\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.0236 - acc: 0.9931 - val_loss: 0.0903 - val_acc: 0.9759\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.0924 - val_acc: 0.9748ss: 0.01 - ETA: 0s - loss: 0.0176 - acc: 0.\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.0142 - acc: 0.9962 - val_loss: 0.0847 - val_acc: 0.9754\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.0925 - val_acc: 0.9757\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.0897 - val_acc: 0.9750\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.0055 - acc: 0.9992 - val_loss: 0.0827 - val_acc: 0.9772\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.0039 - acc: 0.9997 - val_loss: 0.0838 - val_acc: 0.9787\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.0031 - acc: 0.9998 - val_loss: 0.0834 - val_acc: 0.9787\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0854 - val_acc: 0.9788\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.0021 - acc: 0.9999 - val_loss: 0.0856 - val_acc: 0.9787 - acc: 0. - ETA: 0s - loss: 0.0020 -\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0854 - val_acc: 0.9787\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0854 - val_acc: 0.9790\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0868 - val_acc: 0.9782\n",
      "10000/10000 [==============================] - 1s 127us/step\n",
      "Algo 2: [0.07879020950428485, 0.9805]\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 8s 162us/step - loss: 0.3990 - acc: 0.8898 - val_loss: 0.2187 - val_acc: 0.9407\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.1909 - acc: 0.9451 - val_loss: 0.1600 - val_acc: 0.9553\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.1386 - acc: 0.9604 - val_loss: 0.1340 - val_acc: 0.9622\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.1077 - acc: 0.9693 - val_loss: 0.1176 - val_acc: 0.9666\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.0871 - acc: 0.9756 - val_loss: 0.1115 - val_acc: 0.9677\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.0722 - acc: 0.9791 - val_loss: 0.1074 - val_acc: 0.9673\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.0602 - acc: 0.9828 - val_loss: 0.1003 - val_acc: 0.9707\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.0513 - acc: 0.9854 - val_loss: 0.0939 - val_acc: 0.9714\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.0432 - acc: 0.9880 - val_loss: 0.0896 - val_acc: 0.9733\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.0366 - acc: 0.9906 - val_loss: 0.0859 - val_acc: 0.9743\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.0310 - acc: 0.9916 - val_loss: 0.0879 - val_acc: 0.9731\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.0262 - acc: 0.9937 - val_loss: 0.0920 - val_acc: 0.9722\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.0222 - acc: 0.9949 - val_loss: 0.0877 - val_acc: 0.9732\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0191 - acc: 0.9957 - val_loss: 0.0866 - val_acc: 0.9742\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0159 - acc: 0.9966 - val_loss: 0.0863 - val_acc: 0.9752\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.0135 - acc: 0.9975 - val_loss: 0.0894 - val_acc: 0.9739\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.0118 - acc: 0.9979 - val_loss: 0.0878 - val_acc: 0.9756\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 0.0097 - acc: 0.9983 - val_loss: 0.0894 - val_acc: 0.9741\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.0080 - acc: 0.9989 - val_loss: 0.0922 - val_acc: 0.9732\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0072 - acc: 0.9991 - val_loss: 0.0942 - val_acc: 0.9742\n",
      "10000/10000 [==============================] - 2s 157us/step\n",
      "Algo 3: [0.08693548122021894, 0.9762]\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 9s 195us/step - loss: 0.2343 - acc: 0.9283 - val_loss: 0.1333 - val_acc: 0.9607\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.1151 - acc: 0.9645 - val_loss: 0.1173 - val_acc: 0.9649\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 59us/step - loss: 0.0866 - acc: 0.9736 - val_loss: 0.1212 - val_acc: 0.9632\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0725 - acc: 0.9777 - val_loss: 0.1399 - val_acc: 0.9618\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.0716 - acc: 0.9782 - val_loss: 0.1306 - val_acc: 0.9681\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.0578 - acc: 0.9827 - val_loss: 0.1721 - val_acc: 0.9610\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.0600 - acc: 0.9824 - val_loss: 0.1737 - val_acc: 0.9625\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.0584 - acc: 0.9832 - val_loss: 0.1560 - val_acc: 0.9681\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0499 - acc: 0.9850 - val_loss: 0.1819 - val_acc: 0.9678\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.0519 - acc: 0.9860 - val_loss: 0.1694 - val_acc: 0.9666\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 61us/step - loss: 0.0467 - acc: 0.9869 - val_loss: 0.1886 - val_acc: 0.9683\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0430 - acc: 0.9881 - val_loss: 0.1770 - val_acc: 0.9721\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0508 - acc: 0.9879 - val_loss: 0.2188 - val_acc: 0.9655\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0492 - acc: 0.9877 - val_loss: 0.1861 - val_acc: 0.9706\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0376 - acc: 0.9901 - val_loss: 0.1908 - val_acc: 0.9719\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 0.0395 - acc: 0.9902 - val_loss: 0.2160 - val_acc: 0.9708\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 60us/step - loss: 0.0470 - acc: 0.9895 - val_loss: 0.2305 - val_acc: 0.9682\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.0533 - acc: 0.9889 - val_loss: 0.2337 - val_acc: 0.9687\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.0415 - acc: 0.9912 - val_loss: 0.1910 - val_acc: 0.9740\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.0312 - acc: 0.9920 - val_loss: 0.2242 - val_acc: 0.9715\n",
      "10000/10000 [==============================] - 2s 154us/step\n",
      "Algo 4: [0.201716987359012, 0.9741]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "models3 = []\n",
    "histories3 = []\n",
    "\n",
    "NB_HIDDEN_UNIT = 128\n",
    "ACTIVATION_FUNCTION = 'relu'\n",
    "OPTIMIZERS = [SGD(lr = 0.01), SGD(lr = 0.1), SGD(lr = 0.1, momentum = 0.9), Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999), Adam(lr = 0.01, beta_1 = 0.9, beta_2 = 0.999)]\n",
    "\n",
    "i = 0\n",
    "\n",
    "for optimizer in OPTIMIZERS:\n",
    "    np.random.seed(0)\n",
    "    models3.append(Sequential())\n",
    "    models3[-1].add(Dense(NB_HIDDEN_UNIT, input_shape=(INPUT_SIZE,)))\n",
    "    models3[-1].add(Activation(ACTIVATION_FUNCTION))\n",
    "    models3[-1].add(Dense(NB_CLASSES))\n",
    "    models3[-1].add(Activation('softmax'))\n",
    "    models3[-1].compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    histories3.append(models3[i].fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=1, validation_split=VALIDATION_SPLIT))\n",
    "    print(\"Algo {}: {}\".format(i, models3[-1].evaluate(X_test, Y_test)))\n",
    "    i += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Vẽ đồ thị biểu diễn accuracy trên tập train theo quá trình train (20 bước) với từng mô hình trên. Chọn ra thuật toán (cùng với các tham số tốt nhất) được sử dụng. Biểu diễn các giá trị ở dạng DataFrame để có so sánh cụ thể hơn. Bạn có thể thử với các thuật toán tối ưu khác.***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x34b04d30>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "for i in range(len(OPTIMIZERS)):\n",
    "    plt.plot(range(NB_EPOCH), histories3[i].history.get('acc'), label='Algo {}'.format(i))\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.695167</td>\n",
       "      <td>0.856854</td>\n",
       "      <td>0.877167</td>\n",
       "      <td>0.887479</td>\n",
       "      <td>0.894813</td>\n",
       "      <td>0.900167</td>\n",
       "      <td>0.904042</td>\n",
       "      <td>0.907062</td>\n",
       "      <td>0.909646</td>\n",
       "      <td>0.912687</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.917042</td>\n",
       "      <td>0.918458</td>\n",
       "      <td>0.921021</td>\n",
       "      <td>0.922479</td>\n",
       "      <td>0.924042</td>\n",
       "      <td>0.925521</td>\n",
       "      <td>0.926750</td>\n",
       "      <td>0.928208</td>\n",
       "      <td>0.929458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.864062</td>\n",
       "      <td>0.919458</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.943750</td>\n",
       "      <td>0.950313</td>\n",
       "      <td>0.955958</td>\n",
       "      <td>0.960438</td>\n",
       "      <td>0.964396</td>\n",
       "      <td>0.967063</td>\n",
       "      <td>0.970125</td>\n",
       "      <td>0.972042</td>\n",
       "      <td>0.974333</td>\n",
       "      <td>0.976021</td>\n",
       "      <td>0.977792</td>\n",
       "      <td>0.979313</td>\n",
       "      <td>0.980062</td>\n",
       "      <td>0.981604</td>\n",
       "      <td>0.982833</td>\n",
       "      <td>0.983437</td>\n",
       "      <td>0.984729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.911375</td>\n",
       "      <td>0.964396</td>\n",
       "      <td>0.975396</td>\n",
       "      <td>0.981104</td>\n",
       "      <td>0.985208</td>\n",
       "      <td>0.988646</td>\n",
       "      <td>0.990917</td>\n",
       "      <td>0.993083</td>\n",
       "      <td>0.994896</td>\n",
       "      <td>0.996229</td>\n",
       "      <td>0.997604</td>\n",
       "      <td>0.998792</td>\n",
       "      <td>0.999208</td>\n",
       "      <td>0.999708</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.889792</td>\n",
       "      <td>0.945146</td>\n",
       "      <td>0.960438</td>\n",
       "      <td>0.969333</td>\n",
       "      <td>0.975562</td>\n",
       "      <td>0.979146</td>\n",
       "      <td>0.982792</td>\n",
       "      <td>0.985354</td>\n",
       "      <td>0.988021</td>\n",
       "      <td>0.990604</td>\n",
       "      <td>0.991625</td>\n",
       "      <td>0.993687</td>\n",
       "      <td>0.994875</td>\n",
       "      <td>0.995750</td>\n",
       "      <td>0.996604</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.997854</td>\n",
       "      <td>0.998271</td>\n",
       "      <td>0.998917</td>\n",
       "      <td>0.999104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.928292</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>0.973625</td>\n",
       "      <td>0.977708</td>\n",
       "      <td>0.978208</td>\n",
       "      <td>0.982729</td>\n",
       "      <td>0.982437</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.985042</td>\n",
       "      <td>0.986021</td>\n",
       "      <td>0.986917</td>\n",
       "      <td>0.988146</td>\n",
       "      <td>0.987875</td>\n",
       "      <td>0.987708</td>\n",
       "      <td>0.990146</td>\n",
       "      <td>0.990250</td>\n",
       "      <td>0.989458</td>\n",
       "      <td>0.988854</td>\n",
       "      <td>0.991167</td>\n",
       "      <td>0.992021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.695167  0.856854  0.877167  0.887479  0.894813  0.900167  0.904042   \n",
       "1  0.864062  0.919458  0.933333  0.943750  0.950313  0.955958  0.960438   \n",
       "2  0.911375  0.964396  0.975396  0.981104  0.985208  0.988646  0.990917   \n",
       "3  0.889792  0.945146  0.960438  0.969333  0.975562  0.979146  0.982792   \n",
       "4  0.928292  0.964500  0.973625  0.977708  0.978208  0.982729  0.982437   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.907062  0.909646  0.912687  0.915000  0.917042  0.918458  0.921021   \n",
       "1  0.964396  0.967063  0.970125  0.972042  0.974333  0.976021  0.977792   \n",
       "2  0.993083  0.994896  0.996229  0.997604  0.998792  0.999208  0.999708   \n",
       "3  0.985354  0.988021  0.990604  0.991625  0.993687  0.994875  0.995750   \n",
       "4  0.983229  0.985042  0.986021  0.986917  0.988146  0.987875  0.987708   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0  0.922479  0.924042  0.925521  0.926750  0.928208  0.929458  \n",
       "1  0.979313  0.980062  0.981604  0.982833  0.983437  0.984729  \n",
       "2  0.999812  0.999958  0.999917  0.999979  1.000000  1.000000  \n",
       "3  0.996604  0.997500  0.997854  0.998271  0.998917  0.999104  \n",
       "4  0.990146  0.990250  0.989458  0.988854  0.991167  0.992021  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([h.history.get('acc') for h in histories3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 4. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B. Bitcoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô tả\n",
    "\n",
    "Ở TD6 DSC101, ta đã dự đoán độ biến thiên giá Bitcoin sau 1 tuần thông qua Linear và Polynomial Regression trên một số chỉ số tài chính dễ tính. Ta sử dụng file giá bitcoin tại <a href=\"https://raw.githubusercontent.com/riduan91/DSC111/master/Lesson1/TD/Data/BTCPrice.csv\">`Data.csv`</a>. Nhắc lại rằng ở TD6 DSC101 ta đã viết các hàm sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stockstats as sts\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SHIFT_NUMBER = 7\n",
    "SKIP = 10\n",
    "COLUMNS = ['open', 'high', 'low', 'close', 'volume', 'cr', 'cr-ma1', 'cr-ma2', 'cr-ma3', \n",
    "           'rsv_9', 'kdjk_9', 'kdjj_9', 'macd', 'macds', 'macdh', 'rs_6', 'rsi_6', 'rs_12', \n",
    "           'rsi_12', 'wr_6', 'wr_10', 'cci', 'cci_20', 'tr', 'atr', 'dma', 'high_delta', 'um', \n",
    "           'low_delta', 'dm', 'pdm', 'pdm_14', 'pdi_14', 'mdm_14', 'mdi_14', 'dx_14', \n",
    "           'adx', 'adxr', 'trix', 'change', 'vr', 'vr_6_sma']\n",
    "\n",
    "def addTechnicalIndicators(simple_data):\n",
    "    stock = sts.StockDataFrame(simple_data)\n",
    "    stock['cr']\n",
    "    stock['kdjk']\n",
    "    stock['kdjd']\n",
    "    stock['kdjj']\n",
    "    stock['close_10_sma']\n",
    "    stock['macd']\n",
    "    stock['boll']\n",
    "    stock['rsi_6']\n",
    "    stock['rsi_12']\n",
    "    stock['wr_6']\n",
    "    stock['wr_10']\n",
    "    stock['cci']\n",
    "    stock['cci_20']\n",
    "    stock['tr']\n",
    "    stock['atr']\n",
    "    stock['dma']\n",
    "    stock['adxr']\n",
    "    stock['close_12_ema']\n",
    "    stock['trix']\n",
    "    stock['trix_9_sma']\n",
    "    stock['vr']\n",
    "    stock['vr_6_sma']\n",
    "    new_dataframe = pd.DataFrame(stock).loc[:, COLUMNS]\n",
    "    transformed_dataframe = new_dataframe.iloc[SKIP: len(new_dataframe) - SHIFT_NUMBER] # Bỏ các hàng khuyết dữ liệu\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(transformed_dataframe)\n",
    "    return pd.DataFrame(scaler.transform(transformed_dataframe), columns=COLUMNS)\n",
    "\n",
    "def readAsStockDataFrame(filename):\n",
    "    data = pd.read_csv(filename, sep=\",\", names=[\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"volume currency\", \"weighted price\"], skiprows = 1)\n",
    "    data = data.loc[:, [\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]]\n",
    "    return addTechnicalIndicators(data)\n",
    "\n",
    "def getPriceDiff(data, shift_number):\n",
    "    return np.concatenate((data.loc[:, \"Close\"].values[shift_number:] * 100 / data.loc[:, \"Close\"].values[: len(data) - shift_number] - 100, [0] * shift_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy, khi chạy đoạn code sau, ta được dữ liệu là các chỉ số tài chính tương ứng với các ngày giao dịch của Bitcoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ndoannguyen\\AppData\\Local\\Continuum\\anaconda2\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "D:\\Users\\ndoannguyen\\AppData\\Local\\Continuum\\anaconda2\\envs\\py35\\lib\\site-packages\\stockstats.py:387: FutureWarning: Currently, 'apply' passes the values as ndarrays to the applied function. In the future, this will change to passing it as Series objects. You need to specify 'raw=True' to keep the current behaviour, and you can pass 'raw=False' to silence this warning\n",
      "  lambda x: np.fabs(x - x.mean()).mean())\n"
     ]
    }
   ],
   "source": [
    "DATAFILE = \"Data/BTCPrice.csv\"\n",
    "X = readAsStockDataFrame(DATAFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>cr</th>\n",
       "      <th>cr-ma1</th>\n",
       "      <th>cr-ma2</th>\n",
       "      <th>cr-ma3</th>\n",
       "      <th>rsv_9</th>\n",
       "      <th>...</th>\n",
       "      <th>pdi_14</th>\n",
       "      <th>mdm_14</th>\n",
       "      <th>mdi_14</th>\n",
       "      <th>dx_14</th>\n",
       "      <th>adx</th>\n",
       "      <th>adxr</th>\n",
       "      <th>trix</th>\n",
       "      <th>change</th>\n",
       "      <th>vr</th>\n",
       "      <th>vr_6_sma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.557662</td>\n",
       "      <td>-0.555577</td>\n",
       "      <td>-0.567160</td>\n",
       "      <td>-0.558149</td>\n",
       "      <td>-0.684977</td>\n",
       "      <td>0.254511</td>\n",
       "      <td>-0.054549</td>\n",
       "      <td>2.535630</td>\n",
       "      <td>9.440731</td>\n",
       "      <td>-0.223739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512639</td>\n",
       "      <td>-0.389301</td>\n",
       "      <td>-0.390922</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>0.761042</td>\n",
       "      <td>1.047989</td>\n",
       "      <td>-0.435186</td>\n",
       "      <td>-0.146963</td>\n",
       "      <td>-0.961677</td>\n",
       "      <td>-0.787207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.557872</td>\n",
       "      <td>-0.555192</td>\n",
       "      <td>-0.568139</td>\n",
       "      <td>-0.558405</td>\n",
       "      <td>-0.154990</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>0.406567</td>\n",
       "      <td>1.990468</td>\n",
       "      <td>6.653909</td>\n",
       "      <td>-0.299411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109721</td>\n",
       "      <td>-0.386393</td>\n",
       "      <td>-0.291621</td>\n",
       "      <td>-0.491732</td>\n",
       "      <td>0.349655</td>\n",
       "      <td>0.856236</td>\n",
       "      <td>-0.416393</td>\n",
       "      <td>-0.189552</td>\n",
       "      <td>-1.131634</td>\n",
       "      <td>-0.838256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.557937</td>\n",
       "      <td>-0.556164</td>\n",
       "      <td>-0.569415</td>\n",
       "      <td>-0.562664</td>\n",
       "      <td>-0.298147</td>\n",
       "      <td>-0.298446</td>\n",
       "      <td>0.591809</td>\n",
       "      <td>1.894016</td>\n",
       "      <td>4.221087</td>\n",
       "      <td>-1.745237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216358</td>\n",
       "      <td>-0.381915</td>\n",
       "      <td>-0.136020</td>\n",
       "      <td>-1.063830</td>\n",
       "      <td>-0.155752</td>\n",
       "      <td>0.554994</td>\n",
       "      <td>-0.428084</td>\n",
       "      <td>-0.902234</td>\n",
       "      <td>-1.235631</td>\n",
       "      <td>-0.847340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.562264</td>\n",
       "      <td>-0.559159</td>\n",
       "      <td>-0.572859</td>\n",
       "      <td>-0.563380</td>\n",
       "      <td>0.127178</td>\n",
       "      <td>-0.620990</td>\n",
       "      <td>0.687280</td>\n",
       "      <td>1.880327</td>\n",
       "      <td>3.147333</td>\n",
       "      <td>-1.804377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.477820</td>\n",
       "      <td>-0.363203</td>\n",
       "      <td>0.573056</td>\n",
       "      <td>-0.839978</td>\n",
       "      <td>-0.428006</td>\n",
       "      <td>0.253150</td>\n",
       "      <td>-0.457949</td>\n",
       "      <td>-0.275354</td>\n",
       "      <td>-1.344351</td>\n",
       "      <td>-1.097693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.562965</td>\n",
       "      <td>-0.560509</td>\n",
       "      <td>-0.571985</td>\n",
       "      <td>-0.564381</td>\n",
       "      <td>-0.638554</td>\n",
       "      <td>-0.664228</td>\n",
       "      <td>0.828384</td>\n",
       "      <td>1.773829</td>\n",
       "      <td>2.466648</td>\n",
       "      <td>-2.103575</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.617778</td>\n",
       "      <td>-0.371092</td>\n",
       "      <td>0.349826</td>\n",
       "      <td>-0.839978</td>\n",
       "      <td>-0.620778</td>\n",
       "      <td>-0.023577</td>\n",
       "      <td>-0.499683</td>\n",
       "      <td>-0.328561</td>\n",
       "      <td>-1.381465</td>\n",
       "      <td>-1.223595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.563976</td>\n",
       "      <td>-0.561813</td>\n",
       "      <td>-0.595409</td>\n",
       "      <td>-0.586909</td>\n",
       "      <td>3.824167</td>\n",
       "      <td>-1.288034</td>\n",
       "      <td>0.511783</td>\n",
       "      <td>1.623167</td>\n",
       "      <td>1.930341</td>\n",
       "      <td>-2.592556</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.126243</td>\n",
       "      <td>-0.221296</td>\n",
       "      <td>3.730147</td>\n",
       "      <td>1.360778</td>\n",
       "      <td>0.066295</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>-0.640604</td>\n",
       "      <td>-4.326138</td>\n",
       "      <td>-1.567069</td>\n",
       "      <td>-1.352342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.586539</td>\n",
       "      <td>-0.573525</td>\n",
       "      <td>-0.598206</td>\n",
       "      <td>-0.578192</td>\n",
       "      <td>2.155016</td>\n",
       "      <td>-1.342460</td>\n",
       "      <td>0.037690</td>\n",
       "      <td>0.660938</td>\n",
       "      <td>1.835456</td>\n",
       "      <td>-1.381047</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.280835</td>\n",
       "      <td>-0.231281</td>\n",
       "      <td>2.899891</td>\n",
       "      <td>1.440978</td>\n",
       "      <td>0.584858</td>\n",
       "      <td>0.192286</td>\n",
       "      <td>-0.787193</td>\n",
       "      <td>1.786625</td>\n",
       "      <td>-1.259720</td>\n",
       "      <td>-1.405244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.577642</td>\n",
       "      <td>-0.573813</td>\n",
       "      <td>-0.589815</td>\n",
       "      <td>-0.580379</td>\n",
       "      <td>-0.096608</td>\n",
       "      <td>-1.233583</td>\n",
       "      <td>-0.280532</td>\n",
       "      <td>0.230979</td>\n",
       "      <td>1.821989</td>\n",
       "      <td>-1.509907</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.339793</td>\n",
       "      <td>-0.257860</td>\n",
       "      <td>2.363016</td>\n",
       "      <td>1.440978</td>\n",
       "      <td>0.954091</td>\n",
       "      <td>0.446360</td>\n",
       "      <td>-0.933997</td>\n",
       "      <td>-0.594695</td>\n",
       "      <td>-1.307641</td>\n",
       "      <td>-1.436485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.580215</td>\n",
       "      <td>-0.574633</td>\n",
       "      <td>-0.590644</td>\n",
       "      <td>-0.578652</td>\n",
       "      <td>-0.286669</td>\n",
       "      <td>-1.223358</td>\n",
       "      <td>-0.604645</td>\n",
       "      <td>0.224193</td>\n",
       "      <td>1.717222</td>\n",
       "      <td>-1.267710</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.391286</td>\n",
       "      <td>-0.275034</td>\n",
       "      <td>2.026806</td>\n",
       "      <td>1.468815</td>\n",
       "      <td>1.227612</td>\n",
       "      <td>0.716232</td>\n",
       "      <td>-1.062485</td>\n",
       "      <td>0.218046</td>\n",
       "      <td>-1.228005</td>\n",
       "      <td>-1.435132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.578405</td>\n",
       "      <td>-0.575123</td>\n",
       "      <td>-0.589959</td>\n",
       "      <td>-0.581480</td>\n",
       "      <td>-0.258532</td>\n",
       "      <td>-1.218921</td>\n",
       "      <td>-0.889590</td>\n",
       "      <td>0.154650</td>\n",
       "      <td>1.569007</td>\n",
       "      <td>-1.613064</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.430845</td>\n",
       "      <td>-0.294892</td>\n",
       "      <td>1.652392</td>\n",
       "      <td>1.468815</td>\n",
       "      <td>1.422671</td>\n",
       "      <td>0.971971</td>\n",
       "      <td>-1.181011</td>\n",
       "      <td>-0.729020</td>\n",
       "      <td>-1.268552</td>\n",
       "      <td>-1.421677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       open      high       low     close    volume        cr    cr-ma1  \\\n",
       "0 -0.557662 -0.555577 -0.567160 -0.558149 -0.684977  0.254511 -0.054549   \n",
       "1 -0.557872 -0.555192 -0.568139 -0.558405 -0.154990  0.013675  0.406567   \n",
       "2 -0.557937 -0.556164 -0.569415 -0.562664 -0.298147 -0.298446  0.591809   \n",
       "3 -0.562264 -0.559159 -0.572859 -0.563380  0.127178 -0.620990  0.687280   \n",
       "4 -0.562965 -0.560509 -0.571985 -0.564381 -0.638554 -0.664228  0.828384   \n",
       "5 -0.563976 -0.561813 -0.595409 -0.586909  3.824167 -1.288034  0.511783   \n",
       "6 -0.586539 -0.573525 -0.598206 -0.578192  2.155016 -1.342460  0.037690   \n",
       "7 -0.577642 -0.573813 -0.589815 -0.580379 -0.096608 -1.233583 -0.280532   \n",
       "8 -0.580215 -0.574633 -0.590644 -0.578652 -0.286669 -1.223358 -0.604645   \n",
       "9 -0.578405 -0.575123 -0.589959 -0.581480 -0.258532 -1.218921 -0.889590   \n",
       "\n",
       "     cr-ma2    cr-ma3     rsv_9    ...       pdi_14    mdm_14    mdi_14  \\\n",
       "0  2.535630  9.440731 -0.223739    ...     0.512639 -0.389301 -0.390922   \n",
       "1  1.990468  6.653909 -0.299411    ...     0.109721 -0.386393 -0.291621   \n",
       "2  1.894016  4.221087 -1.745237    ...    -0.216358 -0.381915 -0.136020   \n",
       "3  1.880327  3.147333 -1.804377    ...    -0.477820 -0.363203  0.573056   \n",
       "4  1.773829  2.466648 -2.103575    ...    -0.617778 -0.371092  0.349826   \n",
       "5  1.623167  1.930341 -2.592556    ...    -1.126243 -0.221296  3.730147   \n",
       "6  0.660938  1.835456 -1.381047    ...    -1.280835 -0.231281  2.899891   \n",
       "7  0.230979  1.821989 -1.509907    ...    -1.339793 -0.257860  2.363016   \n",
       "8  0.224193  1.717222 -1.267710    ...    -1.391286 -0.275034  2.026806   \n",
       "9  0.154650  1.569007 -1.613064    ...    -1.430845 -0.294892  1.652392   \n",
       "\n",
       "      dx_14       adx      adxr      trix    change        vr  vr_6_sma  \n",
       "0 -0.017700  0.761042  1.047989 -0.435186 -0.146963 -0.961677 -0.787207  \n",
       "1 -0.491732  0.349655  0.856236 -0.416393 -0.189552 -1.131634 -0.838256  \n",
       "2 -1.063830 -0.155752  0.554994 -0.428084 -0.902234 -1.235631 -0.847340  \n",
       "3 -0.839978 -0.428006  0.253150 -0.457949 -0.275354 -1.344351 -1.097693  \n",
       "4 -0.839978 -0.620778 -0.023577 -0.499683 -0.328561 -1.381465 -1.223595  \n",
       "5  1.360778  0.066295  0.003966 -0.640604 -4.326138 -1.567069 -1.352342  \n",
       "6  1.440978  0.584858  0.192286 -0.787193  1.786625 -1.259720 -1.405244  \n",
       "7  1.440978  0.954091  0.446360 -0.933997 -0.594695 -1.307641 -1.436485  \n",
       "8  1.468815  1.227612  0.716232 -1.062485  0.218046 -1.228005 -1.435132  \n",
       "9  1.468815  1.422671  0.971971 -1.181011 -0.729020 -1.268552 -1.421677  \n",
       "\n",
       "[10 rows x 42 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw = getPriceDiff(pd.read_csv(DATAFILE, sep=\",\"), SHIFT_NUMBER)\n",
    "y = y_raw[SKIP: len(y_raw) - 7]\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 1. Polynomial Regression\n",
    "\n",
    "***Thực hiện lại việc train với Polynomial Regression với regularization l1, hệ số phạt `alpha` = 0.1, với `random_state = 0` khi dùng `train_test_split` và dùng 50% dữ liệu để validation. Tính RSE (bình phương của RMSE).*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb_iterations used:  15170\n",
      "TrainMSE:  29.994188126537928\n",
      "ValidationMSE:  87.41474630003407\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 0)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "X_train_transformed = poly.fit_transform(X_train)\n",
    "X_test_transformed = poly.fit_transform(X_test)\n",
    "model = Lasso(fit_intercept = False, max_iter = 100000, alpha = alpha)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "y_predict = model.predict(X_test_transformed)\n",
    "y_predict_train = model.predict(X_train_transformed)\n",
    "print(\"Nb_iterations used: \", model.n_iter_)\n",
    "print(\"TrainMSE: \", mean_squared_error(y_train, y_predict_train))\n",
    "print(\"ValidationMSE: \", mean_squared_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Giảm size của tập validation còn 20% và tính lại RSE trên các tập train, validation.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb_iterations used:  17747\n",
      "TrainMSE:  39.21048873914117\n",
      "ValidationMSE:  59.856822857924\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "alpha = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "X_train_transformed = poly.fit_transform(X_train)\n",
    "X_test_transformed = poly.fit_transform(X_test)\n",
    "model = Lasso(fit_intercept = False, max_iter = 100000, alpha = alpha)\n",
    "model.fit(X_train_transformed, y_train)\n",
    "y_predict = model.predict(X_test_transformed)\n",
    "y_predict_train = model.predict(X_train_transformed)\n",
    "print(\"Nb_iterations used: \", model.n_iter_)\n",
    "print(\"TrainMSE: \", mean_squared_error(y_train, y_predict_train))\n",
    "print(\"ValidationMSE: \", mean_squared_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận xét: Bài toán của ta gặp phải vấn đề thiếu dữ liệu trong khi thuật toán sử dụng phức tạp. Overfitting giảm khi số lượng dữ liệu dùng cho huấn luyện tăng lên. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 2. FFN 2 tầng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - 3s 6ms/step - loss: 140.0365\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 137.4004\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 135.0456\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 132.8879\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 130.8707\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - 0s 19us/step - loss: 128.9566\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 127.0942\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - 0s 21us/step - loss: 125.3209\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 123.5996\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 121.9257\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 120.3003\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - 0s 17us/step - loss: 118.7304\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 117.2268\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 115.7989\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 114.4511\n",
      "Epoch 16/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 113.1833\n",
      "Epoch 17/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 111.9932\n",
      "Epoch 18/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 110.8780\n",
      "Epoch 19/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 109.8359\n",
      "Epoch 20/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 108.8670\n",
      "Epoch 21/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 107.9682\n",
      "Epoch 22/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 107.1330\n",
      "Epoch 23/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 106.3549\n",
      "Epoch 24/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 105.6283\n",
      "Epoch 25/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 104.9479\n",
      "Epoch 26/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 104.3079\n",
      "Epoch 27/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 103.7020\n",
      "Epoch 28/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 103.1223\n",
      "Epoch 29/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 102.5583\n",
      "Epoch 30/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 101.9955\n",
      "Epoch 31/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 101.4263\n",
      "Epoch 32/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 100.8833\n",
      "Epoch 33/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 100.3866\n",
      "Epoch 34/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 99.9114\n",
      "Epoch 35/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 99.4497\n",
      "Epoch 36/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 98.9986\n",
      "Epoch 37/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 98.5555\n",
      "Epoch 38/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 98.1181\n",
      "Epoch 39/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 97.6852\n",
      "Epoch 40/1000\n",
      "572/572 [==============================] - 0s 14us/step - loss: 97.2555\n",
      "Epoch 41/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 96.8290\n",
      "Epoch 42/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 96.4054\n",
      "Epoch 43/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 95.9846\n",
      "Epoch 44/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 95.5660\n",
      "Epoch 45/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 95.1494\n",
      "Epoch 46/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 94.7341\n",
      "Epoch 47/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 94.3197\n",
      "Epoch 48/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 93.9029\n",
      "Epoch 49/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 93.4893\n",
      "Epoch 50/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 93.0759\n",
      "Epoch 51/1000\n",
      "572/572 [==============================] - 0s 14us/step - loss: 92.6625\n",
      "Epoch 52/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 92.2495\n",
      "Epoch 53/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 91.8371\n",
      "Epoch 54/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 91.4258\n",
      "Epoch 55/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 91.0160\n",
      "Epoch 56/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 90.6074\n",
      "Epoch 57/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 90.2002\n",
      "Epoch 58/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 89.7941\n",
      "Epoch 59/1000\n",
      "572/572 [==============================] - 0s 14us/step - loss: 89.3892\n",
      "Epoch 60/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 88.9852\n",
      "Epoch 61/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 88.5820\n",
      "Epoch 62/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 88.1792\n",
      "Epoch 63/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 87.7761\n",
      "Epoch 64/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 87.3714\n",
      "Epoch 65/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 86.9634\n",
      "Epoch 66/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 86.5521\n",
      "Epoch 67/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 86.1427\n",
      "Epoch 68/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 85.7413\n",
      "Epoch 69/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 85.3472\n",
      "Epoch 70/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 84.9578\n",
      "Epoch 71/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 84.5719\n",
      "Epoch 72/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 84.1889\n",
      "Epoch 73/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 83.8086\n",
      "Epoch 74/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 83.4307\n",
      "Epoch 75/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 83.0551\n",
      "Epoch 76/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 82.6817\n",
      "Epoch 77/1000\n",
      "572/572 [==============================] - 0s 23us/step - loss: 82.3103\n",
      "Epoch 78/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 81.9405\n",
      "Epoch 79/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 81.5723\n",
      "Epoch 80/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 81.2055\n",
      "Epoch 81/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 80.8402\n",
      "Epoch 82/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 80.4763\n",
      "Epoch 83/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 80.1138\n",
      "Epoch 84/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 79.7529\n",
      "Epoch 85/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 79.3939\n",
      "Epoch 86/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 79.0372\n",
      "Epoch 87/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 78.6825\n",
      "Epoch 88/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 78.3291\n",
      "Epoch 89/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 77.9770\n",
      "Epoch 90/1000\n",
      "572/572 [==============================] - 0s 17us/step - loss: 77.6263\n",
      "Epoch 91/1000\n",
      "572/572 [==============================] - 0s 17us/step - loss: 77.2777\n",
      "Epoch 92/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 76.9315\n",
      "Epoch 93/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 76.5876\n",
      "Epoch 94/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 76.2456\n",
      "Epoch 95/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 75.9055\n",
      "Epoch 96/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 75.5671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 75.2303\n",
      "Epoch 98/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 74.8950\n",
      "Epoch 99/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 74.5612\n",
      "Epoch 100/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 74.2289\n",
      "Epoch 101/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 73.8982\n",
      "Epoch 102/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 73.5694\n",
      "Epoch 103/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 73.2432\n",
      "Epoch 104/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 72.9216\n",
      "Epoch 105/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 72.6095\n",
      "Epoch 106/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 72.3183\n",
      "Epoch 107/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 72.0827\n",
      "Epoch 108/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 71.9707\n",
      "Epoch 109/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 72.2385\n",
      "Epoch 110/1000\n",
      "572/572 [==============================] - 0s 14us/step - loss: 73.1954\n",
      "Epoch 111/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 76.6577\n",
      "Epoch 112/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 81.5546\n",
      "Epoch 113/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 96.1676\n",
      "Epoch 114/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 97.1311\n",
      "Epoch 115/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 106.7067\n",
      "Epoch 116/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 88.8274\n",
      "Epoch 117/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 83.7653\n",
      "Epoch 118/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 76.0414\n",
      "Epoch 119/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 74.2450\n",
      "Epoch 120/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 72.0388\n",
      "Epoch 121/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 71.6235\n",
      "Epoch 122/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 70.6637\n",
      "Epoch 123/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 70.6900\n",
      "Epoch 124/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 70.0574\n",
      "Epoch 125/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 70.3644\n",
      "Epoch 126/1000\n",
      "572/572 [==============================] - 0s 14us/step - loss: 69.8862\n",
      "Epoch 127/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 70.5620\n",
      "Epoch 128/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 70.1297\n",
      "Epoch 129/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 71.2788\n",
      "Epoch 130/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 70.7211\n",
      "Epoch 131/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 72.3331\n",
      "Epoch 132/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 71.3831\n",
      "Epoch 133/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 73.2000\n",
      "Epoch 134/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 71.6587\n",
      "Epoch 135/1000\n",
      "572/572 [==============================] - 0s 26us/step - loss: 73.2818\n",
      "Epoch 136/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 71.2564\n",
      "Epoch 137/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 72.4621\n",
      "Epoch 138/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 70.3039\n",
      "Epoch 139/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 71.1472\n",
      "Epoch 140/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 69.1534\n",
      "Epoch 141/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 69.8001\n",
      "Epoch 142/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 68.0798\n",
      "Epoch 143/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 68.6669\n",
      "Epoch 144/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 67.1996\n",
      "Epoch 145/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 67.8106\n",
      "Epoch 146/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 66.5282\n",
      "Epoch 147/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 67.2095\n",
      "Epoch 148/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 66.0389\n",
      "Epoch 149/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 66.8144\n",
      "Epoch 150/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 65.6898\n",
      "Epoch 151/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 66.5640\n",
      "Epoch 152/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 65.4287\n",
      "Epoch 153/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 66.3863\n",
      "Epoch 154/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 65.1956\n",
      "Epoch 155/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 66.2023\n",
      "Epoch 156/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 64.9300\n",
      "Epoch 157/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 65.9415\n",
      "Epoch 158/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 64.5862\n",
      "Epoch 159/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 65.5620\n",
      "Epoch 160/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 64.1459\n",
      "Epoch 161/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 65.0616\n",
      "Epoch 162/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 63.6217\n",
      "Epoch 163/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 64.4729\n",
      "Epoch 164/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 63.0466\n",
      "Epoch 165/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 63.8447\n",
      "Epoch 166/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 62.4594\n",
      "Epoch 167/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 63.2239\n",
      "Epoch 168/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 61.8925\n",
      "Epoch 169/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 62.6441\n",
      "Epoch 170/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 61.3669\n",
      "Epoch 171/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 62.1230\n",
      "Epoch 172/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 60.8915\n",
      "Epoch 173/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 61.6643\n",
      "Epoch 174/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 60.4670\n",
      "Epoch 175/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 61.2104\n",
      "Epoch 176/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 60.0146\n",
      "Epoch 177/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 60.8016\n",
      "Epoch 178/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 59.6270\n",
      "Epoch 179/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 60.4368\n",
      "Epoch 180/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 59.2685\n",
      "Epoch 181/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 60.0968\n",
      "Epoch 182/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 58.9210\n",
      "Epoch 183/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 59.7564\n",
      "Epoch 184/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 58.5648\n",
      "Epoch 185/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 59.3939\n",
      "Epoch 186/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 58.1846\n",
      "Epoch 187/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 58.9941\n",
      "Epoch 188/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 57.7716\n",
      "Epoch 189/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 58.5521\n",
      "Epoch 190/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 57.3255\n",
      "Epoch 191/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 58.0727\n",
      "Epoch 192/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 56.8535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 57.5685\n",
      "Epoch 194/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 56.3675\n",
      "Epoch 195/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 57.0556\n",
      "Epoch 196/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 55.8810\n",
      "Epoch 197/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 56.5499\n",
      "Epoch 198/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 55.4059\n",
      "Epoch 199/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 56.0639\n",
      "Epoch 200/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 54.9511\n",
      "Epoch 201/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 55.6055\n",
      "Epoch 202/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 54.5212\n",
      "Epoch 203/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 55.1772\n",
      "Epoch 204/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 54.1165\n",
      "Epoch 205/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 54.7767\n",
      "Epoch 206/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 53.7335\n",
      "Epoch 207/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 54.3978\n",
      "Epoch 208/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 53.3660\n",
      "Epoch 209/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 54.0318\n",
      "Epoch 210/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 53.0059\n",
      "Epoch 211/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 53.6687\n",
      "Epoch 212/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 52.6450\n",
      "Epoch 213/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 53.2996\n",
      "Epoch 214/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 52.2765\n",
      "Epoch 215/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 52.9175\n",
      "Epoch 216/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 51.8957\n",
      "Epoch 217/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 52.5195\n",
      "Epoch 218/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 51.5018\n",
      "Epoch 219/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 52.1065\n",
      "Epoch 220/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 51.0972\n",
      "Epoch 221/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 51.6834\n",
      "Epoch 222/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 50.6871\n",
      "Epoch 223/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 51.2576\n",
      "Epoch 224/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 50.2787\n",
      "Epoch 225/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 50.8381\n",
      "Epoch 226/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 49.8797\n",
      "Epoch 227/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 50.4334\n",
      "Epoch 228/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 49.4967\n",
      "Epoch 229/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 50.0504\n",
      "Epoch 230/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 49.1350\n",
      "Epoch 231/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 49.6937\n",
      "Epoch 232/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 48.7970\n",
      "Epoch 233/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 49.3655\n",
      "Epoch 234/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 48.4832\n",
      "Epoch 235/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 49.0648\n",
      "Epoch 236/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 48.1912\n",
      "Epoch 237/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 48.7885\n",
      "Epoch 238/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 47.9175\n",
      "Epoch 239/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 48.5318\n",
      "Epoch 240/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 47.6611\n",
      "Epoch 241/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 48.2924\n",
      "Epoch 242/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 47.4178\n",
      "Epoch 243/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 48.0567\n",
      "Epoch 244/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 47.1628\n",
      "Epoch 245/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 47.7865\n",
      "Epoch 246/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 46.8598\n",
      "Epoch 247/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 47.4407\n",
      "Epoch 248/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 46.4821\n",
      "Epoch 249/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 46.9992\n",
      "Epoch 250/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 46.0292\n",
      "Epoch 251/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 46.4833\n",
      "Epoch 252/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 45.5511\n",
      "Epoch 253/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 45.9784\n",
      "Epoch 254/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 45.1175\n",
      "Epoch 255/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 45.5475\n",
      "Epoch 256/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 44.7188\n",
      "Epoch 257/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 45.1695\n",
      "Epoch 258/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 44.3460\n",
      "Epoch 259/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 44.8153\n",
      "Epoch 260/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 43.9917\n",
      "Epoch 261/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 44.4390\n",
      "Epoch 262/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 43.6289\n",
      "Epoch 263/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 44.0474\n",
      "Epoch 264/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 43.2580\n",
      "Epoch 265/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 43.6558\n",
      "Epoch 266/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 42.8927\n",
      "Epoch 267/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 43.2792\n",
      "Epoch 268/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 42.5455\n",
      "Epoch 269/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 42.9296\n",
      "Epoch 270/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 42.2260\n",
      "Epoch 271/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 42.6147\n",
      "Epoch 272/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 41.9390\n",
      "Epoch 273/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 42.3369\n",
      "Epoch 274/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 41.6846\n",
      "Epoch 275/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 42.0934\n",
      "Epoch 276/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 41.4583\n",
      "Epoch 277/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 41.8768\n",
      "Epoch 278/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 41.2521\n",
      "Epoch 279/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 41.6764\n",
      "Epoch 280/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 41.0552\n",
      "Epoch 281/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 41.4792\n",
      "Epoch 282/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 40.8557\n",
      "Epoch 283/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 41.2722\n",
      "Epoch 284/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 40.6425\n",
      "Epoch 285/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 41.0442\n",
      "Epoch 286/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 40.4070\n",
      "Epoch 287/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 40.7880\n",
      "Epoch 288/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 40.1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 40.5017\n",
      "Epoch 290/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 39.8565\n",
      "Epoch 291/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 40.1887\n",
      "Epoch 292/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 39.5474\n",
      "Epoch 293/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 39.8574\n",
      "Epoch 294/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 39.2261\n",
      "Epoch 295/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 39.5188\n",
      "Epoch 296/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 38.9033\n",
      "Epoch 297/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 39.1844\n",
      "Epoch 298/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 38.5892\n",
      "Epoch 299/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 38.8650\n",
      "Epoch 300/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 38.2922\n",
      "Epoch 301/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 38.5682\n",
      "Epoch 302/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 38.0183\n",
      "Epoch 303/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 38.2987\n",
      "Epoch 304/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 37.7700\n",
      "Epoch 305/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 38.0573\n",
      "Epoch 306/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 37.5467\n",
      "Epoch 307/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 37.8415\n",
      "Epoch 308/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 37.3446\n",
      "Epoch 309/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 37.6456\n",
      "Epoch 310/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 37.1574\n",
      "Epoch 311/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 37.4615\n",
      "Epoch 312/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 36.9772\n",
      "Epoch 313/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 37.2800\n",
      "Epoch 314/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 36.7950\n",
      "Epoch 315/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 37.0912\n",
      "Epoch 316/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 36.6024\n",
      "Epoch 317/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 36.8870\n",
      "Epoch 318/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 36.3929\n",
      "Epoch 319/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 36.6619\n",
      "Epoch 320/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 36.1631\n",
      "Epoch 321/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 36.4141\n",
      "Epoch 322/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 35.9131\n",
      "Epoch 323/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 36.1458\n",
      "Epoch 324/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 35.6463\n",
      "Epoch 325/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 35.8627\n",
      "Epoch 326/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 35.3690\n",
      "Epoch 327/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 35.5726\n",
      "Epoch 328/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 35.0890\n",
      "Epoch 329/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 35.2842\n",
      "Epoch 330/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 34.8141\n",
      "Epoch 331/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 35.0055\n",
      "Epoch 332/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 34.5511\n",
      "Epoch 333/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 34.7429\n",
      "Epoch 334/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 34.3050\n",
      "Epoch 335/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 34.5007\n",
      "Epoch 336/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 34.0788\n",
      "Epoch 337/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 34.2803\n",
      "Epoch 338/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 33.8726\n",
      "Epoch 339/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 34.0809\n",
      "Epoch 340/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 33.6847\n",
      "Epoch 341/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 33.8992\n",
      "Epoch 342/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 33.5111\n",
      "Epoch 343/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 33.7301\n",
      "Epoch 344/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 33.3466\n",
      "Epoch 345/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 33.5675\n",
      "Epoch 346/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 33.1852\n",
      "Epoch 347/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 33.4048\n",
      "Epoch 348/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 33.0207\n",
      "Epoch 349/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 33.2358\n",
      "Epoch 350/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 32.8480\n",
      "Epoch 351/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 33.0558\n",
      "Epoch 352/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 32.6632\n",
      "Epoch 353/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 32.8619\n",
      "Epoch 354/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 32.4647\n",
      "Epoch 355/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 32.6533\n",
      "Epoch 356/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 32.2528\n",
      "Epoch 357/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 32.4319\n",
      "Epoch 358/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 32.0301\n",
      "Epoch 359/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 32.2011\n",
      "Epoch 360/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 31.8004\n",
      "Epoch 361/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 31.9657\n",
      "Epoch 362/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 31.5685\n",
      "Epoch 363/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 31.7310\n",
      "Epoch 364/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 31.3392\n",
      "Epoch 365/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 31.5019\n",
      "Epoch 366/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 31.1168\n",
      "Epoch 367/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 31.2822\n",
      "Epoch 368/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 30.9044\n",
      "Epoch 369/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 31.0746\n",
      "Epoch 370/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 30.7039\n",
      "Epoch 371/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 30.8800\n",
      "Epoch 372/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 30.5155\n",
      "Epoch 373/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 30.6978\n",
      "Epoch 374/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 30.3380\n",
      "Epoch 375/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 30.5261\n",
      "Epoch 376/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 30.1693\n",
      "Epoch 377/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 30.3618\n",
      "Epoch 378/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 30.0063\n",
      "Epoch 379/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 30.2013\n",
      "Epoch 380/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 29.8453\n",
      "Epoch 381/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 30.0408\n",
      "Epoch 382/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 29.6830\n",
      "Epoch 383/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 29.8767\n",
      "Epoch 384/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 29.5163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 29.7063\n",
      "Epoch 386/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 29.3433\n",
      "Epoch 387/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 29.5281\n",
      "Epoch 388/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 29.1628\n",
      "Epoch 389/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 29.3415\n",
      "Epoch 390/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 28.9750\n",
      "Epoch 391/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 29.1474\n",
      "Epoch 392/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 28.7811\n",
      "Epoch 393/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 28.9476\n",
      "Epoch 394/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 28.5832\n",
      "Epoch 395/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 28.7449\n",
      "Epoch 396/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 28.3841\n",
      "Epoch 397/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 28.5423\n",
      "Epoch 398/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 28.1866\n",
      "Epoch 399/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 28.3427\n",
      "Epoch 400/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 27.9933\n",
      "Epoch 401/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 28.1489\n",
      "Epoch 402/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 27.8067\n",
      "Epoch 403/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 27.9630\n",
      "Epoch 404/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 27.6283\n",
      "Epoch 405/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 27.7864\n",
      "Epoch 406/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 27.4591\n",
      "Epoch 407/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 27.6196\n",
      "Epoch 408/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 27.2992\n",
      "Epoch 409/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 27.4624\n",
      "Epoch 410/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 27.1481\n",
      "Epoch 411/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 27.3138\n",
      "Epoch 412/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 27.0045\n",
      "Epoch 413/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 27.1722\n",
      "Epoch 414/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 26.8665\n",
      "Epoch 415/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 27.0355\n",
      "Epoch 416/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 26.7322\n",
      "Epoch 417/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 26.9013\n",
      "Epoch 418/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 26.5994\n",
      "Epoch 419/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 26.7674\n",
      "Epoch 420/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 26.4658\n",
      "Epoch 421/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 26.6317\n",
      "Epoch 422/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 26.3296\n",
      "Epoch 423/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 26.4923\n",
      "Epoch 424/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 26.1895\n",
      "Epoch 425/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 26.3483\n",
      "Epoch 426/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 26.0448\n",
      "Epoch 427/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 26.1991\n",
      "Epoch 428/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 25.8952\n",
      "Epoch 429/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 26.0449\n",
      "Epoch 430/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 25.7411\n",
      "Epoch 431/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 25.8864\n",
      "Epoch 432/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 25.5838\n",
      "Epoch 433/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 25.7251\n",
      "Epoch 434/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 25.4244\n",
      "Epoch 435/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 25.5625\n",
      "Epoch 436/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 25.2647\n",
      "Epoch 437/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 25.4004\n",
      "Epoch 438/1000\n",
      "572/572 [==============================] - 0s 16us/step - loss: 25.1063\n",
      "Epoch 439/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 25.2404\n",
      "Epoch 440/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 24.9507\n",
      "Epoch 441/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 25.0842\n",
      "Epoch 442/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.7992\n",
      "Epoch 443/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.9328\n",
      "Epoch 444/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.6528\n",
      "Epoch 445/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 24.7869\n",
      "Epoch 446/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.5119\n",
      "Epoch 447/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.6470\n",
      "Epoch 448/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 24.3766\n",
      "Epoch 449/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 24.5128\n",
      "Epoch 450/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.2466\n",
      "Epoch 451/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.3838\n",
      "Epoch 452/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 24.1211\n",
      "Epoch 453/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 24.2591\n",
      "Epoch 454/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 23.9993\n",
      "Epoch 455/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.1375\n",
      "Epoch 456/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 23.8799\n",
      "Epoch 457/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 24.0178\n",
      "Epoch 458/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 23.7617\n",
      "Epoch 459/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 23.8987\n",
      "Epoch 460/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 23.6435\n",
      "Epoch 461/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 23.7790\n",
      "Epoch 462/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 23.5242\n",
      "Epoch 463/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 23.6576\n",
      "Epoch 464/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 23.4030\n",
      "Epoch 465/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 23.5338\n",
      "Epoch 466/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 23.2794\n",
      "Epoch 467/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 23.4073\n",
      "Epoch 468/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 23.1531\n",
      "Epoch 469/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 23.2780\n",
      "Epoch 470/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 23.0243\n",
      "Epoch 471/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 23.1462\n",
      "Epoch 472/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 22.8933\n",
      "Epoch 473/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 23.0123\n",
      "Epoch 474/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 22.7607\n",
      "Epoch 475/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 22.8773\n",
      "Epoch 476/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 22.6274\n",
      "Epoch 477/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 22.7419\n",
      "Epoch 478/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 22.4943\n",
      "Epoch 479/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 22.6071\n",
      "Epoch 480/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 22.3622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 22.4737\n",
      "Epoch 482/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 22.2318\n",
      "Epoch 483/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 22.3426\n",
      "Epoch 484/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 22.1038\n",
      "Epoch 485/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 22.2142\n",
      "Epoch 486/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 21.9786\n",
      "Epoch 487/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 22.0888\n",
      "Epoch 488/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 21.8565\n",
      "Epoch 489/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 21.9666\n",
      "Epoch 490/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 21.7374\n",
      "Epoch 491/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 21.8476\n",
      "Epoch 492/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 21.6211\n",
      "Epoch 493/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 21.7312\n",
      "Epoch 494/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 21.5073\n",
      "Epoch 495/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 21.6172\n",
      "Epoch 496/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 21.3955\n",
      "Epoch 497/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 21.5049\n",
      "Epoch 498/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 21.2850\n",
      "Epoch 499/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 21.3936\n",
      "Epoch 500/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 21.1752\n",
      "Epoch 501/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 21.2827\n",
      "Epoch 502/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 21.0656\n",
      "Epoch 503/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 21.1717\n",
      "Epoch 504/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 20.9557\n",
      "Epoch 505/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 21.0601\n",
      "Epoch 506/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.8449\n",
      "Epoch 507/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.9474\n",
      "Epoch 508/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 20.7332\n",
      "Epoch 509/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.8337\n",
      "Epoch 510/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 20.6204\n",
      "Epoch 511/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.7188\n",
      "Epoch 512/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.5066\n",
      "Epoch 513/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 20.6029\n",
      "Epoch 514/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 20.3920\n",
      "Epoch 515/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.4865\n",
      "Epoch 516/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.2771\n",
      "Epoch 517/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 20.3698\n",
      "Epoch 518/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 20.1623\n",
      "Epoch 519/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.2535\n",
      "Epoch 520/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 20.0480\n",
      "Epoch 521/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 20.1380\n",
      "Epoch 522/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.9349\n",
      "Epoch 523/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 20.0239\n",
      "Epoch 524/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 19.8232\n",
      "Epoch 525/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.9116\n",
      "Epoch 526/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.7135\n",
      "Epoch 527/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.8015\n",
      "Epoch 528/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.6060\n",
      "Epoch 529/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.6937\n",
      "Epoch 530/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.5008\n",
      "Epoch 531/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 19.5884\n",
      "Epoch 532/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 19.3980\n",
      "Epoch 533/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.4855\n",
      "Epoch 534/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 19.2975\n",
      "Epoch 535/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.3850\n",
      "Epoch 536/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 19.1991\n",
      "Epoch 537/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 19.2865\n",
      "Epoch 538/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 19.1025\n",
      "Epoch 539/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 19.1897\n",
      "Epoch 540/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 19.0074\n",
      "Epoch 541/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 19.0941\n",
      "Epoch 542/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.9133\n",
      "Epoch 543/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.9995\n",
      "Epoch 544/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 18.8199\n",
      "Epoch 545/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 18.9053\n",
      "Epoch 546/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.7268\n",
      "Epoch 547/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.8113\n",
      "Epoch 548/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.6336\n",
      "Epoch 549/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 18.7170\n",
      "Epoch 550/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 18.5400\n",
      "Epoch 551/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 18.6222\n",
      "Epoch 552/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 18.4459\n",
      "Epoch 553/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.5267\n",
      "Epoch 554/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 18.3511\n",
      "Epoch 555/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 18.4306\n",
      "Epoch 556/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 18.2557\n",
      "Epoch 557/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.3337\n",
      "Epoch 558/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 18.1596\n",
      "Epoch 559/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.2363\n",
      "Epoch 560/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 18.0632\n",
      "Epoch 561/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.1386\n",
      "Epoch 562/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.9665\n",
      "Epoch 563/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 18.0407\n",
      "Epoch 564/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 17.8699\n",
      "Epoch 565/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.9431\n",
      "Epoch 566/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 17.7736\n",
      "Epoch 567/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 17.8459\n",
      "Epoch 568/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 17.6779\n",
      "Epoch 569/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 17.7494\n",
      "Epoch 570/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.5832\n",
      "Epoch 571/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 17.6541\n",
      "Epoch 572/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.4895\n",
      "Epoch 573/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 17.5599\n",
      "Epoch 574/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.3972\n",
      "Epoch 575/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.4673\n",
      "Epoch 576/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.3064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 577/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.3762\n",
      "Epoch 578/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 17.2172\n",
      "Epoch 579/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 17.2867\n",
      "Epoch 580/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 17.1295\n",
      "Epoch 581/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.1989\n",
      "Epoch 582/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 17.0435\n",
      "Epoch 583/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 17.1127\n",
      "Epoch 584/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 16.9589\n",
      "Epoch 585/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 17.0279\n",
      "Epoch 586/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 16.8758\n",
      "Epoch 587/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.9445\n",
      "Epoch 588/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 16.7939\n",
      "Epoch 589/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.8622\n",
      "Epoch 590/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.7130\n",
      "Epoch 591/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 16.7810\n",
      "Epoch 592/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.6330\n",
      "Epoch 593/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.7005\n",
      "Epoch 594/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.5537\n",
      "Epoch 595/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 16.6206\n",
      "Epoch 596/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.4750\n",
      "Epoch 597/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 16.5411\n",
      "Epoch 598/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.3966\n",
      "Epoch 599/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 16.4620\n",
      "Epoch 600/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.3184\n",
      "Epoch 601/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.3830\n",
      "Epoch 602/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 16.2404\n",
      "Epoch 603/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 16.3041\n",
      "Epoch 604/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 16.1625\n",
      "Epoch 605/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 16.2253\n",
      "Epoch 606/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 16.0847\n",
      "Epoch 607/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 16.1465\n",
      "Epoch 608/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.0069\n",
      "Epoch 609/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 16.0678\n",
      "Epoch 610/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 15.9293\n",
      "Epoch 611/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.9892\n",
      "Epoch 612/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 15.8518\n",
      "Epoch 613/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 15.9109\n",
      "Epoch 614/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.7746\n",
      "Epoch 615/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.8329\n",
      "Epoch 616/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.6978\n",
      "Epoch 617/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 15.7552\n",
      "Epoch 618/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.6214\n",
      "Epoch 619/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.6781\n",
      "Epoch 620/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.5455\n",
      "Epoch 621/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 15.6015\n",
      "Epoch 622/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 15.4704\n",
      "Epoch 623/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.5257\n",
      "Epoch 624/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.3959\n",
      "Epoch 625/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.4506\n",
      "Epoch 626/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 15.3221\n",
      "Epoch 627/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 15.3763\n",
      "Epoch 628/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.2492\n",
      "Epoch 629/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 15.3029\n",
      "Epoch 630/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.1772\n",
      "Epoch 631/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 15.2303\n",
      "Epoch 632/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 15.1059\n",
      "Epoch 633/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.1586\n",
      "Epoch 634/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.0355\n",
      "Epoch 635/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 15.0876\n",
      "Epoch 636/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.9658\n",
      "Epoch 637/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 15.0175\n",
      "Epoch 638/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.8969\n",
      "Epoch 639/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.9481\n",
      "Epoch 640/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.8286\n",
      "Epoch 641/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.8793\n",
      "Epoch 642/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.7610\n",
      "Epoch 643/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.8111\n",
      "Epoch 644/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.6938\n",
      "Epoch 645/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.7434\n",
      "Epoch 646/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.6271\n",
      "Epoch 647/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.6762\n",
      "Epoch 648/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 14.5608\n",
      "Epoch 649/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.6093\n",
      "Epoch 650/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 14.4948\n",
      "Epoch 651/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 14.5427\n",
      "Epoch 652/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.4291\n",
      "Epoch 653/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.4764\n",
      "Epoch 654/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.3636\n",
      "Epoch 655/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.4103\n",
      "Epoch 656/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.2983\n",
      "Epoch 657/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.3444\n",
      "Epoch 658/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.2331\n",
      "Epoch 659/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.2786\n",
      "Epoch 660/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 14.1680\n",
      "Epoch 661/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 14.2130\n",
      "Epoch 662/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.1030\n",
      "Epoch 663/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 14.1475\n",
      "Epoch 664/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.0381\n",
      "Epoch 665/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 14.0820\n",
      "Epoch 666/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 13.9732\n",
      "Epoch 667/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 14.0165\n",
      "Epoch 668/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 13.9083\n",
      "Epoch 669/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 13.9511\n",
      "Epoch 670/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.8432\n",
      "Epoch 671/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.8855\n",
      "Epoch 672/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.8197\n",
      "Epoch 674/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.7124\n",
      "Epoch 675/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.7534\n",
      "Epoch 676/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 13.6463\n",
      "Epoch 677/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 13.6864\n",
      "Epoch 678/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.5793\n",
      "Epoch 679/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.6185\n",
      "Epoch 680/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 13.5112\n",
      "Epoch 681/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.5492\n",
      "Epoch 682/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.4417\n",
      "Epoch 683/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.4784\n",
      "Epoch 684/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.3708\n",
      "Epoch 685/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.4062\n",
      "Epoch 686/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 13.2992\n",
      "Epoch 687/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.3606\n",
      "Epoch 688/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.2495\n",
      "Epoch 689/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 13.2814\n",
      "Epoch 690/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.1766\n",
      "Epoch 691/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.2133\n",
      "Epoch 692/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.1152\n",
      "Epoch 693/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.1573\n",
      "Epoch 694/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 13.0646\n",
      "Epoch 695/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 13.1110\n",
      "Epoch 696/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.0220\n",
      "Epoch 697/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 13.0706\n",
      "Epoch 698/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.9839\n",
      "Epoch 699/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 13.0333\n",
      "Epoch 700/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 12.9477\n",
      "Epoch 701/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.9966\n",
      "Epoch 702/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 12.9113\n",
      "Epoch 703/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 12.9588\n",
      "Epoch 704/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 12.8730\n",
      "Epoch 705/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.9183\n",
      "Epoch 706/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 12.8317\n",
      "Epoch 707/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 12.8740\n",
      "Epoch 708/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 12.7862\n",
      "Epoch 709/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 12.8253\n",
      "Epoch 710/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.7361\n",
      "Epoch 711/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 12.7715\n",
      "Epoch 712/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.6811\n",
      "Epoch 713/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 12.7127\n",
      "Epoch 714/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.6212\n",
      "Epoch 715/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 12.6490\n",
      "Epoch 716/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.5566\n",
      "Epoch 717/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.5809\n",
      "Epoch 718/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 12.4881\n",
      "Epoch 719/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.5090\n",
      "Epoch 720/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 12.4162\n",
      "Epoch 721/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.4342\n",
      "Epoch 722/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.3420\n",
      "Epoch 723/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.3573\n",
      "Epoch 724/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 12.2662\n",
      "Epoch 725/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 12.2794\n",
      "Epoch 726/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 12.1898\n",
      "Epoch 727/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.2015\n",
      "Epoch 728/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 12.1138\n",
      "Epoch 729/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 12.1243\n",
      "Epoch 730/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 12.0389\n",
      "Epoch 731/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 12.0486\n",
      "Epoch 732/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 11.9659\n",
      "Epoch 733/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.9752\n",
      "Epoch 734/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.8953\n",
      "Epoch 735/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.9046\n",
      "Epoch 736/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 11.8276\n",
      "Epoch 737/1000\n",
      "572/572 [==============================] - 0s 14us/step - loss: 11.8370\n",
      "Epoch 738/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 11.7631\n",
      "Epoch 739/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 11.7729\n",
      "Epoch 740/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 11.7019\n",
      "Epoch 741/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.7122\n",
      "Epoch 742/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.6441\n",
      "Epoch 743/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 11.6550\n",
      "Epoch 744/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.5895\n",
      "Epoch 745/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.6010\n",
      "Epoch 746/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 11.5381\n",
      "Epoch 747/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.5502\n",
      "Epoch 748/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.4895\n",
      "Epoch 749/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.5021\n",
      "Epoch 750/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 11.4435\n",
      "Epoch 751/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 11.4563\n",
      "Epoch 752/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.3995\n",
      "Epoch 753/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.4125\n",
      "Epoch 754/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.3571\n",
      "Epoch 755/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.3703\n",
      "Epoch 756/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.3160\n",
      "Epoch 757/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.3290\n",
      "Epoch 758/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.2756\n",
      "Epoch 759/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.2883\n",
      "Epoch 760/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 11.2355\n",
      "Epoch 761/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.2477\n",
      "Epoch 762/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.1953\n",
      "Epoch 763/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 11.2068\n",
      "Epoch 764/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.1546\n",
      "Epoch 765/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.1653\n",
      "Epoch 766/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 11.1130\n",
      "Epoch 767/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 11.1227\n",
      "Epoch 768/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 11.0703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 769/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.0788\n",
      "Epoch 770/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 11.0262\n",
      "Epoch 771/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 11.0335\n",
      "Epoch 772/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.9807\n",
      "Epoch 773/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.9867\n",
      "Epoch 774/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.9337\n",
      "Epoch 775/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.9384\n",
      "Epoch 776/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.8855\n",
      "Epoch 777/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.8893\n",
      "Epoch 778/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.8374\n",
      "Epoch 779/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.8410\n",
      "Epoch 780/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.7912\n",
      "Epoch 781/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.7958\n",
      "Epoch 782/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.7484\n",
      "Epoch 783/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.7546\n",
      "Epoch 784/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.7089\n",
      "Epoch 785/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.7161\n",
      "Epoch 786/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.6709\n",
      "Epoch 787/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.6782\n",
      "Epoch 788/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.6329\n",
      "Epoch 789/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.6395\n",
      "Epoch 790/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.5936\n",
      "Epoch 791/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.5991\n",
      "Epoch 792/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.5526\n",
      "Epoch 793/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.5567\n",
      "Epoch 794/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.5095\n",
      "Epoch 795/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.5123\n",
      "Epoch 796/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.4644\n",
      "Epoch 797/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.4659\n",
      "Epoch 798/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.4175\n",
      "Epoch 799/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.4176\n",
      "Epoch 800/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.3688\n",
      "Epoch 801/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.3678\n",
      "Epoch 802/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.3186\n",
      "Epoch 803/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.3166\n",
      "Epoch 804/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.2674\n",
      "Epoch 805/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.2645\n",
      "Epoch 806/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.2154\n",
      "Epoch 807/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.2117\n",
      "Epoch 808/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.1630\n",
      "Epoch 809/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.1588\n",
      "Epoch 810/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.1105\n",
      "Epoch 811/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.1061\n",
      "Epoch 812/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.0585\n",
      "Epoch 813/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 10.0538\n",
      "Epoch 814/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 10.0070\n",
      "Epoch 815/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 10.0024\n",
      "Epoch 816/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.9566\n",
      "Epoch 817/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.9521\n",
      "Epoch 818/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.9074\n",
      "Epoch 819/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.9032\n",
      "Epoch 820/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.8596\n",
      "Epoch 821/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.8558\n",
      "Epoch 822/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.8135\n",
      "Epoch 823/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.8101\n",
      "Epoch 824/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.7691\n",
      "Epoch 825/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.7662\n",
      "Epoch 826/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.7264\n",
      "Epoch 827/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.7240\n",
      "Epoch 828/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.6855\n",
      "Epoch 829/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.6837\n",
      "Epoch 830/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.6464\n",
      "Epoch 831/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.6451\n",
      "Epoch 832/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.6089\n",
      "Epoch 833/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.6081\n",
      "Epoch 834/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.5730\n",
      "Epoch 835/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.5726\n",
      "Epoch 836/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.5385\n",
      "Epoch 837/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.5384\n",
      "Epoch 838/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.5052\n",
      "Epoch 839/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.5054\n",
      "Epoch 840/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 9.4729\n",
      "Epoch 841/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 9.4732\n",
      "Epoch 842/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.4414\n",
      "Epoch 843/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.4418\n",
      "Epoch 844/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.4105\n",
      "Epoch 845/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 9.4108\n",
      "Epoch 846/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.3799\n",
      "Epoch 847/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.3801\n",
      "Epoch 848/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.3495\n",
      "Epoch 849/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.3494\n",
      "Epoch 850/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.3190\n",
      "Epoch 851/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.3185\n",
      "Epoch 852/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.2882\n",
      "Epoch 853/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 9.2873\n",
      "Epoch 854/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.2571\n",
      "Epoch 855/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.2556\n",
      "Epoch 856/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.2253\n",
      "Epoch 857/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.2233\n",
      "Epoch 858/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.1929\n",
      "Epoch 859/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.1903\n",
      "Epoch 860/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 9.1598\n",
      "Epoch 861/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.1565\n",
      "Epoch 862/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.1259\n",
      "Epoch 863/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.1219\n",
      "Epoch 864/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.0911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 865/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 9.0865\n",
      "Epoch 866/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.0556\n",
      "Epoch 867/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 9.0503\n",
      "Epoch 868/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.0194\n",
      "Epoch 869/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 9.0135\n",
      "Epoch 870/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.9825\n",
      "Epoch 871/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.9761\n",
      "Epoch 872/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.9451\n",
      "Epoch 873/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.9382\n",
      "Epoch 874/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.9073\n",
      "Epoch 875/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.8999\n",
      "Epoch 876/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.8692\n",
      "Epoch 877/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.8614\n",
      "Epoch 878/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.8309\n",
      "Epoch 879/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.8228\n",
      "Epoch 880/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 8.7926\n",
      "Epoch 881/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.7843\n",
      "Epoch 882/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.7544\n",
      "Epoch 883/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.7460\n",
      "Epoch 884/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.7165\n",
      "Epoch 885/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 8.7218\n",
      "Epoch 886/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.6906\n",
      "Epoch 887/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.6801\n",
      "Epoch 888/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.6498\n",
      "Epoch 889/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 8.6396\n",
      "Epoch 890/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.6100\n",
      "Epoch 891/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.6001\n",
      "Epoch 892/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 8.5713\n",
      "Epoch 893/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.5617\n",
      "Epoch 894/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.5335\n",
      "Epoch 895/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.5243\n",
      "Epoch 896/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.4970\n",
      "Epoch 897/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.4881\n",
      "Epoch 898/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.4616\n",
      "Epoch 899/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.4531\n",
      "Epoch 900/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.4274\n",
      "Epoch 901/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.4193\n",
      "Epoch 902/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.3944\n",
      "Epoch 903/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.3867\n",
      "Epoch 904/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.3625\n",
      "Epoch 905/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.3553\n",
      "Epoch 906/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.3319\n",
      "Epoch 907/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.3250\n",
      "Epoch 908/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.3023\n",
      "Epoch 909/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.2958\n",
      "Epoch 910/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.2737\n",
      "Epoch 911/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.2675\n",
      "Epoch 912/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.2460\n",
      "Epoch 913/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.2401\n",
      "Epoch 914/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.2191\n",
      "Epoch 915/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.2134\n",
      "Epoch 916/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.1929\n",
      "Epoch 917/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.1874\n",
      "Epoch 918/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.1672\n",
      "Epoch 919/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.1619\n",
      "Epoch 920/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.1420\n",
      "Epoch 921/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 8.1367\n",
      "Epoch 922/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.1171\n",
      "Epoch 923/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.1118\n",
      "Epoch 924/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.0923\n",
      "Epoch 925/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.0870\n",
      "Epoch 926/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.0677\n",
      "Epoch 927/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 8.0622\n",
      "Epoch 928/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.0429\n",
      "Epoch 929/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 8.0373\n",
      "Epoch 930/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 8.0180\n",
      "Epoch 931/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 8.0122\n",
      "Epoch 932/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.9929\n",
      "Epoch 933/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.9868\n",
      "Epoch 934/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.9675\n",
      "Epoch 935/1000\n",
      "572/572 [==============================] - 0s 14us/step - loss: 7.9611\n",
      "Epoch 936/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 7.9417\n",
      "Epoch 937/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.9350\n",
      "Epoch 938/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.9155\n",
      "Epoch 939/1000\n",
      "572/572 [==============================] - 0s 24us/step - loss: 7.9085\n",
      "Epoch 940/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 7.8888\n",
      "Epoch 941/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.8815\n",
      "Epoch 942/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.8618\n",
      "Epoch 943/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.8542\n",
      "Epoch 944/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.8343\n",
      "Epoch 945/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.8264\n",
      "Epoch 946/1000\n",
      "572/572 [==============================] - 0s 19us/step - loss: 7.8064\n",
      "Epoch 947/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.7982\n",
      "Epoch 948/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.7782\n",
      "Epoch 949/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.7698\n",
      "Epoch 950/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.7497\n",
      "Epoch 951/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.7410\n",
      "Epoch 952/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.7209\n",
      "Epoch 953/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 7.7121\n",
      "Epoch 954/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 7.6860\n",
      "Epoch 955/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.6753\n",
      "Epoch 956/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.6539\n",
      "Epoch 957/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.6436\n",
      "Epoch 958/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.6228\n",
      "Epoch 959/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.6128\n",
      "Epoch 960/1000\n",
      "572/572 [==============================] - 0s 14us/step - loss: 7.5924\n",
      "Epoch 961/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.5937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 962/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.5723\n",
      "Epoch 963/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.5615\n",
      "Epoch 964/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.5407\n",
      "Epoch 965/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.5304\n",
      "Epoch 966/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.5102\n",
      "Epoch 967/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.5003\n",
      "Epoch 968/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.4808\n",
      "Epoch 969/1000\n",
      "572/572 [==============================] - 0s 12us/step - loss: 7.4714\n",
      "Epoch 970/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 7.4524\n",
      "Epoch 971/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.4434\n",
      "Epoch 972/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.4250\n",
      "Epoch 973/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.4165\n",
      "Epoch 974/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.3987\n",
      "Epoch 975/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.3906\n",
      "Epoch 976/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.3734\n",
      "Epoch 977/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 7.3657\n",
      "Epoch 978/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.3490\n",
      "Epoch 979/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.3417\n",
      "Epoch 980/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.3256\n",
      "Epoch 981/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.3188\n",
      "Epoch 982/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.3032\n",
      "Epoch 983/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.2967\n",
      "Epoch 984/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.2816\n",
      "Epoch 985/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.2754\n",
      "Epoch 986/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.2608\n",
      "Epoch 987/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.2549\n",
      "Epoch 988/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.2352\n",
      "Epoch 989/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.2282\n",
      "Epoch 990/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.2132\n",
      "Epoch 991/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.2069\n",
      "Epoch 992/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.1927\n",
      "Epoch 993/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.1869\n",
      "Epoch 994/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.1734\n",
      "Epoch 995/1000\n",
      "572/572 [==============================] - 0s 9us/step - loss: 7.1682\n",
      "Epoch 996/1000\n",
      "572/572 [==============================] - 0s 5us/step - loss: 7.1554\n",
      "Epoch 997/1000\n",
      "572/572 [==============================] - 0s 10us/step - loss: 7.1505\n",
      "Epoch 998/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.1384\n",
      "Epoch 999/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.1340\n",
      "Epoch 1000/1000\n",
      "572/572 [==============================] - 0s 7us/step - loss: 7.1226\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "NB_EPOCH = 1000\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(X.shape[1])\n",
    "\n",
    "np.random.seed(0)\n",
    "bit_model = Sequential()\n",
    "bit_model.add(Dense(64, input_shape=(X.shape[1],)))\n",
    "bit_model.add(Activation('tanh'))\n",
    "bit_model.add(Dense(1))\n",
    "bit_model.add(Activation('linear'))\n",
    "bit_model.compile(loss='mean_squared_error', optimizer=SGD(lr = 0.01))\n",
    "bit_history = bit_model.fit(X_train, y_train, batch_size=1000, epochs=NB_EPOCH, verbose=1, validation_split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.75924146425474"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bit_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] <a href=\"https://toolbox.google.com/datasetsearch/search?query=MNIST%20Database%20(mnist.pkl.gz)&docid=QZBrQgKepkevjOimAAAAAA%3D%3D\">MNIST data set</a>\n",
    "\n",
    "[2] *Antonio Gulli*, Deep Learning with Keras\n",
    "\n",
    "[3] <a href=\"https://keras.io\">Keras Documentation</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
