{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amphi 1 - Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In lesson 6, 7, 8, 9 of DSC101, some linear models were introduce:\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Linear SVM (Support Vector Machine with Linear Kernel)\n",
    "- Perceptron\n",
    "- Linear Discriminant Analysis\n",
    "\n",
    "In reality, these models are usually not adapted because non-linearly separability of problems.\n",
    "\n",
    "Other methods are based on discriminant methods:\n",
    "\n",
    "- Gaussian Naives Bayes\n",
    "- Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "They are quadratic methods and also not adaptable to more complex reality problems.\n",
    "\n",
    "Some generalization strategies were introduced. They share the same idea: explore a larger function class:\n",
    "\n",
    "- Consider more complex function class: Polynomial Regression. (Higher degree means more parameters, more complex model). Similarly to Polynomial Regression, we can use Piecewise Linear Functions. Complexity can be adapted by the number of linear pieces, but it depends on the concrete problem.\n",
    "\n",
    "- Use kernel method: Transform the optimization problem to dual form. Use more complex kernel: polynomial, sigmoid, tanh, rbf...  Each kernel is in fact equivalent to a function class (e.g., linear kernel equivalent to linear model i.e. linear boundary)\n",
    "\n",
    "In this lesson, we would like to use another approach to consider a more general class of functions using composite functions. The idea:\n",
    "\n",
    "- In linear methods, the output is predicted by\n",
    "$$\n",
    "f(\\mathbf x) = w(\\mathbf x)\n",
    "$$\n",
    "\n",
    "for regression, or\n",
    "$$\n",
    "f(\\mathbf x) = 1_{w(\\mathbf x) \\geq 0}\n",
    "$$\n",
    "\n",
    "for binary classification, where $w$ is a linear function.\n",
    "\n",
    "Now if we allow the predicted output to be of the form:\n",
    "$$\n",
    "f(\\mathbf x) = w_{l}(w_{l-1}(\\ldots w_1(\\mathbf x)))\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "f(\\mathbf x) = \\mathbf 1 (w_{l}(w_{l-1}(\\ldots w_1(\\mathbf x))) \\geq 0)\n",
    "$$\n",
    "\n",
    "\n",
    "where $w_l, w_{l-1}, \\ldots, w_1$ are functions, some of them are linear while the others are not, we can target more complex function forms.\n",
    "\n",
    "For example, we can think of $w_1, w_3, \\ldots, w_{9}$ as linear functions and $w_2, w_4, \\ldots, w_8$ as the function $1_{\\mathbf x \\geq 0}$ applied to each coordinate of $\\mathbf x$, this will make the output predicted by piecewise linear functions.\n",
    "\n",
    "A model where prediction can be done by a composition of alternatively linear functions and element-wise non-linear functions are called *Feedforward Neural Network* or *Multilayer Perceptron*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Graphical Representation and Terminology\n",
    "\n",
    "## 2.1 Graphical Representation\n",
    "\n",
    "Suppose that we use a Feedforward Network that suggests a function class ($\\mathbf R^D \\to \\mathbf R^K$) of the form:\n",
    "\n",
    "$$\n",
    "\\mathbf x \\mapsto g(\\mathbf V(h(\\mathbf W \\mathbf x + \\mathbf c)) + \\mathbf d)\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbf R^D$; $\\mathbf W$ is a $M \\times D$-matrix; $\\mathbf c$ is an $M-$dimensional vector, $h$ is a non-linear function $\\mathbf R^M \\to \\mathbf R^M$ acting element-wisely; $V$ is a $K \\times M$ matrix; $\\mathbf d$ is a $K-$dimensional vector and $g$ is a non-linear function   $\\mathbf R^K \\to \\mathbf R^K$ acting element-wisely.\n",
    "\n",
    "We can sketch a graph of this FNN:\n",
    "\n",
    "<img src=\"F3.png\" width=800></img>\n",
    "\n",
    "This is a composition function of linear and non-linear function consecutively.\n",
    "\n",
    "In general, a FFN is like this:\n",
    "\n",
    "<img src=\"F4.png\" width=800></img>\n",
    "\n",
    "where $h^{(l)}$ are known and chosen by us.\n",
    "\n",
    "This represents:\n",
    "\n",
    "$$\n",
    "\\mathbf z^{(0)} \\mapsto \\mathbf z^{(L)} =  h^{(L)} \\left( \\mathbf W^{(L)} \\left( h^{(L-1)}\\left( \\ldots \\left( \\mathbf h^{(1)} (\\mathbf W^{(1)} \\mathbf z^{(0)} + \\mathbf W_0^{(1)})\\right) \\ldots \\right)\\right) + \\mathbf W_0^{(L)} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Terminology\n",
    "\n",
    "- $L$ is called the depth of the network. It equals the number of linear functions in the composition.\n",
    "- The network can be **fully connected** or not.\n",
    "- $\\max(D_0, D_1, \\ldots, D_L)$ is sometimes called **width** of the network.\n",
    "- The **layer** $l$ (the $l$-th layer) can refer to\n",
    "\n",
    " - $(\\mathbf z^{(l)})$\n",
    "or\n",
    "\n",
    " - $(\\mathbf W^{(l)})$\n",
    "or\n",
    "\n",
    " - $(\\mathbf z^{(l-1)}, \\mathbf W^{(l)}, \\mathbf a^{(l)}, h^{(l)}, z^{(l)})$\n",
    "- Each coordinate $z_j^{(i)}$ is called a **unit** of the layer $i$.\n",
    "- The layer 0 is called the **input layer**.\n",
    "- The layer $L$ is called the **output layer**.\n",
    "- The layers $1, 2, \\ldots, L-1$ are called **hidden layers**.\n",
    "- The functions $h^{(l)}$ are called **activation functions**.\n",
    "- $\\mathbf w_{j,0}^{(l)}, j = 1, \\ldots, D_l$ are called **bias**.\n",
    "- $\\mathbf w_{j,i}^{(l)}, j = 1, \\ldots, D_l, i =1 , \\ldots, D_{l-1}$ are called **weights**.\n",
    "\n",
    "By **deep learning** we mean methods considering classes of functions that are composition of several transformations on the input. Feed forward networks are example of deep learning models (if depth >=2). \n",
    "\n",
    "Like many other models, we need to define some quantity depending on the unknown parameters and find functions (i.e., the value of these parameters) in this class that optimize the quantity. The quantity can be a sum of losses between true values and predicted values (Empirical Risk Minimization) or the likelihood of observing the training data (maximum likelihood)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Basic Models as 1-layer Feedforward Network\n",
    "\n",
    "**Linear Regression**\n",
    "\n",
    "Linear Regression is an example of Feed Forward Network where:\n",
    "- $L=1$: the network has depth 1.\n",
    "- The output layer is of size (width) 1.\n",
    "- The activation function $h^{(1)}$ is the identity function\n",
    "- The loss function is (mean) squared loss.\n",
    "\n",
    "<img src=\"F6.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Binary) Logistic Regression**\n",
    "Logistic Regression is another example where:\n",
    "\n",
    "- $L=1$\n",
    "- The output layer has size 1\n",
    "- The activation function $h^{(1)}$ is $\\sigma(a) = \\frac1{1+\\exp(-a)}$\n",
    "- The loss function is the **binary cross-entropy** function\n",
    "$$\n",
    "L(t, y) = -t \\log y - (1-t) \\log (1-y)\n",
    "$$\n",
    "\n",
    "<img src=\"F7.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiclass Logistic Regression with Multinomial Distribution Strategy (also called Softmax Regression)**\n",
    "\n",
    "- $L = 1$\n",
    "- The output layer has size $K$.\n",
    "- The activation function $h^{(1)}$ is the softmax function\n",
    "$$\n",
    "\\sigma(a_k) = \\frac{\\exp(a_k)}{\\sum_{j=1}^K \\exp(a_j)}\n",
    "$$\n",
    "- The loss function is the **cross-entropy** function:\n",
    "$$\n",
    "L(t, y_1, \\ldots, y_K) = -\\sum_{j=1}^K t_j \\log y_j\n",
    "$$\n",
    "\n",
    "where $t_j = 1$ if the true value $t$ belongs to class $j$ and 0 otherwise.\n",
    "\n",
    "<img src=\"F12.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Examples about XOR problem\n",
    "\n",
    "Suppose $\\mathbf x = (x_1, x_2) \\in \\mathbf \\{0, 1\\}^2$ and $t \\in \\{0, 1\\}$ is a function of $\\mathbf x$.\n",
    "\n",
    "**If $t = x_1 \\textrm{ and } x_2$**, a 1-layer network for binary classification (e.g., logistic regression) can efficiently solve the problem.\n",
    "\n",
    "<img src=\"F8.png\" width=400></img>\n",
    "\n",
    "Similarly, **if  $t = x_1 \\textrm{ or } x_2$**\n",
    "\n",
    "<img src=\"F9.png\" width=400></img>\n",
    "\n",
    "**if $t = (\\textrm{not } x_1) \\textrm { or } (\\textrm{not } x_2)$**\n",
    "\n",
    "<img src=\"F10.png\" width=400></img>\n",
    "\n",
    "However, if $t = x_1 \\textrm{ xor } x_2$, a 1-layer FFN cannot solve the problem. Note that we have:\n",
    "$$\n",
    "x_1 \\textrm{ xor } x_2 = (x_1 \\textrm{ or } x_2) \\textrm{ and } ((\\textrm{not } x_1) \\textrm { or } (\\textrm{not } x_2))\n",
    "$$\n",
    "\n",
    "So, the following 2-FNN can help solve the problem:\n",
    "<img src=\"F11.png\" width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Keras\n",
    "\n",
    "## 3.1 Installation\n",
    "\n",
    "**TensorFlow** is a Python library/framework for numerical computation that are used in machine learning. It implements tensors, which help to define deep learning models' architecture.\n",
    "\n",
    "**Keras** is a Python library that can run on top of **Tensorflow** (and other frameworks like Theano). It simplifies the way we code in defining deep learning models.\n",
    "\n",
    "To install **Tensorflow** and **Keras**, Python 3 is recommended, but with version 3.5\n",
    "\n",
    "`conda create -n py35 python=3.5`\n",
    "\n",
    "`conda activate py35`\n",
    "\n",
    "`conda install anaconda`\n",
    "\n",
    "`conda install opencv`\n",
    "\n",
    "`conda install theano`\n",
    "\n",
    "`conda install tensorflow`\n",
    "\n",
    "`conda install keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Example: AND problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "NB_SAMPLES = 1000\n",
    "X = np.random.binomial(1, 0.6, size=(1000, 2))\n",
    "y = X[:, 0] * X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   990  991  992  993  \\\n",
       "0    1    1    0    1    0    1    0    1    1    0 ...     0    1    0    1   \n",
       "1    0    0    1    0    0    1    0    1    1    0 ...     1    0    1    1   \n",
       "2    0    0    0    0    0    1    0    1    1    0 ...     0    0    0    1   \n",
       "\n",
       "   994  995  996  997  998  999  \n",
       "0    0    1    0    1    0    0  \n",
       "1    0    1    0    1    0    0  \n",
       "2    0    1    0    1    0    0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([X[:,0], X[:,1], y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape = (2,)))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.initializers import RandomUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/100\n",
      "560/560 [==============================] - 1s 1ms/step - loss: 0.9295 - acc: 0.4946 - val_loss: 0.9431 - val_acc: 0.4071\n",
      "Epoch 2/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.8803 - acc: 0.4518 - val_loss: 0.8928 - val_acc: 0.4071\n",
      "Epoch 3/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.8416 - acc: 0.4518 - val_loss: 0.8522 - val_acc: 0.4071\n",
      "Epoch 4/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 0.8105 - acc: 0.4518 - val_loss: 0.8193 - val_acc: 0.4071\n",
      "Epoch 5/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.7847 - acc: 0.4518 - val_loss: 0.7918 - val_acc: 0.4071\n",
      "Epoch 6/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.7628 - acc: 0.4518 - val_loss: 0.7674 - val_acc: 0.5929\n",
      "Epoch 7/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.7430 - acc: 0.6357 - val_loss: 0.7451 - val_acc: 0.5929\n",
      "Epoch 8/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.7252 - acc: 0.6357 - val_loss: 0.7264 - val_acc: 0.5929\n",
      "Epoch 9/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.7092 - acc: 0.6357 - val_loss: 0.7082 - val_acc: 0.5929\n",
      "Epoch 10/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6939 - acc: 0.6357 - val_loss: 0.6928 - val_acc: 0.5929\n",
      "Epoch 11/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6798 - acc: 0.6357 - val_loss: 0.6777 - val_acc: 0.5929\n",
      "Epoch 12/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6663 - acc: 0.6357 - val_loss: 0.6633 - val_acc: 0.5929\n",
      "Epoch 13/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6535 - acc: 0.6357 - val_loss: 0.6501 - val_acc: 0.5929\n",
      "Epoch 14/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6410 - acc: 0.6357 - val_loss: 0.6369 - val_acc: 0.5929\n",
      "Epoch 15/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6293 - acc: 0.6357 - val_loss: 0.6244 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.6179 - acc: 0.9089 - val_loss: 0.6127 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6071 - acc: 1.0000 - val_loss: 0.6029 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.5966 - acc: 1.0000 - val_loss: 0.5919 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 0.5865 - acc: 1.0000 - val_loss: 0.5819 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.5766 - acc: 1.0000 - val_loss: 0.5725 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.5672 - acc: 1.0000 - val_loss: 0.5625 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.5581 - acc: 1.0000 - val_loss: 0.5535 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "560/560 [==============================] - 0s 18us/step - loss: 0.5492 - acc: 1.0000 - val_loss: 0.5442 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.5404 - acc: 1.0000 - val_loss: 0.5357 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.5321 - acc: 1.0000 - val_loss: 0.5276 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.5241 - acc: 1.0000 - val_loss: 0.5191 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.5165 - acc: 1.0000 - val_loss: 0.5112 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.5089 - acc: 1.0000 - val_loss: 0.5035 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.5018 - acc: 1.0000 - val_loss: 0.4957 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.4946 - acc: 1.0000 - val_loss: 0.4887 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.4877 - acc: 1.0000 - val_loss: 0.4819 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.4811 - acc: 1.0000 - val_loss: 0.4754 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.4746 - acc: 1.0000 - val_loss: 0.4694 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.4683 - acc: 1.0000 - val_loss: 0.4629 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.4625 - acc: 1.0000 - val_loss: 0.4566 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.4565 - acc: 1.0000 - val_loss: 0.4509 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.4506 - acc: 1.0000 - val_loss: 0.4451 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.4450 - acc: 1.0000 - val_loss: 0.4399 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.4396 - acc: 1.0000 - val_loss: 0.4347 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.4343 - acc: 1.0000 - val_loss: 0.4295 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 1.0000 - val_loss: 0.4243 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.4241 - acc: 1.0000 - val_loss: 0.4193 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.4191 - acc: 1.0000 - val_loss: 0.4145 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.4144 - acc: 1.0000 - val_loss: 0.4099 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.4097 - acc: 1.0000 - val_loss: 0.4056 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.4052 - acc: 1.0000 - val_loss: 0.4008 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.4008 - acc: 1.0000 - val_loss: 0.3966 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3966 - acc: 1.0000 - val_loss: 0.3919 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3922 - acc: 1.0000 - val_loss: 0.3879 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.3881 - acc: 1.0000 - val_loss: 0.3839 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3841 - acc: 1.0000 - val_loss: 0.3802 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3802 - acc: 1.0000 - val_loss: 0.3763 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.3763 - acc: 1.0000 - val_loss: 0.3723 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.3726 - acc: 1.0000 - val_loss: 0.3684 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.3689 - acc: 1.0000 - val_loss: 0.3651 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3653 - acc: 1.0000 - val_loss: 0.3616 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.3617 - acc: 1.0000 - val_loss: 0.3579 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.3583 - acc: 1.0000 - val_loss: 0.3543 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3549 - acc: 1.0000 - val_loss: 0.3509 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3517 - acc: 1.0000 - val_loss: 0.3471 - val_acc: 1.0000\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 20us/step - loss: 0.3483 - acc: 1.0000 - val_loss: 0.3440 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.3452 - acc: 1.0000 - val_loss: 0.3408 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.3421 - acc: 1.0000 - val_loss: 0.3378 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3390 - acc: 1.0000 - val_loss: 0.3348 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.3361 - acc: 1.0000 - val_loss: 0.3317 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "560/560 [==============================] - 0s 18us/step - loss: 0.3331 - acc: 1.0000 - val_loss: 0.3288 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3302 - acc: 1.0000 - val_loss: 0.3260 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3274 - acc: 1.0000 - val_loss: 0.3232 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3246 - acc: 1.0000 - val_loss: 0.3207 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3218 - acc: 1.0000 - val_loss: 0.3179 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3192 - acc: 1.0000 - val_loss: 0.3152 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.3165 - acc: 1.0000 - val_loss: 0.3124 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3140 - acc: 1.0000 - val_loss: 0.3100 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3114 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3091 - acc: 1.0000 - val_loss: 0.3050 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3065 - acc: 1.0000 - val_loss: 0.3027 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3040 - acc: 1.0000 - val_loss: 0.3004 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3016 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2993 - acc: 1.0000 - val_loss: 0.2959 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2970 - acc: 1.0000 - val_loss: 0.2937 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2948 - acc: 1.0000 - val_loss: 0.2916 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "560/560 [==============================] - 0s 18us/step - loss: 0.2926 - acc: 1.0000 - val_loss: 0.2891 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2904 - acc: 1.0000 - val_loss: 0.2869 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.2883 - acc: 1.0000 - val_loss: 0.2847 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.2861 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.2840 - acc: 1.0000 - val_loss: 0.2806 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.2820 - acc: 1.0000 - val_loss: 0.2786 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.2799 - acc: 1.0000 - val_loss: 0.2766 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2780 - acc: 1.0000 - val_loss: 0.2747 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.2760 - acc: 1.0000 - val_loss: 0.2727 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2741 - acc: 1.0000 - val_loss: 0.2709 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.2722 - acc: 1.0000 - val_loss: 0.2688 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.2703 - acc: 1.0000 - val_loss: 0.2670 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.2684 - acc: 1.0000 - val_loss: 0.2652 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.2667 - acc: 1.0000 - val_loss: 0.2635 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2648 - acc: 1.0000 - val_loss: 0.2619 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "560/560 [==============================] - 0s 18us/step - loss: 0.2630 - acc: 1.0000 - val_loss: 0.2602 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.2613 - acc: 1.0000 - val_loss: 0.2586 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.2595 - acc: 1.0000 - val_loss: 0.2569 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.2578 - acc: 1.0000 - val_loss: 0.2552 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZER = SGD(lr = 0.1)\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "VERBOSE = 1\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2.1415565],\n",
       "        [2.0179265]], dtype=float32), array([-3.0638707], dtype=float32)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(index = 0).get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Example: XOR problem with 1-layer Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   990  991  992  993  \\\n",
       "0    0    0    1    0    1    0    1    1    0    0 ...     0    1    0    0   \n",
       "1    0    1    1    0    0    1    0    1    1    1 ...     1    0    1    1   \n",
       "2    0    1    0    0    1    1    1    0    1    1 ...     1    1    1    1   \n",
       "\n",
       "   994  995  996  997  998  999  \n",
       "0    0    1    0    1    1    1  \n",
       "1    0    1    1    1    0    1  \n",
       "2    0    0    1    0    1    0  \n",
       "\n",
       "[3 rows x 1000 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.binomial(1, 0.5, size=(1000, 2))\n",
    "y = (X[:, 0] + X[:, 1]) % 2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "pd.DataFrame([X[:,0], X[:,1], y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/100\n",
      "560/560 [==============================] - 1s 2ms/step - loss: 0.8277 - acc: 0.7321 - val_loss: 0.7801 - val_acc: 0.7571\n",
      "Epoch 2/100\n",
      "560/560 [==============================] - 0s 48us/step - loss: 0.8001 - acc: 0.7321 - val_loss: 0.7598 - val_acc: 0.7571\n",
      "Epoch 3/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 0.7807 - acc: 0.5982 - val_loss: 0.7452 - val_acc: 0.5143\n",
      "Epoch 4/100\n",
      "560/560 [==============================] - 0s 45us/step - loss: 0.7665 - acc: 0.4607 - val_loss: 0.7336 - val_acc: 0.5143\n",
      "Epoch 5/100\n",
      "560/560 [==============================] - 0s 59us/step - loss: 0.7549 - acc: 0.4607 - val_loss: 0.7255 - val_acc: 0.5143\n",
      "Epoch 6/100\n",
      "560/560 [==============================] - 0s 46us/step - loss: 0.7468 - acc: 0.4607 - val_loss: 0.7194 - val_acc: 0.5143\n",
      "Epoch 7/100\n",
      "560/560 [==============================] - 0s 64us/step - loss: 0.7405 - acc: 0.4607 - val_loss: 0.7155 - val_acc: 0.5143\n",
      "Epoch 8/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 0.7360 - acc: 0.4607 - val_loss: 0.7119 - val_acc: 0.5143\n",
      "Epoch 9/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.7320 - acc: 0.4607 - val_loss: 0.7097 - val_acc: 0.5143\n",
      "Epoch 10/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.7294 - acc: 0.4607 - val_loss: 0.7074 - val_acc: 0.5143\n",
      "Epoch 11/100\n",
      "560/560 [==============================] - 0s 50us/step - loss: 0.7265 - acc: 0.4607 - val_loss: 0.7057 - val_acc: 0.5143\n",
      "Epoch 12/100\n",
      "560/560 [==============================] - 0s 45us/step - loss: 0.7243 - acc: 0.4607 - val_loss: 0.7044 - val_acc: 0.5143\n",
      "Epoch 13/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.7223 - acc: 0.4607 - val_loss: 0.7029 - val_acc: 0.5143\n",
      "Epoch 14/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.7202 - acc: 0.4607 - val_loss: 0.7021 - val_acc: 0.5143\n",
      "Epoch 15/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 0.7192 - acc: 0.4607 - val_loss: 0.7010 - val_acc: 0.5143\n",
      "Epoch 16/100\n",
      "560/560 [==============================] - 0s 63us/step - loss: 0.7174 - acc: 0.4607 - val_loss: 0.7000 - val_acc: 0.5143\n",
      "Epoch 17/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.7159 - acc: 0.4607 - val_loss: 0.6993 - val_acc: 0.5143\n",
      "Epoch 18/100\n",
      "560/560 [==============================] - 0s 61us/step - loss: 0.7145 - acc: 0.4607 - val_loss: 0.6987 - val_acc: 0.5143\n",
      "Epoch 19/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.7133 - acc: 0.4607 - val_loss: 0.6980 - val_acc: 0.5143\n",
      "Epoch 20/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.7122 - acc: 0.4607 - val_loss: 0.6975 - val_acc: 0.5143\n",
      "Epoch 21/100\n",
      "560/560 [==============================] - 0s 63us/step - loss: 0.7111 - acc: 0.4607 - val_loss: 0.6971 - val_acc: 0.5143\n",
      "Epoch 22/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.7099 - acc: 0.4607 - val_loss: 0.6966 - val_acc: 0.5143\n",
      "Epoch 23/100\n",
      "560/560 [==============================] - 0s 48us/step - loss: 0.7091 - acc: 0.4607 - val_loss: 0.6963 - val_acc: 0.5143\n",
      "Epoch 24/100\n",
      "560/560 [==============================] - 0s 45us/step - loss: 0.7083 - acc: 0.4607 - val_loss: 0.6958 - val_acc: 0.5143\n",
      "Epoch 25/100\n",
      "560/560 [==============================] - 0s 50us/step - loss: 0.7073 - acc: 0.4607 - val_loss: 0.6955 - val_acc: 0.5143\n",
      "Epoch 26/100\n",
      "560/560 [==============================] - 0s 73us/step - loss: 0.7064 - acc: 0.4607 - val_loss: 0.6953 - val_acc: 0.5143\n",
      "Epoch 27/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.7058 - acc: 0.4607 - val_loss: 0.6950 - val_acc: 0.5143\n",
      "Epoch 28/100\n",
      "560/560 [==============================] - 0s 57us/step - loss: 0.7054 - acc: 0.4607 - val_loss: 0.6947 - val_acc: 0.5143\n",
      "Epoch 29/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.7043 - acc: 0.4607 - val_loss: 0.6943 - val_acc: 0.5143\n",
      "Epoch 30/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.7035 - acc: 0.4607 - val_loss: 0.6942 - val_acc: 0.5143\n",
      "Epoch 31/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 0.7029 - acc: 0.4607 - val_loss: 0.6940 - val_acc: 0.5143\n",
      "Epoch 32/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.7022 - acc: 0.4607 - val_loss: 0.6937 - val_acc: 0.5143\n",
      "Epoch 33/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.7015 - acc: 0.4607 - val_loss: 0.6936 - val_acc: 0.5143\n",
      "Epoch 34/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.7010 - acc: 0.4607 - val_loss: 0.6935 - val_acc: 0.5143\n",
      "Epoch 35/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.7005 - acc: 0.4607 - val_loss: 0.6933 - val_acc: 0.5143\n",
      "Epoch 36/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.7001 - acc: 0.4607 - val_loss: 0.6933 - val_acc: 0.5143\n",
      "Epoch 37/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6999 - acc: 0.4607 - val_loss: 0.6932 - val_acc: 0.5143\n",
      "Epoch 38/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.6993 - acc: 0.4607 - val_loss: 0.6931 - val_acc: 0.5143\n",
      "Epoch 39/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6989 - acc: 0.4607 - val_loss: 0.6930 - val_acc: 0.5143\n",
      "Epoch 40/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6983 - acc: 0.4607 - val_loss: 0.6929 - val_acc: 0.5143\n",
      "Epoch 41/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.6979 - acc: 0.4607 - val_loss: 0.6929 - val_acc: 0.5143\n",
      "Epoch 42/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6975 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 43/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6970 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 44/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6967 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 45/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.6965 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 46/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6963 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 47/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6959 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 48/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6956 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 49/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6953 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 50/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 0.6951 - acc: 0.4607 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 51/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6951 - acc: 0.5089 - val_loss: 0.6928 - val_acc: 0.5143\n",
      "Epoch 52/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6946 - acc: 0.4607 - val_loss: 0.6929 - val_acc: 0.5143\n",
      "Epoch 53/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6944 - acc: 0.4607 - val_loss: 0.6929 - val_acc: 0.5143\n",
      "Epoch 54/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6942 - acc: 0.4607 - val_loss: 0.6929 - val_acc: 0.5143\n",
      "Epoch 55/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.6941 - acc: 0.5946 - val_loss: 0.6930 - val_acc: 0.5143\n",
      "Epoch 56/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6940 - acc: 0.5393 - val_loss: 0.6930 - val_acc: 0.5143\n",
      "Epoch 57/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6937 - acc: 0.6661 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 58/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6935 - acc: 0.5929 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 59/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6934 - acc: 0.5161 - val_loss: 0.6932 - val_acc: 0.7429\n",
      "Epoch 60/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 0.6930 - acc: 0.5250 - val_loss: 0.6933 - val_acc: 0.4857\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 30us/step - loss: 0.6932 - acc: 0.4357 - val_loss: 0.6933 - val_acc: 0.2429\n",
      "Epoch 62/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6929 - acc: 0.3643 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 63/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6925 - acc: 0.4071 - val_loss: 0.6934 - val_acc: 0.2429\n",
      "Epoch 64/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6926 - acc: 0.3821 - val_loss: 0.6935 - val_acc: 0.2429\n",
      "Epoch 65/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6925 - acc: 0.3982 - val_loss: 0.6935 - val_acc: 0.4857\n",
      "Epoch 66/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6923 - acc: 0.5393 - val_loss: 0.6936 - val_acc: 0.4857\n",
      "Epoch 67/100\n",
      "560/560 [==============================] - 0s 48us/step - loss: 0.6922 - acc: 0.5393 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 68/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6922 - acc: 0.5161 - val_loss: 0.6937 - val_acc: 0.4857\n",
      "Epoch 69/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6921 - acc: 0.5143 - val_loss: 0.6938 - val_acc: 0.4857\n",
      "Epoch 70/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6918 - acc: 0.5393 - val_loss: 0.6938 - val_acc: 0.4857\n",
      "Epoch 71/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6917 - acc: 0.5393 - val_loss: 0.6939 - val_acc: 0.4857\n",
      "Epoch 72/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6916 - acc: 0.5393 - val_loss: 0.6939 - val_acc: 0.4857\n",
      "Epoch 73/100\n",
      "560/560 [==============================] - 0s 45us/step - loss: 0.6917 - acc: 0.5393 - val_loss: 0.6940 - val_acc: 0.4857\n",
      "Epoch 74/100\n",
      "560/560 [==============================] - 0s 46us/step - loss: 0.6914 - acc: 0.5393 - val_loss: 0.6941 - val_acc: 0.4857\n",
      "Epoch 75/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.6916 - acc: 0.5393 - val_loss: 0.6942 - val_acc: 0.4857\n",
      "Epoch 76/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 0.6913 - acc: 0.5393 - val_loss: 0.6942 - val_acc: 0.4857\n",
      "Epoch 77/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 0.6912 - acc: 0.5393 - val_loss: 0.6943 - val_acc: 0.4857\n",
      "Epoch 78/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.6911 - acc: 0.5393 - val_loss: 0.6944 - val_acc: 0.4857\n",
      "Epoch 79/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6911 - acc: 0.6000 - val_loss: 0.6945 - val_acc: 0.4857\n",
      "Epoch 80/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6910 - acc: 0.5393 - val_loss: 0.6947 - val_acc: 0.4857\n",
      "Epoch 81/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.6908 - acc: 0.5875 - val_loss: 0.6948 - val_acc: 0.7429\n",
      "Epoch 82/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6908 - acc: 0.7196 - val_loss: 0.6948 - val_acc: 0.4857\n",
      "Epoch 83/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.6908 - acc: 0.5393 - val_loss: 0.6949 - val_acc: 0.7429\n",
      "Epoch 84/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6908 - acc: 0.6179 - val_loss: 0.6948 - val_acc: 0.4857\n",
      "Epoch 85/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6909 - acc: 0.6500 - val_loss: 0.6949 - val_acc: 0.4857\n",
      "Epoch 86/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6905 - acc: 0.5393 - val_loss: 0.6949 - val_acc: 0.4857\n",
      "Epoch 87/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6908 - acc: 0.5393 - val_loss: 0.6950 - val_acc: 0.4857\n",
      "Epoch 88/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.6906 - acc: 0.5393 - val_loss: 0.6951 - val_acc: 0.4857\n",
      "Epoch 89/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6904 - acc: 0.5393 - val_loss: 0.6952 - val_acc: 0.7429\n",
      "Epoch 90/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.6905 - acc: 0.6929 - val_loss: 0.6954 - val_acc: 0.7429\n",
      "Epoch 91/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6905 - acc: 0.7839 - val_loss: 0.6955 - val_acc: 0.7429\n",
      "Epoch 92/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6904 - acc: 0.7839 - val_loss: 0.6955 - val_acc: 0.7429\n",
      "Epoch 93/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6905 - acc: 0.7286 - val_loss: 0.6954 - val_acc: 0.7429\n",
      "Epoch 94/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6902 - acc: 0.6429 - val_loss: 0.6955 - val_acc: 0.7429\n",
      "Epoch 95/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6904 - acc: 0.7018 - val_loss: 0.6957 - val_acc: 0.7429\n",
      "Epoch 96/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6902 - acc: 0.7839 - val_loss: 0.6957 - val_acc: 0.7429\n",
      "Epoch 97/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.6904 - acc: 0.7839 - val_loss: 0.6956 - val_acc: 0.4857\n",
      "Epoch 98/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6901 - acc: 0.5393 - val_loss: 0.6956 - val_acc: 0.4857\n",
      "Epoch 99/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.6900 - acc: 0.5964 - val_loss: 0.6957 - val_acc: 0.4857\n",
      "Epoch 100/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6901 - acc: 0.5393 - val_loss: 0.6957 - val_acc: 0.4857\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZER = SGD(lr=0.1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape = (2,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=100, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Example: XOR problem with 2-layer Net\\\n",
    "\n",
    "We implement a 2-layer Feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape = (2,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/100\n",
      "560/560 [==============================] - 1s 2ms/step - loss: 0.7036 - acc: 0.5161 - val_loss: 0.7012 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.7017 - acc: 0.5161 - val_loss: 0.6987 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6999 - acc: 0.5161 - val_loss: 0.6974 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "560/560 [==============================] - 0s 50us/step - loss: 0.6989 - acc: 0.5161 - val_loss: 0.6960 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "560/560 [==============================] - 0s 45us/step - loss: 0.6981 - acc: 0.5161 - val_loss: 0.6955 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.6978 - acc: 0.5607 - val_loss: 0.6950 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.6975 - acc: 0.6768 - val_loss: 0.6946 - val_acc: 0.7571\n",
      "Epoch 8/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6972 - acc: 0.7321 - val_loss: 0.6943 - val_acc: 0.7571\n",
      "Epoch 9/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 0.6971 - acc: 0.7321 - val_loss: 0.6943 - val_acc: 0.7571\n",
      "Epoch 10/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6970 - acc: 0.7321 - val_loss: 0.6942 - val_acc: 0.7571\n",
      "Epoch 11/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6969 - acc: 0.7321 - val_loss: 0.6940 - val_acc: 0.7571\n",
      "Epoch 12/100\n",
      "560/560 [==============================] - 0s 63us/step - loss: 0.6968 - acc: 0.7321 - val_loss: 0.6939 - val_acc: 0.7571\n",
      "Epoch 13/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6968 - acc: 0.7321 - val_loss: 0.6938 - val_acc: 0.7571\n",
      "Epoch 14/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6966 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 15/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6966 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 16/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6965 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 17/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6966 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 18/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6964 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 19/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6964 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 20/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6964 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 21/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6964 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 22/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6964 - acc: 0.7321 - val_loss: 0.6936 - val_acc: 0.7571\n",
      "Epoch 23/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6965 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 24/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.6963 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 25/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6963 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 26/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.6962 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 27/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6962 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 28/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6962 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 29/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6961 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 30/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6961 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 31/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6963 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 32/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6961 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 33/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6961 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 34/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6960 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 35/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.6960 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 36/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6960 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 37/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6960 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 38/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6959 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 39/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6959 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 40/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6960 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 41/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6960 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 42/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6958 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 43/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.6961 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 44/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6958 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 45/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6957 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 46/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6957 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 47/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.6956 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 48/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6956 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 49/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6955 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 50/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6956 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 51/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.6956 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 52/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6954 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 53/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6955 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 54/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6954 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 55/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6953 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 56/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.6955 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 57/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.6953 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 58/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6953 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 59/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6956 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 60/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6952 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 25us/step - loss: 0.6952 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 62/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6953 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 63/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.6951 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 64/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6951 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 65/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6952 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 66/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6953 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 67/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.6953 - acc: 0.7321 - val_loss: 0.6936 - val_acc: 0.7571\n",
      "Epoch 68/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.6951 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.7571\n",
      "Epoch 69/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6954 - acc: 0.6839 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 70/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6953 - acc: 0.7321 - val_loss: 0.6930 - val_acc: 0.7571\n",
      "Epoch 71/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6950 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 72/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6949 - acc: 0.7321 - val_loss: 0.6930 - val_acc: 0.7571\n",
      "Epoch 73/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6950 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 74/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6949 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 75/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6949 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 76/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6951 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 77/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6948 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 78/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6948 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 79/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6947 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 80/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6950 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 81/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6949 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 82/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6947 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 83/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6947 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 84/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6947 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 85/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6946 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 86/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6946 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 87/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6949 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 88/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6946 - acc: 0.7321 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 89/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6946 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 90/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6946 - acc: 0.7321 - val_loss: 0.6930 - val_acc: 0.7571\n",
      "Epoch 91/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6944 - acc: 0.7321 - val_loss: 0.6930 - val_acc: 0.7571\n",
      "Epoch 92/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6945 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n",
      "Epoch 93/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6945 - acc: 0.7321 - val_loss: 0.6934 - val_acc: 0.7571\n",
      "Epoch 94/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6947 - acc: 0.7071 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 95/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6946 - acc: 0.6732 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 96/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.6945 - acc: 0.7321 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 97/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.6947 - acc: 0.6232 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 98/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6945 - acc: 0.5161 - val_loss: 0.6933 - val_acc: 0.7571\n",
      "Epoch 99/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.6943 - acc: 0.7321 - val_loss: 0.6932 - val_acc: 0.7571\n",
      "Epoch 100/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.6944 - acc: 0.7321 - val_loss: 0.6931 - val_acc: 0.7571\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=100, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the algorithm stucks at some local extremum. In fact, if we initialize with better weights and bias, we can get a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape = (2,), kernel_initializer=RandomUniform(-5, 5), bias_initializer=RandomUniform(-5, 5)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer=RandomUniform(-5, 5), bias_initializer=RandomUniform(-5, 5)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.get_layer(index=0).set_weights([np.array([[10, -10], [10, -10]]), np.array([-5, 15])])\n",
    "model.get_layer(index=2).set_weights([np.array([[10], [10]]), np.array([-15])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/100\n",
      "560/560 [==============================] - 1s 2ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 2/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "560/560 [==============================] - 0s 68us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 25us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "560/560 [==============================] - 0s 46us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=100, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 10.006285, -10.000407],\n",
       "        [ 10.005641, -10.001053]], dtype=float32),\n",
       " array([-4.993556 , 15.0052595], dtype=float32)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(index = 0).get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[10.078076],\n",
       "        [10.096514]], dtype=float32), array([-15.001636], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(index = 2).get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is not a local minimum, it suffices to classify the result. In general, we don't know which initialization is good. We often increase the number of units in the hidden layer to obtain the chance to get a better local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 6)                 18        \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 7         \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(6, input_shape = (2,), kernel_initializer=RandomUniform(-5, 5), bias_initializer=RandomUniform(-5, 5)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer=RandomUniform(-5, 5), bias_initializer=RandomUniform(-5, 5)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/100\n",
      "560/560 [==============================] - 2s 4ms/step - loss: 3.9807 - acc: 0.4839 - val_loss: 3.3641 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 3.3646 - acc: 0.4839 - val_loss: 2.7736 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 2.7599 - acc: 0.4839 - val_loss: 2.2065 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 2.1936 - acc: 0.4839 - val_loss: 1.7251 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.7372 - acc: 0.4839 - val_loss: 1.3918 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "560/560 [==============================] - 0s 50us/step - loss: 1.4317 - acc: 0.5643 - val_loss: 1.1686 - val_acc: 0.7571\n",
      "Epoch 7/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 1.2254 - acc: 0.5143 - val_loss: 1.0276 - val_acc: 0.5143\n",
      "Epoch 8/100\n",
      "560/560 [==============================] - 0s 52us/step - loss: 1.0882 - acc: 0.4607 - val_loss: 0.9280 - val_acc: 0.5143\n",
      "Epoch 9/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.9868 - acc: 0.4607 - val_loss: 0.8441 - val_acc: 0.5143\n",
      "Epoch 10/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.9033 - acc: 0.4607 - val_loss: 0.7831 - val_acc: 0.5143\n",
      "Epoch 11/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.8403 - acc: 0.4607 - val_loss: 0.7332 - val_acc: 0.5143\n",
      "Epoch 12/100\n",
      "560/560 [==============================] - 0s 46us/step - loss: 0.7880 - acc: 0.4607 - val_loss: 0.6892 - val_acc: 0.5143\n",
      "Epoch 13/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.7427 - acc: 0.4607 - val_loss: 0.6552 - val_acc: 0.5143\n",
      "Epoch 14/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.7074 - acc: 0.4607 - val_loss: 0.6269 - val_acc: 0.5143\n",
      "Epoch 15/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.6773 - acc: 0.4607 - val_loss: 0.6031 - val_acc: 0.5143\n",
      "Epoch 16/100\n",
      "560/560 [==============================] - 0s 45us/step - loss: 0.6521 - acc: 0.4607 - val_loss: 0.5823 - val_acc: 0.5143\n",
      "Epoch 17/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.6303 - acc: 0.4607 - val_loss: 0.5629 - val_acc: 0.5143\n",
      "Epoch 18/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.6086 - acc: 0.4607 - val_loss: 0.5464 - val_acc: 0.5143\n",
      "Epoch 19/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.5913 - acc: 0.4607 - val_loss: 0.5312 - val_acc: 0.5143\n",
      "Epoch 20/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.5749 - acc: 0.4607 - val_loss: 0.5173 - val_acc: 0.5143\n",
      "Epoch 21/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.5595 - acc: 0.4607 - val_loss: 0.5049 - val_acc: 0.5143\n",
      "Epoch 22/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.5462 - acc: 0.4607 - val_loss: 0.4930 - val_acc: 0.5143\n",
      "Epoch 23/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 0.5330 - acc: 0.4607 - val_loss: 0.4821 - val_acc: 0.5143\n",
      "Epoch 24/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.5212 - acc: 0.4607 - val_loss: 0.4721 - val_acc: 0.5143\n",
      "Epoch 25/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.5111 - acc: 0.4607 - val_loss: 0.4627 - val_acc: 0.5143\n",
      "Epoch 26/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.5003 - acc: 0.4607 - val_loss: 0.4538 - val_acc: 0.5143\n",
      "Epoch 27/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.4906 - acc: 0.4607 - val_loss: 0.4454 - val_acc: 0.5143\n",
      "Epoch 28/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.4815 - acc: 0.4607 - val_loss: 0.4378 - val_acc: 0.5143\n",
      "Epoch 29/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.4734 - acc: 0.6071 - val_loss: 0.4300 - val_acc: 0.5143\n",
      "Epoch 30/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 0.4647 - acc: 0.4786 - val_loss: 0.4228 - val_acc: 0.5143\n",
      "Epoch 31/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.4566 - acc: 0.4839 - val_loss: 0.4160 - val_acc: 0.7571\n",
      "Epoch 32/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.4494 - acc: 0.6661 - val_loss: 0.4095 - val_acc: 0.7571\n",
      "Epoch 33/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.4420 - acc: 0.7321 - val_loss: 0.4032 - val_acc: 0.7571\n",
      "Epoch 34/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.4351 - acc: 0.7321 - val_loss: 0.3973 - val_acc: 0.7571\n",
      "Epoch 35/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.7321 - val_loss: 0.3917 - val_acc: 0.7571\n",
      "Epoch 36/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.4228 - acc: 0.7321 - val_loss: 0.3861 - val_acc: 0.7571\n",
      "Epoch 37/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.4163 - acc: 0.7321 - val_loss: 0.3808 - val_acc: 0.7571\n",
      "Epoch 38/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.4105 - acc: 0.7321 - val_loss: 0.3757 - val_acc: 0.7571\n",
      "Epoch 39/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.4053 - acc: 0.8375 - val_loss: 0.3706 - val_acc: 0.7571\n",
      "Epoch 40/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3992 - acc: 0.8107 - val_loss: 0.3658 - val_acc: 0.7571\n",
      "Epoch 41/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3944 - acc: 0.8393 - val_loss: 0.3612 - val_acc: 0.7571\n",
      "Epoch 42/100\n",
      "560/560 [==============================] - 0s 48us/step - loss: 0.3889 - acc: 0.8714 - val_loss: 0.3568 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.3841 - acc: 0.9304 - val_loss: 0.3525 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3793 - acc: 1.0000 - val_loss: 0.3483 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.3747 - acc: 1.0000 - val_loss: 0.3441 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3701 - acc: 1.0000 - val_loss: 0.3401 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3659 - acc: 1.0000 - val_loss: 0.3364 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3616 - acc: 1.0000 - val_loss: 0.3328 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.3575 - acc: 1.0000 - val_loss: 0.3287 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3530 - acc: 1.0000 - val_loss: 0.3251 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.3491 - acc: 1.0000 - val_loss: 0.3217 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.3455 - acc: 1.0000 - val_loss: 0.3183 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.3416 - acc: 1.0000 - val_loss: 0.3149 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.3377 - acc: 1.0000 - val_loss: 0.3115 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.3339 - acc: 1.0000 - val_loss: 0.3082 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.3303 - acc: 1.0000 - val_loss: 0.3051 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.3272 - acc: 1.0000 - val_loss: 0.3021 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3234 - acc: 1.0000 - val_loss: 0.2990 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.3199 - acc: 1.0000 - val_loss: 0.2960 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.3166 - acc: 1.0000 - val_loss: 0.2929 - val_acc: 1.0000\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 50us/step - loss: 0.3133 - acc: 1.0000 - val_loss: 0.2900 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.3103 - acc: 1.0000 - val_loss: 0.2872 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3071 - acc: 1.0000 - val_loss: 0.2843 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.3040 - acc: 1.0000 - val_loss: 0.2817 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.3009 - acc: 1.0000 - val_loss: 0.2788 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.2977 - acc: 1.0000 - val_loss: 0.2761 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "560/560 [==============================] - 0s 48us/step - loss: 0.2947 - acc: 1.0000 - val_loss: 0.2735 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.2920 - acc: 1.0000 - val_loss: 0.2709 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2890 - acc: 1.0000 - val_loss: 0.2682 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.2860 - acc: 1.0000 - val_loss: 0.2658 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "560/560 [==============================] - 0s 55us/step - loss: 0.2832 - acc: 1.0000 - val_loss: 0.2633 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.2804 - acc: 1.0000 - val_loss: 0.2609 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.2778 - acc: 1.0000 - val_loss: 0.2585 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.2751 - acc: 1.0000 - val_loss: 0.2561 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.2725 - acc: 1.0000 - val_loss: 0.2538 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 0.2700 - acc: 1.0000 - val_loss: 0.2515 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.2676 - acc: 1.0000 - val_loss: 0.2493 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.2649 - acc: 1.0000 - val_loss: 0.2471 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 0.2626 - acc: 1.0000 - val_loss: 0.2450 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 0.2602 - acc: 1.0000 - val_loss: 0.2429 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.2580 - acc: 1.0000 - val_loss: 0.2411 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.2557 - acc: 1.0000 - val_loss: 0.2388 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.2536 - acc: 1.0000 - val_loss: 0.2367 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.2511 - acc: 1.0000 - val_loss: 0.2347 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.2489 - acc: 1.0000 - val_loss: 0.2327 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.2467 - acc: 1.0000 - val_loss: 0.2308 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.2445 - acc: 1.0000 - val_loss: 0.2288 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.2424 - acc: 1.0000 - val_loss: 0.2268 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.2403 - acc: 1.0000 - val_loss: 0.2249 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.2381 - acc: 1.0000 - val_loss: 0.2232 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.2362 - acc: 1.0000 - val_loss: 0.2215 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 0.2343 - acc: 1.0000 - val_loss: 0.2195 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 0.2321 - acc: 1.0000 - val_loss: 0.2179 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.2303 - acc: 1.0000 - val_loss: 0.2161 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 0.2283 - acc: 1.0000 - val_loss: 0.2144 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 0.2264 - acc: 1.0000 - val_loss: 0.2126 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.2245 - acc: 1.0000 - val_loss: 0.2109 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 0.2226 - acc: 1.0000 - val_loss: 0.2092 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 0.2209 - acc: 1.0000 - val_loss: 0.2076 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 0.2190 - acc: 1.0000 - val_loss: 0.2060 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=100, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 0.6345892 ,  0.21349257,  5.190718  ,  2.6648445 ,  5.9560175 ,\n",
       "           3.2525434 ],\n",
       "         [-2.5391288 ,  0.7896347 , -3.460859  , -2.393607  ,  3.5305624 ,\n",
       "          -2.6143186 ]], dtype=float32),\n",
       "  array([ 0.27963892,  4.7800684 ,  1.5024713 , -6.227105  , -1.5551796 ,\n",
       "          2.260877  ], dtype=float32)],\n",
       " [array([[ 4.3241935],\n",
       "         [ 1.5521915],\n",
       "         [-2.3014896],\n",
       "         [-3.4141047],\n",
       "         [ 4.4846277],\n",
       "         [-3.6434932]], dtype=float32), array([-1.9597361], dtype=float32)])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(index = 0).get_weights(), model.get_layer(index = 2).get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Elements of Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Cost Function\n",
    "\n",
    "The cost function $L(\\mathbf x_{(1)}, \\ldots, \\mathbf t_{(N)}, t_{(1)}, \\ldots, t_{(N)})$ can be defined as the sum/mean of the loss on the $N$ training values\n",
    "\n",
    "$$\n",
    "L(\\mathbf x_{(1)}, \\ldots, \\mathbf t_{(N)}, t_{(1)}, \\ldots, t_{(N)}) = \\sum_{n=1}^N l(f(\\mathbf x_n), t_n)) = \\sum_{n=1}^N l(y_n, t_n))\n",
    "$$\n",
    "\n",
    "where $y$ calculated from $\\mathbf x$ and the model architecture.\n",
    "\n",
    "Like basic regression and classification problem models, we can choose:\n",
    "\n",
    "- Mean Squared Error for regression\n",
    "$$\n",
    "l(t, y) = \\frac12 |t-y|^2\n",
    "$$\n",
    "\n",
    "- Binary Cross-entropy for binary classification\n",
    "$$\n",
    "l(t, y) = -t\\log y - (1-t)\\log(1-y)\n",
    "$$\n",
    "where $y\\in [0,1]$ predicted as a probability.\n",
    "\n",
    "- Cross-entropy for for multiclass classification\n",
    "$$\n",
    "l(t, y) = -\\sum_{k=1}^K t_k \\log y_k\n",
    "$$\n",
    "where $y$ predicted as $(y_1, \\ldots, y_K)$, $0 \\leq y_k \\leq 1$, $\\sum_{k=1}^K y_k = 1.$\n",
    "\n",
    "We know that Mean Squared Error is obtained when we suppose that $t$ is a random variable following Gaussian distribution of mean $y=f(\\mathbf x)$ and some variance; Binary Cross-entropy is obtained when we suppose that $t$ follows a Bernoulli distribution of parameter $y$; while general Cross-entropy is obtained with supposition that $t$ follows a multinomial distribution of paramter $y = (y_1, \\ldots, y_K)$.\n",
    "\n",
    "If we change, for example in case of regression, $t$ follows a symmetric exponential distribution, then the loss become \n",
    "$$\n",
    "l(t, y) = \\Vert t - y \\Vert_1\n",
    "$$\n",
    "It is an example to show that we can change the loss function by changing the hypothesis of the probability distribution $p(t|\\mathbf x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 The Output Layer\n",
    "\n",
    "For regression, there is no constraint of the output layer, it can be any real number, so the simplest activation function can be chosen is the identity function.\n",
    "\n",
    "For binary classification, if we choose binary cross-entropy as the loss function, we expect the output to be some number between $[0,1]$, so the activation function at the output layer un should be a function $\\mathbf R \\to [0,1]$. The sigmoid function is one possible choice and widely used. Note that if we choose another loss function (like $[yt]_{+}$), we can use another activation function (like the identity).\n",
    "\n",
    "For multiclass classification, if we choose the general cross-entropy as the loss function, we also expect the output to be a vector summed to 1. A possible choice of the activation function at the output layer is the softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Hidden Layers\n",
    "\n",
    "Choice of hidden layers does not depend on the cost function. The most popular are:\n",
    "\n",
    "### 4.3.1 Rectified Linear Unit (ReLU)\n",
    "$$\n",
    "h(a) = [a]_{+} = \\max(0, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d579a58>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(a):\n",
    "    return np.max([0, a])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "interval = np.linspace(-5, 5, 101)\n",
    "plt.plot(interval, [relu(x) for x in interval])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Logistic Sigmoid\n",
    "$$\n",
    "\\sigma(a) = \\frac1{1+ \\exp(-a)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d4f6390>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl81NW9//HXJ3tIQgIkQAhh3wWVEllqF21dcLerYLHaqtxabbW3am2xXn/a+9OrbW2t9ipa94WruNGKVVux5aeCBGSHQAgQkgDZQ/ZkknP/SPSXYjADTPKd5f18PHiQmTkm75Hk/Tg5851zzDmHiIiElyivA4iISOCp3EVEwpDKXUQkDKncRUTCkMpdRCQMqdxFRMKQyl1EJAyp3EVEwpDKXUQkDMV49YXT09PdqFGjvPryIiIhae3ateXOuYyexnlW7qNGjSI3N9erLy8iEpLMbK8/47QsIyIShlTuIiJhSOUuIhKGeix3M3vMzErNbPMRHjczu9/M8s1so5l9LvAxRUTkaPgzc38CmPsZj58DjO/8sxD47+OPJSIix6PHcnfO/ROo/IwhFwFPuQ6rgDQzywxUQBEROXqBWHPPAvZ1uV3UeZ+IiHgkENe5Wzf3dXt2n5ktpGPphhEjRgTgS4uIBAfnHI2tbdQ1+TjU5KOu2Udd59/1zT4aWttoaPZR39LGVycN5qTstF7NE4hyLwKyu9weDpR0N9A5txhYDJCTk6PDW0UkKLW1OyrqmymvbaGyvoWK+mYq61uoamilqr6FqoYWahpbP/lT2+TjUGMrvnb/am1wSnxIlPsy4DozWwLMAmqcc/sD8HlFRAKuocVHcVUjxdWN7K9pYn9NEwdqGimtbebgoWbKapuoqG/BddPTZpCaGMuAfnGf/D1qUBKpibGkJMSQkhBL/8QYkuNjSEmIITk+ln5x0STHx5AUH0O/uGgSY6OJiupuwSOweix3M3seOA1IN7Mi4D+AWADn3EPAcuBcIB9oAL7XW2FFRHrinKOivoXd5fXsLq9nT3k9hZUNFFY2sK+ygaqG1n8ZbwYZyfEM6Z9AVloCJ2enkpEcT0ZKPIOS4xmUFMeg5DgG9IsjrV8c0X1QzIHQY7k75+b38LgDrg1YIhERP1XWt7B9/yG2H6hlx8FadpbWkV9aR03j/y/wmCgja0AiIwb2Y+q0TIYPSCQrreNPZloig1PiiY0Ov/dzerZxmIjI0Siva2Z9YTWbimvYUlLD5uJDHDjU9MnjA/rFMn5ICuefmMnYjGRGZyQxJj2JrLREYsKwvHuicheRoNPe7thZWseHeyrJ3VPJusIq9lU2Ah3LKGMzkpkzdhBTMvszKTOFSUP7k5ES73Hq4KJyFxHPOefYXV7Pe7sqeD+/nA8KKqjuXBsfnBLPjJEDWDBrJCdnpzE1K5WkeFVXT/R/SEQ80dTaxvu7ylmxvYx3d5R+MjPPSkvkzMlDmDVmEDNHDSR7YCJmofEiZjBRuYtIn6ltauXv20p5c8sB/rGjjIaWNhJjozl13CAWfmksXxyXzshB/VTmAaByF5Fe1dTaxttbD/LnDSW8u6OMFl87g1Pi+dr0LM6cMoQ5YwcRHxPtdcywo3IXkYBzzrGusJqla4v4y8YSapt8DE6J59KZI7jgpEymZw/okzfyRDKVu4gEzKGmVl5ZV8xzqwvJO1hLYmw050wdyjdmDGf2mEEh8wagcKByF5Hjtru8nsff282LuUU0trYxLSuVu74+jQtOGkayrmzxhP6vi8gxW7u3iv9+dxd/336Q2KgoLjhpGJd/fiQnDu/dTbGkZyp3ETkqzjk+2FXBH97J54OCCgb0i+VHp49jwZyRDE5J8DqedFK5i4jf1u6t5J6/5rF6dyWDU+K59bzJXDprBP3iVCXBRv8iItKjHQdrufuN7byzvZT05Hhuv2AK82aOICFWlzAGK5W7iBxRZX0L9729g+c+LCQpLpqb507kis+P0kw9BOhfSEQ+pa3d8cyqvfz6rTwaWtpYMGsEN5wxgQFJcV5HEz+p3EXkX2wsqmbRK5vZVFzDF8al8x8XTGH8kBSvY8lRUrmLCACNLW385q08HntvN4OS47l//nQuODFT+7yEKJW7iJC7p5Kblm5kd3k935k1gp+dM4n+CbFex5LjoHIXiWAtvnZ++/YOHv7nLrLSEnnuqll8fly617EkAFTuIhGqoKyO65esZ1NxDfNnZnPreVN0CEYY0b+kSAR65aMiFr2ymbiYKB5aMIO5U4d6HUkCTOUuEkGafW3c8eetPLu6kJmjB/L7eSeTmZrodSzpBSp3kQhRUt3INc+sZUNRDf/2pTHcdPZEYqKjvI4lvUTlLhIB1u6t4t+eXktTaxsPLfgcc6dmeh1JepnKXSTMvbS2iJ+/vInMtASWLJzFuMF6Q1IkULmLhCnnHL9+K48HV+xizphB/PE7n9P2ARFE5S4Shlrb2vnZSxt5eV0x807J5s6LpxKr9fWIonIXCTN1zT6ueWYtK3eW8+9nTuBHXxmnLQQikMpdJIxUN7Rw+WMfsrnkEPd840S+fUq215HEIyp3kTBRVtvMZX9aTUF5PQ8vmMEZU4Z4HUk8pHIXCQMl1Y0seHQ1+2uaePyKUzhV+8NEPL9eYTGzuWaWZ2b5ZnZLN4+PMLMVZvaRmW00s3MDH1VEulNS3ci8xasoq23m6StnqtgF8KPczSwaeBA4B5gCzDezKYcNuxV4wTk3HZgH/DHQQUXk0w7UNDH/kVVU1bfw9FWzyBk10OtIEiT8mbnPBPKdcwXOuRZgCXDRYWMc0L/z41SgJHARRaQ7Bw91FHtFXQtPXjmTk7PTvI4kQcSfNfcsYF+X20XArMPG3A68ZWY/ApKAMwKSTkS6VVnfwnceXU3poSaeunImnxsxwOtIEmT8mbl3d4GsO+z2fOAJ59xw4FzgaTP71Oc2s4VmlmtmuWVlZUefVkSoa/ZxxeMfUljZwKOXn8KMkVqKkU/zp9yLgK4Xyw7n08suVwIvADjnPgASgE+9quOcW+ycy3HO5WRkZBxbYpEI1tTaxsKnctlScog/Xvo55owd5HUkCVL+lPsaYLyZjTazODpeMF122JhC4KsAZjaZjnLX1FwkgNraHTcsWc/7uyr49bdO1HXs8pl6LHfnnA+4DngT2EbHVTFbzOwOM7uwc9hPgavNbAPwPHCFc+7wpRsROQ6/en0rf91ygF+eP4WvTR/udRwJcn69ick5txxYfth9t3X5eCtwamCjicjHHl1ZwOPv7eHKL4zmyi+M9jqOhABtEycS5N7YtJ//XL6Nc6YOZdG5k72OIyFC5S4SxDYWVXPD/6xnenYa911yMlFR2t1R/KNyFwlSB2qauPqpXNKT41n83RwSYqO9jiQhRBuHiQShxpY2Fj6dS22Tj5eu+TzpyfFeR5IQo3IXCTLOOW5+aSObimtYfFkOkzP79/wfiRxGyzIiQebRlbv584YSbjxrImfqWnY5Rip3kSDyfn45d73RcWXMD08b63UcCWEqd5EgUVzdyHXPf8SYjGTu/dZJOvdUjovKXSQINPva+OEza2n1tfPwZTNIjtfLYXJ89B0kEgTuWr6dDUU1PLRgBmMzkr2OI2FAM3cRjy3ftJ8n3t/D908dzdypQ72OI2FC5S7ioT3l9dy8dCMnZ6dxyzmTvI4jYUTlLuKRZl8b1z63jugo44FLpxMXox9HCRytuYt45J6/5rGl5BCPfDeH4QP6eR1HwoymCiIeWLG9lD/9v91cPmek3qgkvULlLtLHSg81ceOLG5g0NIWfawtf6SValhHpQ+3tjp++uIH6Fh9L5s/WTo/SazRzF+lDT7y/h5U7y7n1vCmMH5LidRwJYyp3kT6y82Atd/91O1+ZNJjvzBrhdRwJcyp3kT7Q4mvn+iXrSY6P4e5vTNO+MdLrtOYu0gd+97cdbN1/iIcvm8HglASv40gE0MxdpJet3VvFQ//YxbdzhnP2CdpeQPqGyl2kFzW2tHHTixvITE3kl+dP8TqORBAty4j0ol+/lUdBeT3PXjWLlIRYr+NIBNHMXaSXfLi7ksfe281ls0dy6rh0r+NIhFG5i/SChhYfNy3dQPaAftrtUTyhZRmRXvDrN3ewt6KBJQtnk6RTlcQDmrmLBNjavVU8/v5uFswewewxg7yOIxFK5S4SQE2tbdy8dAPDUhO55RxtCibe0e+LIgF0/993squsnie/P1OHXIunNHMXCZDNxTU8/M8CvjljOF+ekOF1HIlwfpW7mc01szwzyzezW44w5ttmttXMtpjZc4GNKRLcfG3t3PLyRgb0i+PW87QcI97r8fdGM4sGHgTOBIqANWa2zDm3tcuY8cDPgVOdc1VmNri3AosEo8ff28Pm4kM8cOl00vrFeR1HxK+Z+0wg3zlX4JxrAZYAFx025mrgQedcFYBzrjSwMUWCV2FFA795O48zJg/mvGmZXscRAfwr9yxgX5fbRZ33dTUBmGBm75nZKjObG6iAIsHMOceiVzcRExXFnRdP1Va+EjT8eTm/u+9W183nGQ+cBgwHVprZVOdc9b98IrOFwEKAESN0WIGEvlfXF7NyZzl3XHQCmamJXscR+YQ/M/ciILvL7eFASTdjXnPOtTrndgN5dJT9v3DOLXbO5TjncjIydDWBhLaq+hbu/Ms2Ts5OY8GskV7HEfkX/pT7GmC8mY02szhgHrDssDGvAqcDmFk6Hcs0BYEMKhJs7npjGzWNrdz19WlERWk5RoJLj+XunPMB1wFvAtuAF5xzW8zsDjO7sHPYm0CFmW0FVgA3Oecqeiu0iNdWFVTwQm4RV31xNJMz+3sdR+RTzLnDl8/7Rk5OjsvNzfXka4scj2ZfG+f+fiXNvnbe/smXSYyL9jqSRBAzW+ucy+lpnN4fLXKUFv+jgF1l9Tz+vVNU7BK0tP2AyFHYU17PH1bkc96JmZw+Ue/Vk+Clchfxk3OOX762mbjoKG7TeagS5FTuIn56fdN+Vu4s56dnTWBI/wSv44h8JpW7iB8ONbVyx5+3MjWrP5fN1jXtEvz0gqqIH3771g7K6pp55Ls5xERrTiTBT9+lIj3YXFzDUx/sYcGskZyUneZ1HBG/qNxFPkN7u2PRq5sZmBTHjWdP9DqOiN9U7iKf4fk1hWzYV82i8yaTmhjrdRwRv6ncRY6goq6Ze/6ax+wxA7n45MN3uRYJbip3kSO4643t1Df7uPMi7dMuoUflLtKNNXsqWbq2iKu+OIbxQ1K8jiNy1FTuIofxtbXzy1c3Myw1gR9/dZzXcUSOia5zFznME+/vYfuBWh6+bAb94vQjIqFJM3eRLg7UNHHf2zv4yqTBnDVliNdxRI6Zyl2kiztf34qv3XH7BSfoRVQJaSp3kU4rd5bx+sb9XHv6OEYM6ud1HJHjonIXoeN0pdte28Lo9CQWfmmM13FEjpteLRKh43Sl3eX1PPX9mSTE6nQlCX2auUvEK6xo4IEV+Zw3LZMvTcjwOo5IQKjcJaI55/iPZZuJiTJuPX+y13FEAkblLhHtzS0HWZFXxk/OnEBmaqLXcUQCRuUuEau+2ccdf97CpKEpXPH5UV7HEQkolbtErPvf2UlJTRO/uniqTleSsKPvaIlIOw7W8qeVu7kkJ5ucUQO9jiMScCp3iTjt7Y5Fr2wiOSGGn50zyes4Ir1C5S4RZ+naItbsqeIX50xmYFKc13FEeoXKXSJKZX0L//eNbZwyagDfnDHc6zgivUblLhHlruXbqGvy8Z9fm0ZUlDYGk/ClcpeIsbqgghfXFnH1l8YwQacrSZhTuUtEaPa18YtXNpGVlsiPvzLe6zgivc6vcjezuWaWZ2b5ZnbLZ4z7ppk5M8sJXESR47f4HwXsKqvnV1+bSmKcNgaT8NdjuZtZNPAgcA4wBZhvZlO6GZcC/BhYHeiQIsejoKyOP6zI5/wTMzl94mCv44j0CX9m7jOBfOdcgXOuBVgCXNTNuDuBe4CmAOYTOS7OOW59dTPxMVHcdv6n5iQiYcufcs8C9nW5XdR53yfMbDqQ7Zz7SwCziRy3l9cV8/6uCn42dxKD+yd4HUekz/hT7t1dL+Y+edAsCrgP+GmPn8hsoZnlmlluWVmZ/ylFjkF5XTN3vr6VGSMHcOnMEV7HEelT/pR7EZDd5fZwoKTL7RRgKvCume0BZgPLuntR1Tm32DmX45zLycjQoQjSu+78y1bqm33c/XVd0y6Rx59yXwOMN7PRZhYHzAOWffygc67GOZfunBvlnBsFrAIudM7l9kpiET+syCvltfUlXHv6OMbrmnaJQD2Wu3POB1wHvAlsA15wzm0xszvM7MLeDihytOqbfdz6ymbGDU7mmtPGeh1HxBN+HZDtnFsOLD/svtuOMPa0448lcuzufTOPkppGlv5gDvExuqZdIpPeoSphJXdPJU9+sIfvzh7JjJHap10il8pdwkZTaxs3L93IsNREbp6rfdolsvm1LCMSCn73t50UlNfzzJWzSIrXt7ZENs3cJSxsLKrmkZUFXJKTzRfGp3sdR8RzKncJec2+Nm58cQPpyXH84rzJXscRCQr63VVC3u/+tpMdB+t4/HunkJoY63UckaCgmbuEtHWFVTz8j11ckpOtHR9FulC5S8hqau1YjhnaP4Fbz9dyjEhXWpaRkHXPX/MoKOu4OiYlQcsxIl1p5i4h6b38ch57bzeXzR6pq2NEuqFyl5BT09DKjS9uYEx6Er84V8sxIt3RsoyEnNuWbaastpmXrvm8zkMVOQLN3CWkLNtQwmvrS/jxV8dzUnaa13FEgpbKXULGvsoGFr28iekj0vihtvIV+UwqdwkJvrZ2rl/yEQD3z5tOTLS+dUU+i9bcJST8/u87WVdYzf3zp5M9sJ/XcUSCnqY/EvRWFVTwwIp8vjVjOBeeNMzrOCIhQeUuQa28rpnrl3zE6EFJ3H7hCV7HEQkZWpaRoNXW7vjJ/6ynuqGVx6+YqT3aRY6CflokaD24Ip+VO8u5++vTmDKsv9dxREKKlmUkKL2fX859f9vB16Zncckp2V7HEQk5KncJOvtrGvnR8x8xJj2JX108FTPzOpJIyFG5S1Bp9rXxg2fW0exr5+HLcrTOLnKM9JMjQeX2ZVvYsK+ahxbMYNzgZK/jiIQszdwlaDz/YSHPf7iPa08fy9ypQ72OIxLSVO4SFFYXVHDba5v50oQM/v3MiV7HEQl5Knfx3L7KBq55dh3ZA/vxh/nTiY7SC6gix0vlLp6qbWrlyifX0Nbu+NPlp5CaqOPyRAJBL6iKZzp2elzPrrJ6nvr+TEanJ3kdSSRsaOYunnDOcduyLbyzvZT/c+EJnDpO56CKBJLKXTzxx3d38dzqQq45bSwLZo/0Oo5I2FG5S5979aNi7n0zjwtPGsZNZ+nKGJHe4Fe5m9lcM8szs3wzu6Wbx//dzLaa2UYz+7uZaSom3VqRV8qNL25g9piB3PutE4nSlTEivaLHcjezaOBB4BxgCjDfzKYcNuwjIMc5dyKwFLgn0EEl9OXuqeSaZ9YycWgKi7+bQ3xMtNeRRMKWPzP3mUC+c67AOdcCLAEu6jrAObfCOdfQeXMVMDywMSXUbS05xPeeWMOw1ESe/P5M+ifokkeR3uRPuWcB+7rcLuq870iuBN7o7gEzW2hmuWaWW1ZW5n9KCWk7D9by3cdWkxwfw1NXziQ9Od7rSCJhz59y725R1HU70GwBkAPc293jzrnFzrkc51xORkaG/yklZOWX1jH/kdWYGc9cNYvhA3S4tUhf8OdNTEVA19MShgMlhw8yszOARcCXnXPNgYknoaygrI5LH1kFwPNXz2JshnZ5FOkr/szc1wDjzWy0mcUB84BlXQeY2XTgYeBC51xp4GNKqMkvrWX+I6toa3c8d/Usxg1O8TqSSETpsdydcz7gOuBNYBvwgnNui5ndYWYXdg67F0gGXjSz9Wa27AifTiLA5uIavv3wKtra4bmrZzNhiIpdpK/5tbeMc245sPyw+27r8vEZAc4lIWrt3iquePxDUuJjePbq2dovRsQj2jhMAmZFXinXPruOwSnxPHv1bLLSEr2OJBKxtP2ABMQLufu46slcxmQk8cIP5qjYRTymmbscF+ccf3gnn9++vYMvjk/nvxfMIFmHWot4Tj+FcsyaWtu45aWNvLq+hK9/Lov/+saJxEbrl0GRYKByl2NSeqiJq59ey4Z91dx09kR+eNpYzLQJmEiwULnLUVtXWMUPn1lHTWMrDy2YwdypQ72OJCKHUbmL35xzPPXBXn71+laGpiaw9Jo5nDAs1etYItINlbv4pa7Zxy9e3sSyDSWcMXkwv/nWyaT2086OIsFK5S49+qiwiuuXrKeoqoGbzp7INV8eq0M2RIKcyl2OyNfWzkP/2MV9f9vJ0P4JvPBvc8gZNdDrWCLiB5W7dGvnwVpuXLqRDfuqueCkYfzq4qmkJmoZRiRUqNzlX7S2tfPoyt3c9/YOkuKj+cP86Zx/YqYucxQJMSp3+cTavZUsemUz2w/UMveEodx58VQyUnRqkkgoUrkLFXXN3PtmHkvW7CMzNYGHFszg7BOGaLYuEsJU7hGsxdfOk+/v4f53dtLQ0sbVXxzNDWdMIEl7w4iEPP0UR6D2dsfrm/bzm7fy2FPRwGkTM1h07mTG61ANkbChco8gzjne2V7Kr9/awbb9h5g4JIUnvncKp00c7HU0EQkwlXsEaG93vL3tIA+8k8+m4hpGDurH7y45mQtOGka03owkEpZU7mGs2dfGnzfs55F/FpB3sJaRg/px99en8Y0Zw7U1r0iYU7mHobLaZp7/sJCnPthLeV0zE4ek8Pt5J3PetExiVOoiEUHlHiacc3xQUMGzqwt5a8sBWtscp03M4KovjOHUcYN0WaNIhFG5h7h9lQ28tK6Il9cVU1jZQGpiLJfPGcX8WSMYm5HsdTwR8YjKPQSVHmri9U37eX3jfnL3VmEGc8YM4oYzxnPutEwSYqO9jigiHlO5h4g95fW8vfUgb209QO7eKpyDSUNTuOnsiVw8PYustESvI4pIEFG5B6mm1jbW7Knk3bwy3s0rZVdZPQCTM/vz46+M54KTMhk3WG86EpHuqdyDRLOvjc3FNawqqOT9XeXk7qmi2ddOXEwUs0YP5DuzRnLmlCFkD+zndVQRCQEqd4+U1jaxvrCaj/ZVs3ZvFRv2VdPsawc6ZucLZo/k1HGDmDMmncQ4raGLyNFRufcy5xzF1Y1s31/LlpJDbC6pYXNxDftrmgCIiTKmDOso81NGDeSUUQMYlKxtdkXk+KjcA8TX1k5xdSMF5fXsKq0jv7SOnaV17DhQS22zDwAzGJOexMzRA5mWlcr0EWmcMCxVV7eISMCp3P3knKOqoZXiqkaKqxsoqmpkX2UDeysbKKxoYF9VA61t7pPxg5LiGDs4mYumD2NyZn8mDe3PxKEpJGs7XRHpAxHfNL62dqoaWqmsb6G8rpmy2mbK65o5eKiJ0tqOvw/UNLG/pumTNfGPJcfHMGJgPyYMSeGsE4YyJj2JUelJjM1I0tKKiHjKr3I3s7nA74Fo4FHn3N2HPR4PPAXMACqAS5xzewIbtXvOOZpa26lr9lHf7KOu2Udtk4/aplZqm3wcamrlUKOP6sYWahpbqWlopaqhheqGViobOu5z7tOfNz4miiH9ExicEs/UrFTOOmEoQ/snMCwtkeEDEslKSyStX6ze1i8iQanHcjezaOBB4EygCFhjZsucc1u7DLsSqHLOjTOzecB/AZf0RuD/WVPIw/8soKG5jfoWHw0tbbS1d9POh0mOjyE1MZbUxFgGJMUyLC2RAf3iGJgUx6Dkjr/Tk+PJSIknPTme/gkxKm4RCVn+zNxnAvnOuQIAM1sCXAR0LfeLgNs7P14KPGBm5lx3c+LjMzApnimZ/UmKiyExLpqk+GiS4mNIjo8hKS6GlIQYkhNiSInvKPKUhI77tBuiiEQSf8o9C9jX5XYRMOtIY5xzPjOrAQYB5V0HmdlCYCHAiBEjjinwmVOGcOaUIcf034qIRAp/prPdrU0cPiP3ZwzOucXOuRznXE5GRoY/+URE5Bj4U+5FQHaX28OBkiONMbMYIBWoDERAERE5ev6U+xpgvJmNNrM4YB6w7LAxy4DLOz/+JvBOb6y3i4iIf3pcc+9cQ78OeJOOSyEfc85tMbM7gFzn3DLgT8DTZpZPx4x9Xm+GFhGRz+bXde7OueXA8sPuu63Lx03AtwIbTUREjpWuDxQRCUMqdxGRMKRyFxEJQ+bVRS1mVgbs9eSLH590DntzVgSItOccac8X9JxDyUjnXI9vFPKs3EOVmeU653K8ztGXIu05R9rzBT3ncKRlGRGRMKRyFxEJQyr3o7fY6wAeiLTnHGnPF/Scw47W3EVEwpBm7iIiYUjlfhzM7EYzc2aW7nWW3mRm95rZdjPbaGavmFma15l6i5nNNbM8M8s3s1u8ztPbzCzbzFaY2TYz22Jm13udqa+YWbSZfWRmf/E6S29QuR8jM8um4+jBQq+z9IG3ganOuROBHcDPPc7TK7ocKXkOMAWYb2ZTvE3V63zAT51zk4HZwLUR8Jw/dj2wzesQvUXlfuzuA26mm0NJwo1z7i3nnK/z5io69vQPR58cKemcawE+PlIybDnn9jvn1nV+XEtH2WV5m6r3mdlw4DzgUa+z9BaV+zEwswuBYufcBq+zeOD7wBteh+gl3R0pGfZF9zEzGwVMB1Z7m6RP/I6OyVm710F6i19b/kYiM/sbMLSbhxYBvwDO6ttEveuznq9z7rXOMYvo+DX+2b7M1of8Oi4yHJlZMvAScINz7pDXeXqTmZ0PlDrn1prZaV7n6S0q9yNwzp3R3f1mNg0YDWwwM+hYolhnZjOdcwf6MGJAHen5fszMLgfOB74axqds+XOkZNgxs1g6iv1Z59zLXufpA6cCF5rZuUAC0N/MnnHOLfA4V0DpOvfjZGZ7gBznXChuQOQXM5sL/Bb4snOuzOs8vaXz/N8dwFeBYjqOmLzUObfF02C9yDpmKE8Clc65G7zO09c6Z+43OufO9zpLoGnNXfzxAJACvG1m683sIa8D9YbOF40/PlJyG/BCOBd7p1OBy4CvdP7bru+c0UqI08xdRCQMaeYuIhKGVO7NET3wAAAAK0lEQVQiImFI5S4iEoZU7iIiYUjlLiIShlTuIiJhSOUuIhKGVO4iImHofwHDdBgKCn2g0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(a):\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "plt.plot(interval, sigmoid(interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoidal units saturate across most of their domain (i.e., their gradient is nearly zero across most of their domain). This will make gradient-based learning very difficult. Their use as hidden units is usually discouraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 tanh Function\n",
    "$$\n",
    "h(a) = \\tanh (a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d51afd0>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(interval, np.tanh(interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Universal Approximation Properties\n",
    "\n",
    "The **universal approximation theorem** states that a feedforward network with a linear output layer and at least one hidden layer with \n",
    "- logistic sigmoid activation function\n",
    "- tanh activation function\n",
    "- rectified linear unit activation function\n",
    "can approximately any Borel measureable function from one finite-dimensional space to another, with any desired non-zero amount of errors like:\n",
    "- MSE\n",
    "- (binary, multiclass) cross-entropy\n",
    "provided that the network is given **enough** hidden units.\n",
    "\n",
    "So, a feedforward network with single layer is sufficient to represent any function, but the layer may be very large and may fail to learn correctly with gradient-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Approximation of the function $x^2$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.636879</td>\n",
       "      <td>-3.266407</td>\n",
       "      <td>-1.131050</td>\n",
       "      <td>1.954376</td>\n",
       "      <td>-0.192230</td>\n",
       "      <td>-3.321515</td>\n",
       "      <td>-4.689132</td>\n",
       "      <td>-1.377835</td>\n",
       "      <td>-4.627739</td>\n",
       "      <td>4.002532</td>\n",
       "      <td>...</td>\n",
       "      <td>4.33169</td>\n",
       "      <td>-3.581613</td>\n",
       "      <td>-3.325948</td>\n",
       "      <td>3.811807</td>\n",
       "      <td>-0.006702</td>\n",
       "      <td>-1.193012</td>\n",
       "      <td>4.599222</td>\n",
       "      <td>-2.518922</td>\n",
       "      <td>1.616815</td>\n",
       "      <td>-4.772549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.226886</td>\n",
       "      <td>10.669415</td>\n",
       "      <td>1.279275</td>\n",
       "      <td>3.819586</td>\n",
       "      <td>0.036952</td>\n",
       "      <td>11.032459</td>\n",
       "      <td>21.987963</td>\n",
       "      <td>1.898430</td>\n",
       "      <td>21.415967</td>\n",
       "      <td>16.020263</td>\n",
       "      <td>...</td>\n",
       "      <td>18.76354</td>\n",
       "      <td>12.827953</td>\n",
       "      <td>11.061933</td>\n",
       "      <td>14.529872</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>1.423278</td>\n",
       "      <td>21.152844</td>\n",
       "      <td>6.344969</td>\n",
       "      <td>2.614092</td>\n",
       "      <td>22.777221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1         2         3         4          5          6    \\\n",
       "0  -3.636879  -3.266407 -1.131050  1.954376 -0.192230  -3.321515  -4.689132   \n",
       "1  13.226886  10.669415  1.279275  3.819586  0.036952  11.032459  21.987963   \n",
       "\n",
       "        7          8          9      ...           990        991        992  \\\n",
       "0 -1.377835  -4.627739   4.002532    ...       4.33169  -3.581613  -3.325948   \n",
       "1  1.898430  21.415967  16.020263    ...      18.76354  12.827953  11.061933   \n",
       "\n",
       "         993       994       995        996       997       998        999  \n",
       "0   3.811807 -0.006702 -1.193012   4.599222 -2.518922  1.616815  -4.772549  \n",
       "1  14.529872  0.000045  1.423278  21.152844  6.344969  2.614092  22.777221  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.uniform(-5, 5, size=(1000, 1))\n",
    "y = (X ** 2).flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "pd.DataFrame([X[:,0], y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/100\n",
      "560/560 [==============================] - 2s 4ms/step - loss: 67.0041 - val_loss: 26.4730\n",
      "Epoch 2/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 22.1227 - val_loss: 21.2005\n",
      "Epoch 3/100\n",
      "560/560 [==============================] - 0s 46us/step - loss: 18.3511 - val_loss: 18.5854\n",
      "Epoch 4/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 16.1558 - val_loss: 16.4015\n",
      "Epoch 5/100\n",
      "560/560 [==============================] - 0s 45us/step - loss: 14.1631 - val_loss: 14.4693\n",
      "Epoch 6/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 13.0349 - val_loss: 13.6919\n",
      "Epoch 7/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 11.8046 - val_loss: 11.9613\n",
      "Epoch 8/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 10.8738 - val_loss: 11.0355\n",
      "Epoch 9/100\n",
      "560/560 [==============================] - 0s 50us/step - loss: 9.9145 - val_loss: 10.6021\n",
      "Epoch 10/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 9.2531 - val_loss: 9.4695\n",
      "Epoch 11/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 8.5979 - val_loss: 8.8155\n",
      "Epoch 12/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 8.0236 - val_loss: 8.2725\n",
      "Epoch 13/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 7.3837 - val_loss: 7.7706\n",
      "Epoch 14/100\n",
      "560/560 [==============================] - 0s 50us/step - loss: 6.9683 - val_loss: 7.3715\n",
      "Epoch 15/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 6.7191 - val_loss: 6.8272\n",
      "Epoch 16/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 6.3348 - val_loss: 6.5492\n",
      "Epoch 17/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 6.0335 - val_loss: 6.1675\n",
      "Epoch 18/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 5.5795 - val_loss: 6.1818\n",
      "Epoch 19/100\n",
      "560/560 [==============================] - 0s 45us/step - loss: 5.5234 - val_loss: 5.5472\n",
      "Epoch 20/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 5.1633 - val_loss: 6.0941\n",
      "Epoch 21/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 5.0551 - val_loss: 5.0999\n",
      "Epoch 22/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 4.6822 - val_loss: 4.9782\n",
      "Epoch 23/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 4.5000 - val_loss: 5.2731\n",
      "Epoch 24/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 4.5159 - val_loss: 4.6112\n",
      "Epoch 25/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 4.2222 - val_loss: 4.4328\n",
      "Epoch 26/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 4.0597 - val_loss: 4.2244\n",
      "Epoch 27/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 3.9272 - val_loss: 4.0786\n",
      "Epoch 28/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 3.8184 - val_loss: 4.0048\n",
      "Epoch 29/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 3.7184 - val_loss: 3.8354\n",
      "Epoch 30/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 3.5941 - val_loss: 3.7904\n",
      "Epoch 31/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 3.5919 - val_loss: 3.8272\n",
      "Epoch 32/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 3.3515 - val_loss: 3.5750\n",
      "Epoch 33/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 3.3348 - val_loss: 3.8271\n",
      "Epoch 34/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 3.3035 - val_loss: 3.6798\n",
      "Epoch 35/100\n",
      "560/560 [==============================] - 0s 46us/step - loss: 3.2402 - val_loss: 3.2357\n",
      "Epoch 36/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 3.0690 - val_loss: 3.4253\n",
      "Epoch 37/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 3.1368 - val_loss: 3.1710\n",
      "Epoch 38/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 2.9811 - val_loss: 3.0751\n",
      "Epoch 39/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 2.9140 - val_loss: 3.0500\n",
      "Epoch 40/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 2.8949 - val_loss: 3.0578\n",
      "Epoch 41/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 2.8152 - val_loss: 3.3445\n",
      "Epoch 42/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 2.9023 - val_loss: 2.8449\n",
      "Epoch 43/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.7077 - val_loss: 2.8572\n",
      "Epoch 44/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.6904 - val_loss: 2.8667\n",
      "Epoch 45/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.7667 - val_loss: 2.6935\n",
      "Epoch 46/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 2.5441 - val_loss: 2.6485\n",
      "Epoch 47/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.5224 - val_loss: 2.6874\n",
      "Epoch 48/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.5714 - val_loss: 2.5300\n",
      "Epoch 49/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.4591 - val_loss: 2.4919\n",
      "Epoch 50/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 2.4856 - val_loss: 2.4772\n",
      "Epoch 51/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 2.4545 - val_loss: 2.4694\n",
      "Epoch 52/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 2.3867 - val_loss: 2.3925\n",
      "Epoch 53/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 2.3603 - val_loss: 2.5830\n",
      "Epoch 54/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 2.3781 - val_loss: 2.4637\n",
      "Epoch 55/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 2.3747 - val_loss: 2.3557\n",
      "Epoch 56/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 2.2595 - val_loss: 2.3843\n",
      "Epoch 57/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 2.2269 - val_loss: 2.4703\n",
      "Epoch 58/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.2744 - val_loss: 2.4079\n",
      "Epoch 59/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 2.3043 - val_loss: 2.2396\n",
      "Epoch 60/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 2.1694 - val_loss: 2.2479\n",
      "Epoch 61/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.1584 - val_loss: 2.1805\n",
      "Epoch 62/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 2.1328 - val_loss: 2.3013\n",
      "Epoch 63/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.1784 - val_loss: 2.3265\n",
      "Epoch 64/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 2.1171 - val_loss: 2.2274\n",
      "Epoch 65/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 2.0871 - val_loss: 2.2418\n",
      "Epoch 66/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.0561 - val_loss: 2.0816\n",
      "Epoch 67/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.9915 - val_loss: 2.0492\n",
      "Epoch 68/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 1.9851 - val_loss: 2.0241\n",
      "Epoch 69/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.9702 - val_loss: 2.0714\n",
      "Epoch 70/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 2.0101 - val_loss: 2.0170\n",
      "Epoch 71/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 1.9074 - val_loss: 1.9364\n",
      "Epoch 72/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.8833 - val_loss: 1.8980\n",
      "Epoch 73/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 1.8683 - val_loss: 1.9440\n",
      "Epoch 74/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 1.8851 - val_loss: 2.0647\n",
      "Epoch 75/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.8810 - val_loss: 1.8473\n",
      "Epoch 76/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.8315 - val_loss: 1.8998\n",
      "Epoch 77/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.8094 - val_loss: 1.8222\n",
      "Epoch 78/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 1.8207 - val_loss: 1.8414\n",
      "Epoch 79/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.7619 - val_loss: 1.8160\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 48us/step - loss: 1.7393 - val_loss: 1.7731\n",
      "Epoch 81/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.7405 - val_loss: 1.7591\n",
      "Epoch 82/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 1.7267 - val_loss: 1.8874\n",
      "Epoch 83/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 1.7781 - val_loss: 1.8381\n",
      "Epoch 84/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.6987 - val_loss: 1.7002\n",
      "Epoch 85/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.6803 - val_loss: 1.7603\n",
      "Epoch 86/100\n",
      "560/560 [==============================] - 0s 18us/step - loss: 1.6924 - val_loss: 1.8362\n",
      "Epoch 87/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.6364 - val_loss: 1.6814\n",
      "Epoch 88/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 1.6062 - val_loss: 1.6793\n",
      "Epoch 89/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.6423 - val_loss: 1.7415\n",
      "Epoch 90/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.6081 - val_loss: 1.6661\n",
      "Epoch 91/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.6303 - val_loss: 1.6271\n",
      "Epoch 92/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.5268 - val_loss: 1.6519\n",
      "Epoch 93/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.6022 - val_loss: 1.7651\n",
      "Epoch 94/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.5626 - val_loss: 1.6491\n",
      "Epoch 95/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.5631 - val_loss: 1.6602\n",
      "Epoch 96/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.5351 - val_loss: 1.5712\n",
      "Epoch 97/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.4791 - val_loss: 1.5486\n",
      "Epoch 98/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 1.4728 - val_loss: 1.6777\n",
      "Epoch 99/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 1.5091 - val_loss: 1.6052\n",
      "Epoch 100/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 1.4672 - val_loss: 1.6242\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_shape = (1,), kernel_initializer=RandomUniform(-2, 2), bias_initializer=RandomUniform(-2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, kernel_initializer=RandomUniform(-2, 2), bias_initializer=RandomUniform(-2, 2)))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01))\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=100, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ebb0668>,\n",
       " <matplotlib.lines.Line2D at 0x1ebb07b8>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = model.get_layer(index=0).get_weights()\n",
    "a, b = A[0][0], A[1]\n",
    "B = model.get_layer(index=2).get_weights()\n",
    "c, d = B[0][:, 0], B[1][0]\n",
    "\n",
    "def approximate(x):\n",
    "    logit = np.zeros((len(a)))\n",
    "    for i in range(len(a)):\n",
    "        weighted_sum = a[i] * x + b[i]\n",
    "        logit[i] = (weighted_sum + abs(weighted_sum))/2\n",
    "    answer = sum([logit[i] * c[i] for i in range(len(c))]) + d\n",
    "    return answer\n",
    "\n",
    "interval = np.linspace(-5, 5, 101)\n",
    "squared_interval = [approximate(x) for x in interval]\n",
    "plt.plot(interval, squared_interval, 'g', interval, interval ** 2, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using tanh function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/100\n",
      "560/560 [==============================] - 2s 3ms/step - loss: 110.0394 - val_loss: 86.5305\n",
      "Epoch 2/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 73.0932 - val_loss: 63.6668\n",
      "Epoch 3/100\n",
      "560/560 [==============================] - 0s 50us/step - loss: 59.6977 - val_loss: 58.4199\n",
      "Epoch 4/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 53.9511 - val_loss: 53.5884\n",
      "Epoch 5/100\n",
      "560/560 [==============================] - 0s 80us/step - loss: 49.9636 - val_loss: 50.1778\n",
      "Epoch 6/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 46.5741 - val_loss: 46.5179\n",
      "Epoch 7/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 43.2523 - val_loss: 43.3287\n",
      "Epoch 8/100\n",
      "560/560 [==============================] - 0s 68us/step - loss: 40.4494 - val_loss: 40.5961\n",
      "Epoch 9/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 37.5799 - val_loss: 38.5105\n",
      "Epoch 10/100\n",
      "560/560 [==============================] - 0s 52us/step - loss: 35.3633 - val_loss: 35.7305\n",
      "Epoch 11/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 33.0787 - val_loss: 33.4582\n",
      "Epoch 12/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 30.8465 - val_loss: 31.4378\n",
      "Epoch 13/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 28.8071 - val_loss: 29.6073\n",
      "Epoch 14/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 26.7555 - val_loss: 27.7218\n",
      "Epoch 15/100\n",
      "560/560 [==============================] - 0s 54us/step - loss: 24.7998 - val_loss: 25.1130\n",
      "Epoch 16/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 22.8827 - val_loss: 23.4280\n",
      "Epoch 17/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 20.8998 - val_loss: 21.4451\n",
      "Epoch 18/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 19.0926 - val_loss: 19.5617\n",
      "Epoch 19/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 17.5095 - val_loss: 18.1781\n",
      "Epoch 20/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 16.2600 - val_loss: 16.8964\n",
      "Epoch 21/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 14.9885 - val_loss: 15.7119\n",
      "Epoch 22/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 13.8704 - val_loss: 14.8805\n",
      "Epoch 23/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 12.8733 - val_loss: 13.8503\n",
      "Epoch 24/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 11.9958 - val_loss: 12.9330\n",
      "Epoch 25/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 11.1555 - val_loss: 12.0145\n",
      "Epoch 26/100\n",
      "560/560 [==============================] - 0s 66us/step - loss: 10.3257 - val_loss: 11.3014\n",
      "Epoch 27/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 9.6058 - val_loss: 10.6630\n",
      "Epoch 28/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 8.9875 - val_loss: 9.9667\n",
      "Epoch 29/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 8.3680 - val_loss: 9.3476\n",
      "Epoch 30/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 7.8168 - val_loss: 8.7566\n",
      "Epoch 31/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 7.3757 - val_loss: 8.2614\n",
      "Epoch 32/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 6.8404 - val_loss: 8.0052\n",
      "Epoch 33/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 6.5734 - val_loss: 7.3856\n",
      "Epoch 34/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 6.0397 - val_loss: 7.1956\n",
      "Epoch 35/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 5.7567 - val_loss: 6.7886\n",
      "Epoch 36/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 5.4259 - val_loss: 6.3796\n",
      "Epoch 37/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 5.0920 - val_loss: 6.1057\n",
      "Epoch 38/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 4.8267 - val_loss: 5.7810\n",
      "Epoch 39/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 4.6344 - val_loss: 5.6356\n",
      "Epoch 40/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 4.3696 - val_loss: 5.2874\n",
      "Epoch 41/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 4.1399 - val_loss: 5.0562\n",
      "Epoch 42/100\n",
      "560/560 [==============================] - 0s 41us/step - loss: 3.9315 - val_loss: 4.9299\n",
      "Epoch 43/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 3.8046 - val_loss: 4.7003\n",
      "Epoch 44/100\n",
      "560/560 [==============================] - 0s 39us/step - loss: 3.5493 - val_loss: 4.4187\n",
      "Epoch 45/100\n",
      "560/560 [==============================] - 0s 38us/step - loss: 3.4053 - val_loss: 4.2661\n",
      "Epoch 46/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 3.2965 - val_loss: 4.0590\n",
      "Epoch 47/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 3.0976 - val_loss: 3.9188\n",
      "Epoch 48/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 2.9868 - val_loss: 3.9174\n",
      "Epoch 49/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 2.8952 - val_loss: 3.6447\n",
      "Epoch 50/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.8071 - val_loss: 3.5084\n",
      "Epoch 51/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 2.6576 - val_loss: 3.4458\n",
      "Epoch 52/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.5698 - val_loss: 3.3077\n",
      "Epoch 53/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 2.4527 - val_loss: 3.2147\n",
      "Epoch 54/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 2.4204 - val_loss: 3.1016\n",
      "Epoch 55/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 2.2754 - val_loss: 3.0979\n",
      "Epoch 56/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.2468 - val_loss: 2.9836\n",
      "Epoch 57/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.1863 - val_loss: 3.0163\n",
      "Epoch 58/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 2.1740 - val_loss: 2.8800\n",
      "Epoch 59/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 2.0896 - val_loss: 2.6460\n",
      "Epoch 60/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.0690 - val_loss: 2.7578\n",
      "Epoch 61/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 2.1290 - val_loss: 2.5498\n",
      "Epoch 62/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.8630 - val_loss: 2.5478\n",
      "Epoch 63/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.8118 - val_loss: 2.4953\n",
      "Epoch 64/100\n",
      "560/560 [==============================] - 0s 43us/step - loss: 1.9455 - val_loss: 2.9399\n",
      "Epoch 65/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 2.0665 - val_loss: 2.3253\n",
      "Epoch 66/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.6773 - val_loss: 2.2254\n",
      "Epoch 67/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.6695 - val_loss: 2.1638\n",
      "Epoch 68/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.6755 - val_loss: 3.1131\n",
      "Epoch 69/100\n",
      "560/560 [==============================] - 0s 34us/step - loss: 2.3310 - val_loss: 2.1421\n",
      "Epoch 70/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 1.5173 - val_loss: 2.0520\n",
      "Epoch 71/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 1.4787 - val_loss: 2.0450\n",
      "Epoch 72/100\n",
      "560/560 [==============================] - 0s 46us/step - loss: 1.4659 - val_loss: 1.9519\n",
      "Epoch 73/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.4142 - val_loss: 1.9759\n",
      "Epoch 74/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.3884 - val_loss: 1.9101\n",
      "Epoch 75/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.4044 - val_loss: 1.8590\n",
      "Epoch 76/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 1.5029 - val_loss: 1.8635\n",
      "Epoch 77/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.3653 - val_loss: 2.0549\n",
      "Epoch 78/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 1.4674 - val_loss: 1.9123\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 0s 21us/step - loss: 1.4495 - val_loss: 1.8487\n",
      "Epoch 80/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.5829 - val_loss: 2.3325\n",
      "Epoch 81/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 1.7693 - val_loss: 2.7065\n",
      "Epoch 82/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 2.3319 - val_loss: 1.6166\n",
      "Epoch 83/100\n",
      "560/560 [==============================] - 0s 18us/step - loss: 1.1822 - val_loss: 1.7620\n",
      "Epoch 84/100\n",
      "560/560 [==============================] - 0s 32us/step - loss: 1.1724 - val_loss: 1.7143\n",
      "Epoch 85/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.3063 - val_loss: 1.6526\n",
      "Epoch 86/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.7289 - val_loss: 1.7275\n",
      "Epoch 87/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 1.3503 - val_loss: 1.6254\n",
      "Epoch 88/100\n",
      "560/560 [==============================] - 0s 23us/step - loss: 1.5487 - val_loss: 1.4794\n",
      "Epoch 89/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.0794 - val_loss: 1.5054\n",
      "Epoch 90/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 1.1145 - val_loss: 1.4246\n",
      "Epoch 91/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.1069 - val_loss: 1.5510\n",
      "Epoch 92/100\n",
      "560/560 [==============================] - 0s 30us/step - loss: 1.0826 - val_loss: 1.5525\n",
      "Epoch 93/100\n",
      "560/560 [==============================] - 0s 36us/step - loss: 1.2846 - val_loss: 1.7323\n",
      "Epoch 94/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 1.0796 - val_loss: 1.4276\n",
      "Epoch 95/100\n",
      "560/560 [==============================] - 0s 27us/step - loss: 1.0051 - val_loss: 1.5081\n",
      "Epoch 96/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 0.9864 - val_loss: 1.3673\n",
      "Epoch 97/100\n",
      "560/560 [==============================] - 0s 21us/step - loss: 1.0620 - val_loss: 1.5531\n",
      "Epoch 98/100\n",
      "560/560 [==============================] - 0s 25us/step - loss: 1.0757 - val_loss: 1.3313\n",
      "Epoch 99/100\n",
      "560/560 [==============================] - 0s 20us/step - loss: 0.9661 - val_loss: 1.2292\n",
      "Epoch 100/100\n",
      "560/560 [==============================] - 0s 29us/step - loss: 0.9626 - val_loss: 1.2537\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape = (1,), kernel_initializer=RandomUniform(-2, 2), bias_initializer=RandomUniform(-2, 2)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1, kernel_initializer=RandomUniform(-2, 2), bias_initializer=RandomUniform(-2, 2)))\n",
    "model.add(Activation('linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01))\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=100, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ed7f6d8>,\n",
       " <matplotlib.lines.Line2D at 0x1ed7f828>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = model.get_layer(index=0).get_weights()\n",
    "a, b = A[0][0], A[1]\n",
    "B = model.get_layer(index=2).get_weights()\n",
    "c, d = B[0][:, 0], B[1][0]\n",
    "\n",
    "def approximate(x):\n",
    "    logit = np.zeros((len(a)))\n",
    "    for i in range(len(a)):\n",
    "        weighted_sum = a[i] * x + b[i]\n",
    "        logit[i] = np.tanh(weighted_sum)\n",
    "    answer = sum([logit[i] * c[i] for i in range(len(c))]) + d\n",
    "    return answer\n",
    "\n",
    "interval = np.linspace(-5, 5, 101)\n",
    "squared_interval = [approximate(x) for x in interval]\n",
    "plt.plot(interval, squared_interval, 'g', interval, interval ** 2, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gradient-based Method for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many iterative methods can be used to solve the optimization problem, like Gradient Descent and Schochastic Gradient Descent (SGD). Some of them needs the first order derivative (gradient) of the cost w.r.t. the paramters (i.e., weights and bias).\n",
    "\n",
    "These derivatives can be calcuated from the higher layers, then back to lower layers. This technique is called **backpropagation**.\n",
    "\n",
    "**Example for 2-layer feedforward network for regression**\n",
    "\n",
    "<img src=\"F3.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cost function: \n",
    "$$\n",
    "L(\\mathbf x_{(1)}, \\ldots, \\mathbf x_{(N)}, t_{(1)}, \\ldots, t_{(N)}) = \\sum_{n} E_n := \\sum_{n} \\frac12 |y_{(n)} - t_{(n)}|^2\n",
    "$$\n",
    "\n",
    "We ignore the index $n$ for short.\n",
    "\n",
    "- We need to calculate \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial v_{k,m}}$$ for $k=1, \\ldots, K; m=0, \\ldots, M$ \n",
    "and \n",
    "$$\\frac{\\partial E}{\\partial w_{m,d}}$$ for $m = 1, \\ldots, M; d = 0, \\ldots, D$.\n",
    "\n",
    "**Second layer**\n",
    "\n",
    "We have\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial v_{k,m}}(\\mathbf w, \\mathbf v)  = \\frac{\\partial E}{\\partial b_k} (\\mathbf y) \\frac{\\partial b_k}{\\partial v_{k,m}}(\\mathbf w, \\mathbf v) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial b_k} = b_k - t_k = y_k - t_k\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{\\partial b_k}{\\partial v_{k,m}} = z_m\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial v_{k,m}}(\\mathbf w, \\mathbf v) = (y_k - t_k)z_m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First layer**\n",
    "\n",
    "We have\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{m,d}} = \\frac{\\partial E}{\\partial a_m} \\frac{\\partial a_m}{\\partial w_{m,d}}\n",
    "$$\n",
    "\n",
    "Note that\n",
    "$$\n",
    "\\frac{\\partial a_m}{\\partial w_{m,d}} = x_d\n",
    "$$\n",
    "\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial a_m} =  \\frac{\\partial z_m}{\\partial a_m} \\frac{\\partial E}{\\partial z_m} = h' \\cdot \\sum_{k=1}^K v_{k,m} \\frac{\\partial E}{\\partial b_k}\n",
    "$$\n",
    "\n",
    "So\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{m,d}} = x_d h' \\cdot \\sum_{k=1}^K v_{k,m} \\frac{\\partial E}{\\partial b_k}\n",
    "$$\n",
    "\n",
    "We see that the values $\\frac{\\partial E}{\\partial b_k}$ of the second layer are first calculated and then propagate to the first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complexity**\n",
    "\n",
    "The algorithm requires *O(iW)* where $i$ is the number of iterations, $W$ is the total number of paramaters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[1] C. Bishop, *Pattern Recognition and Machine Learning*  \n",
    "[2] I. Goodfellow, Y. Bengio, A.Courville, *Deep Learning*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
